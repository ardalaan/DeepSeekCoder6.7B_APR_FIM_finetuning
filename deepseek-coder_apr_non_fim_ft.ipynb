{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4853d470-9b9c-4455-ab98-770114f668cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers==0.0.23\n",
      "  Using cached xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.23) (1.24.1)\n",
      "Collecting torch==2.1.1 (from xformers==0.0.23)\n",
      "  Using cached torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (2.1.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1->xformers==0.0.23)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.1->xformers==0.0.23) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.1->xformers==0.0.23) (1.3.0)\n",
      "Using cached xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
      "Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 torch-2.1.1 xformers-0.0.23\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Found existing installation: ninja 1.11.1.1\n",
      "Uninstalling ninja-1.11.1.1:\n",
      "  Successfully uninstalled ninja-1.11.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting ninja\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "Installing collected packages: ninja\n",
      "Successfully installed ninja-1.11.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "1.11.1.git.kitware.jobserver-1\n",
      "0\n",
      "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-vloy_l11/unsloth_8e31ae14f8a048f79db8f19a94e96298\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-vloy_l11/unsloth_8e31ae14f8a048f79db8f19a94e96298\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit ae9e264e33c69b53dd5d533a4c5a264af4141c28\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2)\n",
      "Collecting tyro (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting transformers<4.45.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets>=2.16.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tqdm (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.6)\n",
      "Collecting wheel>=0.42.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.24.1)\n",
      "Collecting protobuf<4.0.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting huggingface-hub (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hf-transfer (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4.0)\n",
      "Collecting aiohttp (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiohttp-3.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.0)\n",
      "Collecting regex!=2019.12.17 (from transformers<4.45.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<4.45.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<4.45.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached rich-13.9.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "INFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2022.12.7)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.11-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m247.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.1-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m185.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m140.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m200.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m169.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (447 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m447.9/447.9 kB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.9.post4-py3-none-any.whl size=165293 sha256=cd6d7c31511fe03eb22ba8275aa2a60a059a67e30d1f1ca6a45d2c0e216ead1e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fywqa63a/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "Installing collected packages: sentencepiece, pytz, xxhash, wheel, unsloth, tzdata, typing-extensions, tqdm, shtab, safetensors, requests, regex, pyarrow, protobuf, mdurl, hf-transfer, fsspec, frozenlist, docstring-parser, dill, async-timeout, aiohappyeyeballs, pandas, multiprocess, multidict, markdown-it-py, huggingface-hub, aiosignal, yarl, tokenizers, rich, tyro, transformers, aiohttp, datasets\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.3\n",
      "    Uninstalling wheel-0.41.3:\n",
      "      Successfully uninstalled wheel-0.41.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.8 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.4.1 fsspec-2024.6.1 hf-transfer-0.1.8 huggingface-hub-0.25.1 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 protobuf-3.20.3 pyarrow-17.0.0 pytz-2024.2 regex-2024.9.11 requests-2.32.3 rich-13.9.1 safetensors-0.4.5 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2 typing-extensions-4.12.2 tyro-0.8.11 tzdata-2024.2 unsloth-2024.9.post4 wheel-0.44.0 xxhash-3.5.0 yarl-1.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: xformers<0.0.27 in /usr/local/lib/python3.10/dist-packages (0.0.23)\n",
      "Collecting trl<0.9.0\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, trl, peft, accelerate\n",
      "Successfully installed accelerate-0.34.2 bitsandbytes-0.44.1 peft-0.13.0 trl-0.8.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.15.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.15.0-py2.py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.0/311.0 kB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, click, gitdb, gitpython, wandb\n",
      "Successfully installed click-8.1.7 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.15.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install xformers==0.0.23\n",
    "\n",
    "!pip install packaging\n",
    "!pip uninstall -y ninja && pip install ninja\n",
    "!ninja --version\n",
    "!echo $?\n",
    "\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0de358-9833-42ec-bae6-b8783323c129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6150930d1a084e83a99c537cc6e67171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c6dd5-7d2e-4624-af47-03aafa818c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'DeepSeekCoder6.7B_APR_FIM_finetuning' already exists and is not an empty directory.\n",
      "/workspace/DeepSeekCoder6.7B_APR_FIM_finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64643/64643 [00:01<00:00, 35027.41 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 33918.31 examples/s]\n",
      "Size of the train set: 64643. Size of the validation set: 1000\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 387/400 [00:00<00:00, 526.48it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (17110 > 16384). Running this sequence through the model will result in indexing errors\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 494.94it/s]\n",
      "The character to token ratio of the dataset is: 3.31\n",
      "A sample of valid dataset: {'input_ids': tensor([6831, 1977,  810,  ...,  507,  185,  459]), 'labels': tensor([6831, 1977,  810,  ...,  507,  185,  459])}\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.097 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.1+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.23. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "model.safetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.1k/25.1k [00:00<00:00, 96.2MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|     | 21.0M/9.98G [00:00<01:12, 138MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|     | 41.9M/9.98G [00:00<01:02, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 62.9M/9.98G [00:00<00:59, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 83.9M/9.98G [00:00<00:58, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|      | 105M/9.98G [00:00<00:56, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|      | 126M/9.98G [00:00<00:56, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|      | 147M/9.98G [00:00<00:57, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|      | 168M/9.98G [00:00<00:57, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|      | 189M/9.98G [00:01<00:58, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|â–     | 210M/9.98G [00:01<00:55, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|â–     | 231M/9.98G [00:01<00:55, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–     | 252M/9.98G [00:01<00:54, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–     | 273M/9.98G [00:01<00:54, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–     | 294M/9.98G [00:01<00:54, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–     | 315M/9.98G [00:01<00:52, 185MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|â–     | 336M/9.98G [00:01<00:52, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–     | 357M/9.98G [00:02<00:53, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–     | 377M/9.98G [00:02<00:53, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–     | 398M/9.98G [00:02<00:53, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–Ž     | 419M/9.98G [00:02<00:52, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|â–Ž     | 440M/9.98G [00:02<00:54, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–Ž     | 461M/9.98G [00:02<00:56, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–Ž     | 482M/9.98G [00:02<00:57, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–Ž     | 503M/9.98G [00:02<00:57, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–Ž     | 524M/9.98G [00:03<00:56, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|â–Ž     | 545M/9.98G [00:03<00:55, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–Ž     | 566M/9.98G [00:03<00:55, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–Ž     | 587M/9.98G [00:03<00:55, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–Ž     | 608M/9.98G [00:03<00:53, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–     | 629M/9.98G [00:03<00:52, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–     | 650M/9.98G [00:03<00:51, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–     | 671M/9.98G [00:03<00:51, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–     | 692M/9.98G [00:03<00:50, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–     | 713M/9.98G [00:04<00:50, 184MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|â–     | 734M/9.98G [00:04<00:53, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–     | 755M/9.98G [00:04<00:57, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–     | 776M/9.98G [00:04<01:00, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–     | 797M/9.98G [00:04<00:59, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–     | 818M/9.98G [00:04<00:57, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–Œ     | 839M/9.98G [00:04<00:56, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–Œ     | 860M/9.98G [00:05<00:56, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–Œ     | 881M/9.98G [00:05<00:57, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–Œ     | 902M/9.98G [00:05<00:53, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–Œ     | 923M/9.98G [00:05<00:51, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|â–Œ     | 944M/9.98G [00:05<00:50, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|â–Œ     | 965M/9.98G [00:05<00:49, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|â–Œ     | 986M/9.98G [00:05<00:49, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|â–Œ    | 1.01G/9.98G [00:05<00:49, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|â–Œ    | 1.03G/9.98G [00:05<00:49, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 1.05G/9.98G [00:06<00:49, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 1.07G/9.98G [00:06<00:49, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 1.09G/9.98G [00:06<00:49, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 1.11G/9.98G [00:06<00:48, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–Œ    | 1.13G/9.98G [00:06<00:47, 186MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 1.15G/9.98G [00:06<00:46, 189MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 1.17G/9.98G [00:06<00:46, 190MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 1.20G/9.98G [00:06<00:46, 189MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 1.22G/9.98G [00:06<00:45, 192MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|â–Œ    | 1.24G/9.98G [00:07<00:46, 190MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 1.26G/9.98G [00:07<00:46, 187MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 1.28G/9.98G [00:07<00:46, 186MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 1.30G/9.98G [00:07<00:46, 186MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 1.32G/9.98G [00:07<00:46, 187MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|â–‹    | 1.34G/9.98G [00:07<00:48, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–‹    | 1.36G/9.98G [00:07<00:50, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–‹    | 1.38G/9.98G [00:07<00:51, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–‹    | 1.41G/9.98G [00:08<00:51, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|â–‹    | 1.43G/9.98G [00:08<00:48, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–‹    | 1.45G/9.98G [00:08<00:46, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–‹    | 1.47G/9.98G [00:08<00:45, 186MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–‹    | 1.49G/9.98G [00:08<00:44, 189MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–Š    | 1.51G/9.98G [00:08<00:44, 191MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|â–Š    | 1.53G/9.98G [00:08<00:43, 192MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 1.55G/9.98G [00:08<00:46, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 1.57G/9.98G [00:08<00:48, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 1.59G/9.98G [00:09<00:50, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 1.61G/9.98G [00:09<00:49, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|â–Š    | 1.64G/9.98G [00:09<00:50, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 1.66G/9.98G [00:09<00:51, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 1.68G/9.98G [00:09<00:51, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 1.70G/9.98G [00:09<00:51, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 1.72G/9.98G [00:09<00:48, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|â–Š    | 1.74G/9.98G [00:09<00:47, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 1.76G/9.98G [00:10<00:46, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 1.78G/9.98G [00:10<00:53, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 1.80G/9.98G [00:10<00:55, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 1.82G/9.98G [00:10<00:52, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–‰    | 1.85G/9.98G [00:10<00:50, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 1.87G/9.98G [00:10<00:49, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 1.89G/9.98G [00:10<00:48, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 1.91G/9.98G [00:11<00:48, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|â–‰    | 1.93G/9.98G [00:11<00:53, 149MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–‰    | 1.95G/9.98G [00:11<00:56, 142MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–‰    | 1.97G/9.98G [00:11<01:03, 125MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–‰    | 1.99G/9.98G [00:11<00:56, 141MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–ˆ    | 2.01G/9.98G [00:11<01:02, 127MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–ˆ    | 2.03G/9.98G [00:12<01:10, 113MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–ˆ    | 2.06G/9.98G [00:12<01:16, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–ˆ    | 2.08G/9.98G [00:12<01:06, 118MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–ˆ    | 2.10G/9.98G [00:12<01:06, 118MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–ˆ    | 2.12G/9.98G [00:12<01:01, 129MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|â–ˆ    | 2.14G/9.98G [00:12<00:58, 133MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–ˆ    | 2.16G/9.98G [00:13<00:58, 134MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–ˆ    | 2.18G/9.98G [00:13<00:59, 131MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–‰   | 2.20G/9.98G [00:13<01:27, 89.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–‰   | 2.22G/9.98G [00:14<01:47, 71.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–‰   | 2.24G/9.98G [00:14<01:31, 84.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|â–‰   | 2.26G/9.98G [00:14<01:30, 85.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|â–ˆâ–   | 2.30G/9.98G [00:14<01:07, 113MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|â–ˆâ–   | 2.32G/9.98G [00:14<00:59, 129MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|â–ˆâ–   | 2.34G/9.98G [00:14<00:54, 139MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|â–ˆâ–   | 2.36G/9.98G [00:14<00:50, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|â–ˆâ–   | 2.38G/9.98G [00:15<00:47, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|â–ˆâ–   | 2.41G/9.98G [00:15<00:43, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|â–ˆâ–   | 2.43G/9.98G [00:15<00:41, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|â–ˆâ–   | 2.45G/9.98G [00:15<00:39, 189MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|â–ˆâ–   | 2.47G/9.98G [00:15<00:40, 184MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|â–ˆâ–Ž   | 2.50G/9.98G [00:15<00:39, 189MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|â–ˆâ–Ž   | 2.52G/9.98G [00:15<00:41, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|â–ˆâ–Ž   | 2.54G/9.98G [00:15<00:42, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|â–ˆâ–Ž   | 2.56G/9.98G [00:16<00:42, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|â–ˆâ–Ž   | 2.58G/9.98G [00:16<00:41, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|â–ˆâ–Ž   | 2.60G/9.98G [00:16<00:39, 185MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|â–ˆâ–Ž   | 2.62G/9.98G [00:16<00:38, 190MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|â–ˆâ–Ž   | 2.64G/9.98G [00:16<00:37, 194MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|â–ˆâ–Ž   | 2.66G/9.98G [00:16<00:37, 193MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|â–ˆâ–Ž   | 2.68G/9.98G [00:16<00:38, 192MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|â–ˆâ–Ž   | 2.71G/9.98G [00:16<00:40, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|â–ˆâ–Ž   | 2.73G/9.98G [00:16<00:41, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|â–ˆâ–   | 2.75G/9.98G [00:17<00:42, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|â–ˆâ–   | 2.77G/9.98G [00:17<00:42, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|â–ˆâ–   | 2.79G/9.98G [00:17<00:41, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|â–ˆâ–   | 2.81G/9.98G [00:17<00:40, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|â–ˆâ–   | 2.83G/9.98G [00:17<00:39, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|â–ˆâ–   | 2.85G/9.98G [00:17<00:39, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|â–ˆâ–   | 2.87G/9.98G [00:17<00:38, 185MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|â–ˆâ–   | 2.89G/9.98G [00:17<00:40, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|â–ˆâ–   | 2.92G/9.98G [00:17<00:42, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|â–ˆâ–   | 2.94G/9.98G [00:18<00:44, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|â–ˆâ–   | 2.96G/9.98G [00:18<00:44, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|â–ˆâ–   | 2.98G/9.98G [00:18<00:43, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|â–ˆâ–Œ   | 3.00G/9.98G [00:18<00:42, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|â–ˆâ–Œ   | 3.02G/9.98G [00:18<00:42, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|â–ˆâ–Œ   | 3.04G/9.98G [00:18<00:40, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|â–ˆâ–Œ   | 3.06G/9.98G [00:18<00:39, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|â–ˆâ–Œ   | 3.08G/9.98G [00:18<00:39, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|â–ˆâ–Œ   | 3.10G/9.98G [00:19<00:39, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|â–ˆâ–Œ   | 3.12G/9.98G [00:19<00:39, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|â–ˆâ–Œ   | 3.15G/9.98G [00:19<00:39, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|â–ˆâ–Œ   | 3.17G/9.98G [00:19<00:38, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|â–ˆâ–Œ   | 3.19G/9.98G [00:19<00:36, 185MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|â–ˆâ–Œ   | 3.21G/9.98G [00:19<00:35, 190MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|â–ˆâ–Œ   | 3.23G/9.98G [00:19<00:34, 194MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|â–ˆâ–‹   | 3.25G/9.98G [00:19<00:34, 196MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|â–ˆâ–‹   | 3.27G/9.98G [00:19<00:33, 198MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|â–ˆâ–‹   | 3.29G/9.98G [00:20<00:36, 186MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|â–ˆâ–‹   | 3.31G/9.98G [00:20<00:38, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|â–ˆâ–‹   | 3.33G/9.98G [00:20<00:39, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|â–ˆâ–‹   | 3.36G/9.98G [00:20<00:40, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|â–ˆâ–‹   | 3.38G/9.98G [00:20<00:38, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|â–ˆâ–‹   | 3.40G/9.98G [00:20<00:37, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|â–ˆâ–‹   | 3.42G/9.98G [00:20<00:36, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|â–ˆâ–‹   | 3.44G/9.98G [00:20<00:36, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|â–ˆâ–‹   | 3.46G/9.98G [00:21<00:35, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|â–ˆâ–‹   | 3.48G/9.98G [00:21<00:36, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|â–ˆâ–Š   | 3.50G/9.98G [00:21<00:37, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|â–ˆâ–Š   | 3.52G/9.98G [00:21<00:37, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|â–ˆâ–Š   | 3.54G/9.98G [00:21<00:37, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|â–ˆâ–Š   | 3.57G/9.98G [00:21<00:36, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|â–ˆâ–Š   | 3.59G/9.98G [00:21<00:35, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|â–ˆâ–Š   | 3.61G/9.98G [00:21<00:34, 185MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|â–ˆâ–Š   | 3.63G/9.98G [00:22<00:33, 188MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|â–ˆâ–Š   | 3.65G/9.98G [00:22<00:33, 190MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|â–ˆâ–Š   | 3.67G/9.98G [00:22<00:32, 195MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|â–ˆâ–Š   | 3.69G/9.98G [00:22<00:34, 184MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|â–ˆâ–Š   | 3.71G/9.98G [00:22<00:35, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|â–ˆâ–Š   | 3.73G/9.98G [00:22<00:37, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|â–ˆâ–‰   | 3.75G/9.98G [00:22<00:37, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|â–ˆâ–‰   | 3.77G/9.98G [00:22<00:38, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|â–ˆâ–‰   | 3.80G/9.98G [00:23<00:39, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|â–ˆâ–‰   | 3.82G/9.98G [00:23<00:40, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|â–ˆâ–‰   | 3.84G/9.98G [00:23<00:40, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|â–ˆâ–‰   | 3.86G/9.98G [00:23<00:38, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|â–ˆâ–‰   | 3.88G/9.98G [00:23<00:37, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|â–ˆâ–‰   | 3.90G/9.98G [00:23<00:36, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|â–ˆâ–‰   | 3.92G/9.98G [00:23<00:35, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|â–ˆâ–‰   | 3.94G/9.98G [00:23<00:34, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|â–ˆâ–‰   | 3.96G/9.98G [00:24<00:33, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|â–ˆâ–‰   | 3.98G/9.98G [00:24<00:33, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|â–ˆâ–ˆ   | 4.01G/9.98G [00:24<00:33, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|â–ˆâ–ˆ   | 4.03G/9.98G [00:24<00:33, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|â–ˆâ–ˆ   | 4.05G/9.98G [00:24<00:34, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|â–ˆâ–ˆ   | 4.07G/9.98G [00:24<00:48, 121MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|â–ˆâ–‹  | 4.09G/9.98G [00:25<01:05, 90.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|â–ˆâ–‹  | 4.11G/9.98G [00:25<01:20, 72.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|â–ˆâ–‹  | 4.12G/9.98G [00:25<01:24, 69.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|â–ˆâ–‹  | 4.15G/9.98G [00:26<01:07, 86.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|â–ˆâ–‹  | 4.16G/9.98G [00:26<01:12, 79.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|â–ˆâ–‹  | 4.17G/9.98G [00:26<01:14, 78.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|â–ˆâ–‹  | 4.19G/9.98G [00:26<01:01, 94.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|â–ˆâ–‹  | 4.20G/9.98G [00:26<01:08, 83.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|â–ˆâ–‹  | 4.22G/9.98G [00:26<01:17, 74.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|â–ˆâ–‹  | 4.23G/9.98G [00:27<01:23, 69.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|â–ˆâ–‹  | 4.25G/9.98G [00:27<01:06, 86.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|â–ˆâ–ˆâ–  | 4.27G/9.98G [00:27<00:55, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|â–ˆâ–‹  | 4.29G/9.98G [00:27<01:17, 73.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|â–ˆâ–‹  | 4.30G/9.98G [00:28<01:26, 65.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|â–ˆâ–‹  | 4.31G/9.98G [00:28<01:35, 59.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|â–ˆâ–‹  | 4.32G/9.98G [00:28<01:33, 60.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|â–ˆâ–‹  | 4.33G/9.98G [00:28<01:24, 66.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|â–ˆâ–‹  | 4.35G/9.98G [00:28<01:00, 92.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|â–ˆâ–Š  | 4.37G/9.98G [00:28<00:56, 99.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|â–ˆâ–ˆâ–  | 4.39G/9.98G [00:28<00:46, 119MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|â–ˆâ–ˆâ–  | 4.41G/9.98G [00:29<00:41, 135MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|â–ˆâ–ˆâ–  | 4.44G/9.98G [00:29<00:37, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|â–ˆâ–ˆâ–  | 4.46G/9.98G [00:29<00:34, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|â–ˆâ–ˆâ–  | 4.48G/9.98G [00:29<00:33, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|â–ˆâ–ˆâ–Ž  | 4.50G/9.98G [00:29<00:31, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|â–ˆâ–ˆâ–Ž  | 4.52G/9.98G [00:29<00:31, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|â–ˆâ–ˆâ–Ž  | 4.54G/9.98G [00:29<00:30, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|â–ˆâ–ˆâ–Ž  | 4.56G/9.98G [00:29<00:29, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|â–ˆâ–ˆâ–Ž  | 4.58G/9.98G [00:29<00:29, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|â–ˆâ–ˆâ–Ž  | 4.60G/9.98G [00:30<00:29, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|â–ˆâ–ˆâ–Ž  | 4.62G/9.98G [00:30<00:30, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|â–ˆâ–ˆâ–Ž  | 4.65G/9.98G [00:30<00:31, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|â–ˆâ–ˆâ–Ž  | 4.67G/9.98G [00:30<00:32, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|â–ˆâ–ˆâ–Ž  | 4.69G/9.98G [00:30<00:31, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|â–ˆâ–ˆâ–Ž  | 4.71G/9.98G [00:30<00:30, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|â–ˆâ–ˆâ–Ž  | 4.73G/9.98G [00:30<00:29, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|â–ˆâ–ˆâ–  | 4.75G/9.98G [00:30<00:29, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|â–ˆâ–ˆâ–  | 4.77G/9.98G [00:31<00:28, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|â–ˆâ–ˆâ–  | 4.79G/9.98G [00:31<00:28, 185MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|â–ˆâ–ˆâ–  | 4.81G/9.98G [00:31<00:29, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|â–ˆâ–ˆâ–  | 4.83G/9.98G [00:31<00:31, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|â–ˆâ–ˆâ–  | 4.85G/9.98G [00:31<00:32, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|â–ˆâ–ˆâ–  | 4.88G/9.98G [00:31<00:33, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|â–ˆâ–ˆâ–  | 4.90G/9.98G [00:31<00:31, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|â–ˆâ–ˆâ–  | 4.92G/9.98G [00:31<00:29, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|â–ˆâ–ˆâ–  | 4.94G/9.98G [00:32<00:28, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|â–ˆâ–ˆâ–  | 4.96G/9.98G [00:32<00:28, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|â–ˆâ–ˆâ–  | 4.98G/9.98G [00:32<00:27, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|â–ˆâ–ˆâ–Œ  | 5.00G/9.98G [00:32<00:28, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|â–ˆâ–ˆâ–Œ  | 5.02G/9.98G [00:32<00:30, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 5.04G/9.98G [00:32<00:31, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 5.06G/9.98G [00:32<00:32, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 5.09G/9.98G [00:32<00:31, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 5.11G/9.98G [00:33<00:31, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 5.13G/9.98G [00:33<00:31, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 5.15G/9.98G [00:33<00:30, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 5.17G/9.98G [00:33<00:29, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 5.19G/9.98G [00:33<00:29, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 5.21G/9.98G [00:33<00:29, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 5.23G/9.98G [00:33<00:29, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|â–ˆâ–ˆâ–‹  | 5.25G/9.98G [00:34<00:28, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|â–ˆâ–ˆâ–‹  | 5.27G/9.98G [00:34<00:27, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|â–ˆâ–ˆâ–‹  | 5.30G/9.98G [00:34<00:26, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|â–ˆâ–ˆâ–‹  | 5.32G/9.98G [00:34<00:25, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|â–ˆâ–ˆâ–‹  | 5.34G/9.98G [00:34<00:25, 184MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|â–ˆâ–ˆâ–‹  | 5.36G/9.98G [00:34<00:25, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|â–ˆâ–ˆâ–‹  | 5.38G/9.98G [00:34<00:25, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|â–ˆâ–ˆâ–‹  | 5.40G/9.98G [00:34<00:25, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|â–ˆâ–ˆâ–‹  | 5.42G/9.98G [00:34<00:25, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|â–ˆâ–ˆâ–‹  | 5.44G/9.98G [00:35<00:25, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|â–ˆâ–ˆâ–‹  | 5.46G/9.98G [00:35<00:24, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|â–ˆâ–ˆâ–‹  | 5.48G/9.98G [00:35<00:26, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|â–ˆâ–ˆâ–Š  | 5.51G/9.98G [00:35<00:26, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|â–ˆâ–ˆâ–Š  | 5.53G/9.98G [00:35<00:27, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|â–ˆâ–ˆâ–Š  | 5.55G/9.98G [00:35<00:26, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|â–ˆâ–ˆâ–Š  | 5.57G/9.98G [00:35<00:26, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|â–ˆâ–ˆâ–Š  | 5.59G/9.98G [00:35<00:25, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|â–ˆâ–ˆâ–Š  | 5.61G/9.98G [00:36<00:24, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|â–ˆâ–ˆâ–Š  | 5.63G/9.98G [00:36<00:25, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|â–ˆâ–ˆâ–Š  | 5.65G/9.98G [00:36<00:23, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|â–ˆâ–ˆâ–Š  | 5.67G/9.98G [00:36<00:24, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|â–ˆâ–ˆâ–Š  | 5.69G/9.98G [00:36<00:26, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|â–ˆâ–ˆâ–Š  | 5.71G/9.98G [00:36<00:27, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|â–ˆâ–ˆâ–Š  | 5.74G/9.98G [00:36<00:27, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|â–ˆâ–ˆâ–‰  | 5.76G/9.98G [00:36<00:25, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|â–ˆâ–ˆâ–‰  | 5.78G/9.98G [00:37<00:25, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|â–ˆâ–ˆâ–‰  | 5.80G/9.98G [00:37<00:24, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|â–ˆâ–ˆâ–‰  | 5.82G/9.98G [00:37<00:24, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|â–ˆâ–ˆâ–‰  | 5.84G/9.98G [00:37<00:23, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|â–ˆâ–ˆâ–‰  | 5.86G/9.98G [00:37<00:23, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|â–ˆâ–ˆâ–‰  | 5.88G/9.98G [00:37<00:23, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|â–ˆâ–ˆâ–‰  | 5.90G/9.98G [00:37<00:23, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|â–ˆâ–ˆâ–‰  | 5.92G/9.98G [00:37<00:23, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|â–ˆâ–ˆâ–‰  | 5.95G/9.98G [00:37<00:22, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|â–ˆâ–ˆâ–‰  | 5.97G/9.98G [00:38<00:21, 184MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆ  | 5.99G/9.98G [00:38<00:21, 188MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆ  | 6.01G/9.98G [00:38<00:20, 191MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆ  | 6.03G/9.98G [00:38<00:20, 189MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 6.05G/9.98G [00:38<00:21, 184MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 6.07G/9.98G [00:38<00:21, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 6.09G/9.98G [00:38<00:22, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 6.11G/9.98G [00:39<00:32, 119MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 6.13G/9.98G [00:39<00:32, 119MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 6.16G/9.98G [00:39<00:28, 132MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 6.18G/9.98G [00:39<00:26, 141MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 6.20G/9.98G [00:39<00:34, 111MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 6.22G/9.98G [00:40<00:36, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 6.24G/9.98G [00:40<00:34, 108MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 6.26G/9.98G [00:40<00:32, 113MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 6.28G/9.98G [00:40<00:32, 114MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 6.30G/9.98G [00:40<00:30, 119MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 6.32G/9.98G [00:40<00:32, 113MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 6.34G/9.98G [00:41<00:33, 109MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 6.36G/9.98G [00:41<00:28, 125MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 6.39G/9.98G [00:41<00:26, 134MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 6.41G/9.98G [00:41<00:24, 147MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 6.43G/9.98G [00:41<00:32, 109MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ– | 6.45G/9.98G [00:41<00:28, 125MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ– | 6.47G/9.98G [00:42<00:34, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|â–ˆâ–ˆâ–Œ | 6.49G/9.98G [00:42<00:43, 80.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|â–ˆâ–ˆâ–Œ | 6.51G/9.98G [00:42<00:37, 93.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|â–ˆâ–ˆâ–Œ | 6.53G/9.98G [00:43<00:40, 85.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–Ž | 6.55G/9.98G [00:43<00:33, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–Ž | 6.57G/9.98G [00:43<00:29, 117MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–Ž | 6.60G/9.98G [00:43<00:25, 130MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–Ž | 6.62G/9.98G [00:43<00:23, 141MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–Ž | 6.64G/9.98G [00:43<00:22, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–Ž | 6.66G/9.98G [00:43<00:22, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–Ž | 6.68G/9.98G [00:43<00:22, 146MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–Ž | 6.70G/9.98G [00:44<00:22, 144MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–Ž | 6.72G/9.98G [00:44<00:22, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 6.74G/9.98G [00:44<00:22, 142MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 6.76G/9.98G [00:44<00:22, 141MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 6.78G/9.98G [00:44<00:22, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 6.81G/9.98G [00:44<00:21, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 6.83G/9.98G [00:44<00:20, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 6.85G/9.98G [00:45<00:20, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 6.87G/9.98G [00:45<00:20, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 6.89G/9.98G [00:45<00:18, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 6.91G/9.98G [00:45<00:18, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 6.93G/9.98G [00:45<00:17, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ– | 6.95G/9.98G [00:45<00:17, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ– | 6.97G/9.98G [00:45<00:16, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–Œ | 6.99G/9.98G [00:45<00:16, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–Œ | 7.01G/9.98G [00:45<00:17, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 7.04G/9.98G [00:46<00:17, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 7.06G/9.98G [00:46<00:17, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 7.08G/9.98G [00:46<00:16, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 7.10G/9.98G [00:46<00:16, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 7.12G/9.98G [00:46<00:15, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 7.14G/9.98G [00:46<00:15, 182MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 7.16G/9.98G [00:46<00:15, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 7.18G/9.98G [00:46<00:16, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 7.20G/9.98G [00:47<00:17, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 7.22G/9.98G [00:47<00:17, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 7.25G/9.98G [00:47<00:17, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 7.27G/9.98G [00:47<00:16, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 7.29G/9.98G [00:47<00:16, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 7.31G/9.98G [00:47<00:16, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 7.33G/9.98G [00:47<00:16, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 7.35G/9.98G [00:47<00:15, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 7.37G/9.98G [00:48<00:15, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 7.39G/9.98G [00:48<00:15, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 7.41G/9.98G [00:48<00:16, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–‹ | 7.43G/9.98G [00:48<00:16, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–‹ | 7.46G/9.98G [00:48<00:16, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–‹ | 7.48G/9.98G [00:48<00:16, 149MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–Š | 7.50G/9.98G [00:48<00:17, 145MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–Š | 7.52G/9.98G [00:49<00:17, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 7.54G/9.98G [00:49<00:17, 140MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 7.56G/9.98G [00:49<00:17, 138MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 7.58G/9.98G [00:49<00:17, 137MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 7.60G/9.98G [00:49<00:16, 144MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 7.62G/9.98G [00:49<00:15, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 7.64G/9.98G [00:49<00:14, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 7.67G/9.98G [00:50<00:14, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 7.69G/9.98G [00:50<00:13, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 7.71G/9.98G [00:50<00:13, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 7.73G/9.98G [00:50<00:13, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 7.75G/9.98G [00:50<00:13, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 7.77G/9.98G [00:50<00:13, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 7.79G/9.98G [00:50<00:12, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 7.81G/9.98G [00:50<00:11, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 7.83G/9.98G [00:51<00:11, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 7.85G/9.98G [00:51<00:11, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 7.87G/9.98G [00:51<00:11, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 7.90G/9.98G [00:51<00:12, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 7.92G/9.98G [00:51<00:12, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–‰ | 7.94G/9.98G [00:51<00:12, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–‰ | 7.96G/9.98G [00:51<00:12, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–‰ | 7.98G/9.98G [00:51<00:12, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.00G/9.98G [00:52<00:12, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.02G/9.98G [00:52<00:11, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.04G/9.98G [00:52<00:11, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.06G/9.98G [00:52<00:11, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.08G/9.98G [00:52<00:11, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.11G/9.98G [00:52<00:11, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.13G/9.98G [00:52<00:11, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.15G/9.98G [00:53<00:11, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.17G/9.98G [00:53<00:10, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.19G/9.98G [00:53<00:10, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.21G/9.98G [00:53<00:14, 122MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 8.23G/9.98G [00:53<00:14, 121MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.25G/9.98G [00:53<00:14, 123MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.27G/9.98G [00:54<00:16, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.29G/9.98G [00:54<00:13, 122MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.32G/9.98G [00:54<00:12, 135MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–Ž| 8.34G/9.98G [00:54<00:20, 79.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–Ž| 8.36G/9.98G [00:55<00:27, 59.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–Ž| 8.38G/9.98G [00:55<00:21, 74.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–Ž| 8.40G/9.98G [00:55<00:22, 69.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–Ž| 8.41G/9.98G [00:56<00:29, 53.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–| 8.42G/9.98G [00:56<00:28, 54.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–| 8.43G/9.98G [00:56<00:30, 51.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–| 8.45G/9.98G [00:56<00:22, 69.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–| 8.47G/9.98G [00:56<00:16, 90.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8.50G/9.98G [00:57<00:12, 121MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8.54G/9.98G [00:57<00:09, 145MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8.57G/9.98G [00:57<00:08, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8.60G/9.98G [00:57<00:07, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8.63G/9.98G [00:57<00:07, 187MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8.66G/9.98G [00:57<00:06, 192MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8.69G/9.98G [00:58<00:06, 197MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8.71G/9.98G [00:58<00:06, 199MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.73G/9.98G [00:58<00:06, 201MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.76G/9.98G [00:58<00:06, 202MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.78G/9.98G [00:58<00:06, 197MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.80G/9.98G [00:58<00:06, 196MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.82G/9.98G [00:58<00:05, 195MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.84G/9.98G [00:58<00:05, 194MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.86G/9.98G [00:58<00:05, 194MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.88G/9.98G [00:58<00:05, 194MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.90G/9.98G [00:59<00:05, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.92G/9.98G [00:59<00:06, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.94G/9.98G [00:59<00:06, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 8.97G/9.98G [00:59<00:06, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 8.99G/9.98G [00:59<00:06, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.01G/9.98G [00:59<00:06, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.03G/9.98G [00:59<00:06, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.05G/9.98G [01:00<00:05, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.07G/9.98G [01:00<00:05, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.09G/9.98G [01:00<00:05, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.11G/9.98G [01:00<00:05, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.13G/9.98G [01:00<00:04, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.15G/9.98G [01:00<00:04, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.18G/9.98G [01:00<00:04, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.20G/9.98G [01:00<00:04, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.22G/9.98G [01:01<00:04, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.24G/9.98G [01:01<00:04, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.26G/9.98G [01:01<00:04, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.28G/9.98G [01:01<00:03, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.30G/9.98G [01:01<00:03, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.32G/9.98G [01:01<00:03, 185MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.34G/9.98G [01:01<00:03, 187MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.36G/9.98G [01:01<00:03, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.38G/9.98G [01:01<00:03, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.41G/9.98G [01:02<00:03, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.43G/9.98G [01:02<00:03, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.45G/9.98G [01:02<00:03, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.47G/9.98G [01:02<00:03, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.49G/9.98G [01:02<00:02, 173MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.51G/9.98G [01:02<00:02, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.53G/9.98G [01:02<00:02, 180MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.55G/9.98G [01:02<00:02, 183MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.57G/9.98G [01:03<00:02, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.59G/9.98G [01:03<00:02, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.62G/9.98G [01:03<00:02, 176MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.64G/9.98G [01:03<00:01, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.66G/9.98G [01:03<00:01, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.68G/9.98G [01:03<00:01, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.70G/9.98G [01:03<00:01, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.72G/9.98G [01:03<00:01, 179MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.74G/9.98G [01:03<00:01, 178MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.76G/9.98G [01:04<00:01, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.78G/9.98G [01:04<00:01, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.80G/9.98G [01:04<00:01, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.83G/9.98G [01:04<00:01, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.85G/9.98G [01:04<00:00, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.87G/9.98G [01:04<00:00, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.89G/9.98G [01:04<00:00, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.91G/9.98G [01:05<00:00, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.93G/9.98G [01:05<00:00, 181MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.95G/9.98G [01:05<00:00, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.98G/9.98G [01:05<00:00, 152MB/s]\u001b[A\n",
      "Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 1/2 [01:05<01:05, 65.68s/it]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|     | 21.0M/3.50G [00:00<00:22, 154MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|     | 41.9M/3.50G [00:00<00:21, 160MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|     | 62.9M/3.50G [00:00<00:20, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|     | 83.9M/3.50G [00:00<00:19, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|â–     | 105M/3.50G [00:00<00:19, 176MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|â–     | 126M/3.50G [00:00<00:19, 177MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|â–Ž     | 147M/3.50G [00:00<00:18, 181MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|â–Ž     | 168M/3.50G [00:00<00:18, 179MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|â–Ž     | 189M/3.50G [00:01<00:18, 178MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|â–Ž     | 210M/3.50G [00:01<00:18, 178MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|â–     | 231M/3.50G [00:01<00:18, 178MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|â–     | 252M/3.50G [00:01<00:20, 160MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|â–     | 273M/3.50G [00:01<00:22, 146MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|â–Œ     | 294M/3.50G [00:01<00:23, 138MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|â–Œ     | 315M/3.50G [00:01<00:22, 145MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|â–Œ     | 336M/3.50G [00:02<00:20, 157MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|â–Œ     | 357M/3.50G [00:02<00:18, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|â–‹     | 377M/3.50G [00:02<00:17, 175MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|â–‹     | 398M/3.50G [00:02<00:17, 181MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|â–‹     | 419M/3.50G [00:02<00:16, 187MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|â–Š     | 440M/3.50G [00:02<00:16, 190MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|â–Š     | 461M/3.50G [00:02<00:15, 193MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|â–Š     | 482M/3.50G [00:02<00:15, 194MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|â–Š     | 503M/3.50G [00:02<00:15, 196MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|â–‰     | 524M/3.50G [00:03<00:15, 194MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|â–‰     | 545M/3.50G [00:03<00:16, 178MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|â–‰     | 566M/3.50G [00:03<00:17, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|â–ˆ     | 587M/3.50G [00:03<00:17, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|â–ˆ     | 608M/3.50G [00:03<00:17, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|â–ˆ     | 629M/3.50G [00:03<00:17, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|â–ˆ     | 650M/3.50G [00:03<00:16, 170MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|â–ˆâ–    | 671M/3.50G [00:03<00:16, 173MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|â–ˆâ–    | 692M/3.50G [00:04<00:16, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|â–ˆâ–    | 713M/3.50G [00:04<00:15, 177MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|â–ˆâ–Ž    | 734M/3.50G [00:04<00:16, 173MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|â–ˆâ–Ž    | 755M/3.50G [00:04<00:16, 170MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|â–ˆâ–Ž    | 776M/3.50G [00:04<00:16, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|â–ˆâ–Ž    | 797M/3.50G [00:04<00:16, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|â–ˆâ–    | 818M/3.50G [00:04<00:15, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|â–ˆâ–    | 839M/3.50G [00:04<00:15, 177MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|â–ˆâ–    | 860M/3.50G [00:04<00:14, 180MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|â–ˆâ–Œ    | 881M/3.50G [00:05<00:14, 182MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|â–ˆâ–Œ    | 902M/3.50G [00:05<00:14, 184MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|â–ˆâ–Œ    | 923M/3.50G [00:05<00:13, 186MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|â–ˆâ–Œ    | 944M/3.50G [00:05<00:14, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|â–ˆâ–‹    | 965M/3.50G [00:05<00:15, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|â–ˆâ–‹    | 986M/3.50G [00:05<00:15, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|â–ˆâ–   | 1.01G/3.50G [00:05<00:15, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|â–ˆâ–   | 1.03G/3.50G [00:05<00:14, 172MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|â–ˆâ–   | 1.05G/3.50G [00:06<00:14, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|â–ˆâ–Œ   | 1.07G/3.50G [00:06<00:13, 177MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|â–ˆâ–Œ   | 1.09G/3.50G [00:06<00:13, 175MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|â–ˆâ–Œ   | 1.11G/3.50G [00:06<00:13, 180MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|â–ˆâ–Œ   | 1.13G/3.50G [00:06<00:14, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|â–ˆâ–‹   | 1.15G/3.50G [00:06<00:14, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|â–ˆâ–‹   | 1.17G/3.50G [00:06<00:15, 153MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|â–ˆâ–‹   | 1.20G/3.50G [00:07<00:15, 146MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|â–ˆâ–‹   | 1.22G/3.50G [00:07<00:16, 135MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|â–ˆâ–Š   | 1.24G/3.50G [00:07<00:17, 129MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|â–ˆâ–Š   | 1.26G/3.50G [00:07<00:16, 138MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|â–ˆâ–Š   | 1.28G/3.50G [00:07<00:15, 146MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|â–ˆâ–Š   | 1.30G/3.50G [00:07<00:14, 152MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|â–ˆâ–‰   | 1.32G/3.50G [00:07<00:13, 157MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|â–ˆâ–‰   | 1.34G/3.50G [00:08<00:13, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|â–ˆâ–‰   | 1.36G/3.50G [00:08<00:13, 164MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|â–ˆâ–‰   | 1.38G/3.50G [00:08<00:12, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|â–ˆâ–ˆ   | 1.41G/3.50G [00:08<00:12, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|â–ˆâ–ˆ   | 1.43G/3.50G [00:08<00:12, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|â–ˆâ–ˆ   | 1.45G/3.50G [00:08<00:11, 177MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|â–ˆâ–ˆ   | 1.47G/3.50G [00:08<00:11, 183MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|â–ˆâ–ˆâ–  | 1.49G/3.50G [00:08<00:11, 175MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|â–ˆâ–ˆâ–  | 1.51G/3.50G [00:08<00:11, 170MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|â–ˆâ–ˆâ–  | 1.53G/3.50G [00:09<00:11, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|â–ˆâ–ˆâ–  | 1.55G/3.50G [00:09<00:11, 175MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|â–ˆâ–ˆâ–  | 1.57G/3.50G [00:09<00:11, 175MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|â–ˆâ–ˆâ–Ž  | 1.59G/3.50G [00:09<00:11, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|â–ˆâ–ˆâ–Ž  | 1.61G/3.50G [00:09<00:11, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|â–ˆâ–ˆâ–Ž  | 1.64G/3.50G [00:09<00:11, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|â–ˆâ–ˆâ–Ž  | 1.66G/3.50G [00:09<00:11, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|â–ˆâ–ˆâ–  | 1.68G/3.50G [00:10<00:11, 160MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|â–ˆâ–ˆâ–  | 1.70G/3.50G [00:10<00:11, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|â–ˆâ–ˆâ–  | 1.72G/3.50G [00:10<00:11, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|â–ˆâ–ˆâ–  | 1.74G/3.50G [00:10<00:10, 172MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|â–ˆâ–ˆâ–Œ  | 1.76G/3.50G [00:10<00:09, 181MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 1.78G/3.50G [00:10<00:09, 187MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 1.80G/3.50G [00:10<00:08, 191MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 1.82G/3.50G [00:10<00:08, 195MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|â–ˆâ–ˆâ–‹  | 1.85G/3.50G [00:10<00:08, 186MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|â–ˆâ–ˆâ–‹  | 1.87G/3.50G [00:11<00:09, 178MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|â–ˆâ–ˆâ–‹  | 1.89G/3.50G [00:11<00:09, 172MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|â–ˆâ–ˆâ–‹  | 1.91G/3.50G [00:11<00:09, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|â–ˆâ–ˆâ–Š  | 1.93G/3.50G [00:11<00:09, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|â–ˆâ–ˆâ–Š  | 1.95G/3.50G [00:11<00:08, 177MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|â–ˆâ–ˆâ–Š  | 1.97G/3.50G [00:11<00:08, 179MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|â–ˆâ–ˆâ–Š  | 1.99G/3.50G [00:11<00:08, 181MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|â–ˆâ–ˆâ–Š  | 2.01G/3.50G [00:11<00:08, 182MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|â–ˆâ–ˆâ–‰  | 2.03G/3.50G [00:11<00:07, 186MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|â–ˆâ–ˆâ–‰  | 2.06G/3.50G [00:12<00:07, 188MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|â–ˆâ–ˆâ–‰  | 2.08G/3.50G [00:12<00:07, 190MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|â–ˆâ–ˆâ–‰  | 2.10G/3.50G [00:12<00:07, 191MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆ  | 2.12G/3.50G [00:12<00:07, 192MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 2.14G/3.50G [00:12<00:07, 192MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 2.16G/3.50G [00:12<00:07, 179MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 2.18G/3.50G [00:12<00:07, 171MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.20G/3.50G [00:12<00:07, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.22G/3.50G [00:13<00:07, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 2.24G/3.50G [00:13<00:07, 173MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ– | 2.26G/3.50G [00:13<00:07, 171MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–Ž | 2.29G/3.50G [00:13<00:07, 170MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–Ž | 2.31G/3.50G [00:13<00:07, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–Ž | 2.33G/3.50G [00:13<00:06, 172MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–Ž | 2.35G/3.50G [00:13<00:06, 178MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.37G/3.50G [00:13<00:06, 182MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.39G/3.50G [00:13<00:06, 185MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 2.41G/3.50G [00:14<00:05, 187MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 2.43G/3.50G [00:14<00:05, 191MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–Œ | 2.45G/3.50G [00:14<00:05, 178MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.47G/3.50G [00:14<00:06, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.50G/3.50G [00:14<00:06, 157MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 2.52G/3.50G [00:14<00:06, 156MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 2.54G/3.50G [00:14<00:05, 161MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 2.56G/3.50G [00:14<00:05, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 2.58G/3.50G [00:15<00:05, 171MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 2.60G/3.50G [00:15<00:05, 175MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–‹ | 2.62G/3.50G [00:15<00:04, 179MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–Š | 2.64G/3.50G [00:15<00:04, 181MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 2.66G/3.50G [00:15<00:04, 182MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 2.68G/3.50G [00:15<00:04, 184MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 2.71G/3.50G [00:15<00:04, 185MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 2.73G/3.50G [00:15<00:04, 186MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 2.75G/3.50G [00:16<00:04, 179MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 2.77G/3.50G [00:16<00:04, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–‰ | 2.79G/3.50G [00:16<00:04, 171MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.81G/3.50G [00:16<00:04, 171MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.83G/3.50G [00:16<00:03, 175MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.85G/3.50G [00:16<00:03, 178MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.87G/3.50G [00:16<00:03, 177MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 2.89G/3.50G [00:16<00:03, 176MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 2.92G/3.50G [00:16<00:03, 179MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 2.94G/3.50G [00:17<00:03, 180MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 2.96G/3.50G [00:17<00:03, 176MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2.98G/3.50G [00:17<00:03, 174MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.00G/3.50G [00:17<00:02, 171MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.02G/3.50G [00:17<00:02, 176MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.04G/3.50G [00:17<00:02, 183MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3.06G/3.50G [00:17<00:02, 187MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.08G/3.50G [00:17<00:02, 191MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.10G/3.50G [00:17<00:02, 193MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.12G/3.50G [00:18<00:01, 197MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.15G/3.50G [00:18<00:01, 187MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.17G/3.50G [00:18<00:01, 179MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.19G/3.50G [00:18<00:01, 173MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.21G/3.50G [00:18<00:01, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.23G/3.50G [00:18<00:01, 145MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.25G/3.50G [00:19<00:01, 130MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.27G/3.50G [00:19<00:01, 123MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.29G/3.50G [00:19<00:01, 125MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.31G/3.50G [00:19<00:01, 125MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.33G/3.50G [00:19<00:01, 126MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.36G/3.50G [00:19<00:01, 133MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.38G/3.50G [00:19<00:00, 141MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.40G/3.50G [00:20<00:00, 147MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.42G/3.50G [00:20<00:00, 152MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.44G/3.50G [00:20<00:00, 156MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.46G/3.50G [00:20<00:00, 157MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.48G/3.50G [00:20<00:00, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50G/3.50G [00:20<00:00, 169MB/s]\u001b[A\n",
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:26<00:00, 43.34s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.57s/it]\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:00<00:00, 527kB/s]\n",
      "deepseek-ai/deepseek-coder-6.7b-base does not have a padding token! Will use pad_token = <pad>.\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2024.9.post4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 39,976,960 || all params: 6,780,489,728 || trainable%: 0.5896\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 128,000 | Num Epochs = 9,223,372,036,854,775,807\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 2,000\n",
      " \"-____-\"     Number of trainable parameters = 39,976,960\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfake0make\u001b[0m (\u001b[33mfake0make-university-of-guilan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/DeepSeekCoder6.7B_APR_FIM_finetuning/wandb/run-20241002_113858-menvrzju\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeepseek-coder-6.7b-base-APR-finetuning\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/fake0make-university-of-guilan/deepseekcoder_apr_non_fim\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/fake0make-university-of-guilan/deepseekcoder_apr_non_fim/runs/menvrzju\u001b[0m\n",
      "{'loss': 0.7246, 'grad_norm': 0.021193798631429672, 'learning_rate': 5e-06, 'epoch': 0.0}\n",
      "{'loss': 0.7175, 'grad_norm': 0.02319401130080223, 'learning_rate': 1e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7248, 'grad_norm': 0.026905354112386703, 'learning_rate': 1.5e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7006, 'grad_norm': 0.02132747694849968, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7082, 'grad_norm': 0.034592460840940475, 'learning_rate': 2.5e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7396, 'grad_norm': 0.044634636491537094, 'learning_rate': 3e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7151, 'grad_norm': 0.05363329127430916, 'learning_rate': 3.5e-05, 'epoch': 0.02}\n",
      "{'loss': 0.728, 'grad_norm': 0.07714305073022842, 'learning_rate': 4e-05, 'epoch': 0.02}\n",
      "{'loss': 0.715, 'grad_norm': 0.05902973189949989, 'learning_rate': 4.5e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7478, 'grad_norm': 0.0486447736620903, 'learning_rate': 5e-05, 'epoch': 0.03}\n",
      "{'loss': 0.7092, 'grad_norm': 0.03771006688475609, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6815, 'grad_norm': 0.0345999114215374, 'learning_rate': 6e-05, 'epoch': 0.03}\n",
      "{'loss': 0.7041, 'grad_norm': 0.04595688357949257, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6733, 'grad_norm': 0.06485185772180557, 'learning_rate': 7e-05, 'epoch': 0.04}\n",
      "{'loss': 0.651, 'grad_norm': 0.025595098733901978, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6427, 'grad_norm': 0.018419429659843445, 'learning_rate': 8e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6252, 'grad_norm': 0.016313323751091957, 'learning_rate': 8.5e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6424, 'grad_norm': 0.015491588972508907, 'learning_rate': 9e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6272, 'grad_norm': 0.022728506475687027, 'learning_rate': 9.5e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6332, 'grad_norm': 0.014767380431294441, 'learning_rate': 0.0001, 'epoch': 0.05}\n",
      "  5%|â–ˆâ–Š                                   | 100/2000 [52:01<16:27:01, 31.17s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.6258626580238342, 'eval_runtime': 83.676, 'eval_samples_per_second': 6.86, 'eval_steps_per_second': 0.43, 'epoch': 0.05}\n",
      "{'loss': 0.6241, 'grad_norm': 0.016157614067196846, 'learning_rate': 0.000105, 'epoch': 0.05}\n",
      "{'loss': 0.6208, 'grad_norm': 0.014224478043615818, 'learning_rate': 0.00011000000000000002, 'epoch': 0.06}\n",
      "{'loss': 0.63, 'grad_norm': 0.016092009842395782, 'learning_rate': 0.00011499999999999999, 'epoch': 0.06}\n",
      "{'loss': 0.6336, 'grad_norm': 0.02180352434515953, 'learning_rate': 0.00012, 'epoch': 0.06}\n",
      "{'loss': 0.627, 'grad_norm': 0.016859591007232666, 'learning_rate': 0.000125, 'epoch': 0.06}\n",
      "{'loss': 0.6295, 'grad_norm': 0.016182858496904373, 'learning_rate': 0.00013000000000000002, 'epoch': 0.07}\n",
      "{'loss': 0.6269, 'grad_norm': 0.01625468023121357, 'learning_rate': 0.00013500000000000003, 'epoch': 0.07}\n",
      "{'loss': 0.6285, 'grad_norm': 0.017074843868613243, 'learning_rate': 0.00014, 'epoch': 0.07}\n",
      "{'loss': 0.6194, 'grad_norm': 0.01718832552433014, 'learning_rate': 0.000145, 'epoch': 0.07}\n",
      "{'loss': 0.6202, 'grad_norm': 0.017456477507948875, 'learning_rate': 0.00015000000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.6126, 'grad_norm': 0.017839472740888596, 'learning_rate': 0.000155, 'epoch': 0.08}\n",
      "{'loss': 0.611, 'grad_norm': 0.018800795078277588, 'learning_rate': 0.00016, 'epoch': 0.08}\n",
      "{'loss': 0.6264, 'grad_norm': 0.017914732918143272, 'learning_rate': 0.000165, 'epoch': 0.08}\n",
      "{'loss': 0.6403, 'grad_norm': 0.025787215679883957, 'learning_rate': 0.00017, 'epoch': 0.09}\n",
      "{'loss': 0.6339, 'grad_norm': 0.019009293988347054, 'learning_rate': 0.000175, 'epoch': 0.09}\n",
      "{'loss': 0.635, 'grad_norm': 0.01881069503724575, 'learning_rate': 0.00018, 'epoch': 0.09}\n",
      "{'loss': 0.6475, 'grad_norm': 0.028783485293388367, 'learning_rate': 0.00018500000000000002, 'epoch': 0.09}\n",
      "{'loss': 0.6449, 'grad_norm': 0.017961807548999786, 'learning_rate': 0.00019, 'epoch': 0.1}\n",
      "{'loss': 0.5966, 'grad_norm': 0.018559735268354416, 'learning_rate': 0.000195, 'epoch': 0.1}\n",
      "{'loss': 0.6011, 'grad_norm': 0.019446050748229027, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–Œ                               | 200/2000 [1:45:22<15:35:37, 31.19s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.6138726472854614, 'eval_runtime': 83.6214, 'eval_samples_per_second': 6.864, 'eval_steps_per_second': 0.431, 'epoch': 0.1}\n",
      "{'loss': 0.5924, 'grad_norm': 0.02083572931587696, 'learning_rate': 0.00019999619230641713, 'epoch': 0.1}\n",
      "{'loss': 0.6029, 'grad_norm': 0.021818287670612335, 'learning_rate': 0.00019998476951563915, 'epoch': 0.1}\n",
      "{'loss': 0.5998, 'grad_norm': 0.02135375887155533, 'learning_rate': 0.00019996573249755572, 'epoch': 0.11}\n",
      "{'loss': 0.6311, 'grad_norm': 0.021336639299988747, 'learning_rate': 0.0001999390827019096, 'epoch': 0.11}\n",
      "{'loss': 0.6172, 'grad_norm': 0.02096051722764969, 'learning_rate': 0.0001999048221581858, 'epoch': 0.11}\n",
      "{'loss': 0.6082, 'grad_norm': 0.0202645231038332, 'learning_rate': 0.0001998629534754574, 'epoch': 0.12}\n",
      "{'loss': 0.6187, 'grad_norm': 0.01930173486471176, 'learning_rate': 0.0001998134798421867, 'epoch': 0.12}\n",
      "{'loss': 0.6496, 'grad_norm': 0.020327841863036156, 'learning_rate': 0.00019975640502598244, 'epoch': 0.12}\n",
      "{'loss': 0.6046, 'grad_norm': 0.020944466814398766, 'learning_rate': 0.0001996917333733128, 'epoch': 0.12}\n",
      "{'loss': 0.6086, 'grad_norm': 0.01857466995716095, 'learning_rate': 0.00019961946980917456, 'epoch': 0.12}\n",
      "{'loss': 0.7579, 'grad_norm': 0.04117374122142792, 'learning_rate': 0.00019953961983671788, 'epoch': 0.13}\n",
      "{'loss': 0.9144, 'grad_norm': 0.022385377436876297, 'learning_rate': 0.00019945218953682734, 'epoch': 0.13}\n",
      "{'loss': 1.1005, 'grad_norm': 0.032834988087415695, 'learning_rate': 0.00019935718556765876, 'epoch': 0.13}\n",
      "{'loss': 0.9556, 'grad_norm': 0.028801076114177704, 'learning_rate': 0.00019925461516413223, 'epoch': 0.14}\n",
      "{'loss': 0.5826, 'grad_norm': 0.021884405985474586, 'learning_rate': 0.00019914448613738106, 'epoch': 0.14}\n",
      "{'loss': 0.6089, 'grad_norm': 0.02210225909948349, 'learning_rate': 0.00019902680687415705, 'epoch': 0.14}\n",
      "{'loss': 0.6238, 'grad_norm': 0.02047651633620262, 'learning_rate': 0.0001989015863361917, 'epoch': 0.14}\n",
      "{'loss': 0.6501, 'grad_norm': 0.021721316501498222, 'learning_rate': 0.00019876883405951377, 'epoch': 0.14}\n",
      "{'loss': 0.6753, 'grad_norm': 0.023154983296990395, 'learning_rate': 0.00019862856015372317, 'epoch': 0.15}\n",
      "{'loss': 0.6662, 'grad_norm': 0.022378791123628616, 'learning_rate': 0.00019848077530122083, 'epoch': 0.15}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             | 300/2000 [2:38:43<14:44:11, 31.21s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.6085906624794006, 'eval_runtime': 82.6533, 'eval_samples_per_second': 6.945, 'eval_steps_per_second': 0.436, 'epoch': 0.15}\n",
      "{'loss': 0.5952, 'grad_norm': 0.02264714241027832, 'learning_rate': 0.0001983254907563955, 'epoch': 0.15}\n",
      "{'loss': 0.6044, 'grad_norm': 0.0227067731320858, 'learning_rate': 0.00019816271834476642, 'epoch': 0.15}\n",
      "{'loss': 0.6048, 'grad_norm': 0.020747048780322075, 'learning_rate': 0.00019799247046208297, 'epoch': 0.16}\n",
      "{'loss': 0.6083, 'grad_norm': 0.02142214961349964, 'learning_rate': 0.00019781476007338058, 'epoch': 0.16}\n",
      "{'loss': 0.6437, 'grad_norm': 0.028071608394384384, 'learning_rate': 0.00019762960071199333, 'epoch': 0.16}\n",
      "{'loss': 0.6211, 'grad_norm': 0.019863130524754524, 'learning_rate': 0.00019743700647852354, 'epoch': 0.17}\n",
      "{'loss': 0.6233, 'grad_norm': 0.02039594016969204, 'learning_rate': 0.00019723699203976766, 'epoch': 0.17}\n",
      "{'loss': 0.6203, 'grad_norm': 0.020925214514136314, 'learning_rate': 0.00019702957262759965, 'epoch': 0.17}\n",
      "{'loss': 0.6019, 'grad_norm': 0.02007274702191353, 'learning_rate': 0.0001968147640378108, 'epoch': 0.17}\n",
      "{'loss': 0.6039, 'grad_norm': 0.021928120404481888, 'learning_rate': 0.00019659258262890683, 'epoch': 0.17}\n",
      "{'loss': 0.6303, 'grad_norm': 0.02048099786043167, 'learning_rate': 0.0001963630453208623, 'epoch': 0.18}\n",
      "{'loss': 0.605, 'grad_norm': 0.06326840817928314, 'learning_rate': 0.0001961261695938319, 'epoch': 0.18}\n",
      "{'loss': 0.608, 'grad_norm': 0.022341269999742508, 'learning_rate': 0.0001958819734868193, 'epoch': 0.18}\n",
      "{'loss': 0.6166, 'grad_norm': 0.021194657310843468, 'learning_rate': 0.00019563047559630357, 'epoch': 0.18}\n",
      "{'loss': 0.6191, 'grad_norm': 0.02217390574514866, 'learning_rate': 0.0001953716950748227, 'epoch': 0.19}\n",
      "{'loss': 0.6049, 'grad_norm': 0.020820435136556625, 'learning_rate': 0.00019510565162951537, 'epoch': 0.19}\n",
      "{'loss': 0.6173, 'grad_norm': 0.023948300629854202, 'learning_rate': 0.00019483236552061994, 'epoch': 0.19}\n",
      "{'loss': 0.634, 'grad_norm': 0.021327361464500427, 'learning_rate': 0.0001945518575599317, 'epoch': 0.2}\n",
      "{'loss': 0.623, 'grad_norm': 0.02085990086197853, 'learning_rate': 0.00019426414910921787, 'epoch': 0.2}\n",
      "{'loss': 0.6164, 'grad_norm': 0.02495076321065426, 'learning_rate': 0.00019396926207859084, 'epoch': 0.2}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 400/2000 [3:32:05<13:52:46, 31.23s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.6027957797050476, 'eval_runtime': 83.5665, 'eval_samples_per_second': 6.869, 'eval_steps_per_second': 0.431, 'epoch': 0.2}\n",
      "{'loss': 0.6193, 'grad_norm': 0.02228604257106781, 'learning_rate': 0.00019366721892483978, 'epoch': 0.2}\n",
      "{'loss': 0.615, 'grad_norm': 0.024872999638319016, 'learning_rate': 0.00019335804264972018, 'epoch': 0.2}\n",
      "{'loss': 0.6192, 'grad_norm': 0.024158116430044174, 'learning_rate': 0.00019304175679820247, 'epoch': 0.21}\n",
      "{'loss': 0.5953, 'grad_norm': 0.022470511496067047, 'learning_rate': 0.00019271838545667876, 'epoch': 0.21}\n",
      "{'loss': 0.6151, 'grad_norm': 0.023302190005779266, 'learning_rate': 0.0001923879532511287, 'epoch': 0.21}\n",
      "{'loss': 0.5975, 'grad_norm': 0.025604337453842163, 'learning_rate': 0.00019205048534524406, 'epoch': 0.21}\n",
      "{'loss': 0.6041, 'grad_norm': 0.024475760757923126, 'learning_rate': 0.0001917060074385124, 'epoch': 0.22}\n",
      "{'loss': 0.5965, 'grad_norm': 0.02384008839726448, 'learning_rate': 0.0001913545457642601, 'epoch': 0.22}\n",
      "{'loss': 0.5953, 'grad_norm': 0.023801861330866814, 'learning_rate': 0.00019099612708765434, 'epoch': 0.22}\n",
      "{'loss': 0.6139, 'grad_norm': 0.027516983449459076, 'learning_rate': 0.000190630778703665, 'epoch': 0.23}\n",
      "{'loss': 0.598, 'grad_norm': 0.024851568043231964, 'learning_rate': 0.00019025852843498607, 'epoch': 0.23}\n",
      "{'loss': 0.5944, 'grad_norm': 0.025703420862555504, 'learning_rate': 0.0001898794046299167, 'epoch': 0.23}\n",
      "{'loss': 0.6311, 'grad_norm': 0.022832833230495453, 'learning_rate': 0.00018949343616020252, 'epoch': 0.23}\n",
      "{'loss': 0.6159, 'grad_norm': 0.024481303989887238, 'learning_rate': 0.0001891006524188368, 'epoch': 0.23}\n",
      "{'loss': 0.6217, 'grad_norm': 0.02377268858253956, 'learning_rate': 0.00018870108331782217, 'epoch': 0.24}\n",
      "{'loss': 0.6045, 'grad_norm': 0.02090342529118061, 'learning_rate': 0.00018829475928589271, 'epoch': 0.24}\n",
      "{'loss': 0.6208, 'grad_norm': 0.022794436663389206, 'learning_rate': 0.00018788171126619653, 'epoch': 0.24}\n",
      "{'loss': 0.6164, 'grad_norm': 0.023174531757831573, 'learning_rate': 0.00018746197071393958, 'epoch': 0.24}\n",
      "{'loss': 0.6089, 'grad_norm': 0.022555742412805557, 'learning_rate': 0.00018703556959398998, 'epoch': 0.25}\n",
      "{'loss': 0.5958, 'grad_norm': 0.025013752281665802, 'learning_rate': 0.00018660254037844388, 'epoch': 0.25}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 500/2000 [4:25:26<13:00:09, 31.21s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5991335511207581, 'eval_runtime': 83.4337, 'eval_samples_per_second': 6.88, 'eval_steps_per_second': 0.431, 'epoch': 0.25}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 500/2000 [4:26:49<13:00:09, 31.21s/it]Saving model checkpoint to deepseek-coder-6.7b-base-APR-finetuning/checkpoint-500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "{'loss': 0.5929, 'grad_norm': 0.02702976018190384, 'learning_rate': 0.00018616291604415258, 'epoch': 0.25}\n",
      "{'loss': 0.6092, 'grad_norm': 0.02408680133521557, 'learning_rate': 0.00018571673007021123, 'epoch': 0.26}\n",
      "{'loss': 0.6222, 'grad_norm': 0.024385547265410423, 'learning_rate': 0.00018526401643540922, 'epoch': 0.26}\n",
      "{'loss': 0.6019, 'grad_norm': 0.02457750029861927, 'learning_rate': 0.0001848048096156426, 'epoch': 0.26}\n",
      "{'loss': 0.6123, 'grad_norm': 0.02452668733894825, 'learning_rate': 0.0001843391445812886, 'epoch': 0.26}\n",
      "{'loss': 0.5703, 'grad_norm': 0.02595396712422371, 'learning_rate': 0.00018386705679454242, 'epoch': 0.27}\n",
      "{'loss': 0.5663, 'grad_norm': 0.02725064940750599, 'learning_rate': 0.00018338858220671682, 'epoch': 0.27}\n",
      "{'loss': 0.5994, 'grad_norm': 0.023584961891174316, 'learning_rate': 0.00018290375725550417, 'epoch': 0.27}\n",
      "{'loss': 0.6155, 'grad_norm': 0.023703129962086678, 'learning_rate': 0.00018241261886220154, 'epoch': 0.27}\n",
      "{'loss': 0.5981, 'grad_norm': 0.022923264652490616, 'learning_rate': 0.0001819152044288992, 'epoch': 0.28}\n",
      "{'loss': 0.6026, 'grad_norm': 0.023021945729851723, 'learning_rate': 0.00018141155183563193, 'epoch': 0.28}\n",
      "{'loss': 0.6379, 'grad_norm': 0.024819014593958855, 'learning_rate': 0.00018090169943749476, 'epoch': 0.28}\n",
      "{'loss': 0.613, 'grad_norm': 0.023518046364188194, 'learning_rate': 0.00018038568606172173, 'epoch': 0.28}\n",
      "{'loss': 0.6009, 'grad_norm': 0.02632075361907482, 'learning_rate': 0.00017986355100472928, 'epoch': 0.28}\n",
      "{'loss': 0.6072, 'grad_norm': 0.023053467273712158, 'learning_rate': 0.00017933533402912354, 'epoch': 0.29}\n",
      "{'loss': 0.5887, 'grad_norm': 0.023727430030703545, 'learning_rate': 0.00017880107536067218, 'epoch': 0.29}\n",
      "{'loss': 0.5919, 'grad_norm': 0.02393299527466297, 'learning_rate': 0.0001782608156852414, 'epoch': 0.29}\n",
      "{'loss': 0.5936, 'grad_norm': 0.023366641253232956, 'learning_rate': 0.0001777145961456971, 'epoch': 0.29}\n",
      "{'loss': 0.5764, 'grad_norm': 0.03011762723326683, 'learning_rate': 0.00017716245833877201, 'epoch': 0.3}\n",
      "{'loss': 0.6025, 'grad_norm': 0.02651190757751465, 'learning_rate': 0.0001766044443118978, 'epoch': 0.3}\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 600/2000 [5:18:47<12:08:03, 31.20s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5949850678443909, 'eval_runtime': 82.1908, 'eval_samples_per_second': 6.984, 'eval_steps_per_second': 0.438, 'epoch': 0.3}\n",
      "{'loss': 0.5736, 'grad_norm': 0.026346251368522644, 'learning_rate': 0.0001760405965600031, 'epoch': 0.3}\n",
      "{'loss': 0.5789, 'grad_norm': 0.029504047706723213, 'learning_rate': 0.00017547095802227723, 'epoch': 0.3}\n",
      "{'loss': 0.592, 'grad_norm': 0.030524002388119698, 'learning_rate': 0.00017489557207890023, 'epoch': 0.31}\n",
      "{'loss': 0.6095, 'grad_norm': 0.03035910055041313, 'learning_rate': 0.00017431448254773944, 'epoch': 0.31}\n",
      "{'loss': 0.6069, 'grad_norm': 0.023909514769911766, 'learning_rate': 0.0001737277336810124, 'epoch': 0.31}\n",
      "{'loss': 0.6137, 'grad_norm': 0.026170572265982628, 'learning_rate': 0.00017313537016191706, 'epoch': 0.32}\n",
      "{'loss': 0.6034, 'grad_norm': 0.027130361646413803, 'learning_rate': 0.00017253743710122875, 'epoch': 0.32}\n",
      "{'loss': 0.6192, 'grad_norm': 0.024900473654270172, 'learning_rate': 0.0001719339800338651, 'epoch': 0.32}\n",
      "{'loss': 0.6156, 'grad_norm': 0.02634538896381855, 'learning_rate': 0.00017132504491541818, 'epoch': 0.32}\n",
      "{'loss': 0.6159, 'grad_norm': 0.028818948194384575, 'learning_rate': 0.00017071067811865476, 'epoch': 0.33}\n",
      "{'loss': 0.5981, 'grad_norm': 0.027303772047162056, 'learning_rate': 0.0001700909264299851, 'epoch': 0.33}\n",
      "{'loss': 0.5931, 'grad_norm': 0.027048790827393532, 'learning_rate': 0.00016946583704589973, 'epoch': 0.33}\n",
      "{'loss': 0.574, 'grad_norm': 0.026244884356856346, 'learning_rate': 0.0001688354575693754, 'epoch': 0.33}\n",
      "{'loss': 0.5887, 'grad_norm': 0.026746459305286407, 'learning_rate': 0.00016819983600624986, 'epoch': 0.34}\n",
      "{'loss': 0.581, 'grad_norm': 0.03216877579689026, 'learning_rate': 0.00016755902076156604, 'epoch': 0.34}\n",
      "{'loss': 0.5849, 'grad_norm': 0.026095090433955193, 'learning_rate': 0.00016691306063588583, 'epoch': 0.34}\n",
      "{'loss': 0.5672, 'grad_norm': 0.024621659889817238, 'learning_rate': 0.00016626200482157378, 'epoch': 0.34}\n",
      "{'loss': 0.581, 'grad_norm': 0.033395327627658844, 'learning_rate': 0.00016560590289905073, 'epoch': 0.34}\n",
      "{'loss': 0.5797, 'grad_norm': 0.028723085299134254, 'learning_rate': 0.00016494480483301836, 'epoch': 0.35}\n",
      "{'loss': 0.5746, 'grad_norm': 0.025824179872870445, 'learning_rate': 0.00016427876096865394, 'epoch': 0.35}\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 700/2000 [6:12:08<11:18:53, 31.33s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5907266736030579, 'eval_runtime': 83.477, 'eval_samples_per_second': 6.876, 'eval_steps_per_second': 0.431, 'epoch': 0.35}\n",
      "{'loss': 0.5748, 'grad_norm': 0.027997704222798347, 'learning_rate': 0.0001636078220277764, 'epoch': 0.35}\n",
      "{'loss': 0.5709, 'grad_norm': 0.026081057265400887, 'learning_rate': 0.00016293203910498376, 'epoch': 0.35}\n",
      "{'loss': 0.5589, 'grad_norm': 0.028215834870934486, 'learning_rate': 0.00016225146366376198, 'epoch': 0.36}\n",
      "{'loss': 0.5923, 'grad_norm': 0.028859466314315796, 'learning_rate': 0.0001615661475325658, 'epoch': 0.36}\n",
      "{'loss': 0.6066, 'grad_norm': 0.026725664734840393, 'learning_rate': 0.00016087614290087208, 'epoch': 0.36}\n",
      "{'loss': 0.5908, 'grad_norm': 0.050136495381593704, 'learning_rate': 0.00016018150231520486, 'epoch': 0.36}\n",
      "{'loss': 0.5678, 'grad_norm': 0.026574667543172836, 'learning_rate': 0.00015948227867513415, 'epoch': 0.37}\n",
      "{'loss': 0.5906, 'grad_norm': 0.027885964140295982, 'learning_rate': 0.00015877852522924732, 'epoch': 0.37}\n",
      "{'loss': 0.5585, 'grad_norm': 0.026279672980308533, 'learning_rate': 0.00015807029557109398, 'epoch': 0.37}\n",
      "{'loss': 0.5894, 'grad_norm': 0.031544990837574005, 'learning_rate': 0.0001573576436351046, 'epoch': 0.38}\n",
      "{'loss': 0.5873, 'grad_norm': 0.03240537270903587, 'learning_rate': 0.00015664062369248328, 'epoch': 0.38}\n",
      "{'loss': 0.6163, 'grad_norm': 0.030824953690171242, 'learning_rate': 0.0001559192903470747, 'epoch': 0.38}\n",
      "{'loss': 0.5869, 'grad_norm': 0.03236198052763939, 'learning_rate': 0.0001551936985312058, 'epoch': 0.38}\n",
      "{'loss': 0.5626, 'grad_norm': 0.031225353479385376, 'learning_rate': 0.00015446390350150273, 'epoch': 0.39}\n",
      "{'loss': 0.5609, 'grad_norm': 0.03162957727909088, 'learning_rate': 0.0001537299608346824, 'epoch': 0.39}\n",
      "{'loss': 0.5587, 'grad_norm': 0.030769381672143936, 'learning_rate': 0.0001529919264233205, 'epoch': 0.39}\n",
      "{'loss': 0.5592, 'grad_norm': 0.030587665736675262, 'learning_rate': 0.0001522498564715949, 'epoch': 0.39}\n",
      "{'loss': 0.5773, 'grad_norm': 0.029348138719797134, 'learning_rate': 0.00015150380749100545, 'epoch': 0.4}\n",
      "{'loss': 0.5636, 'grad_norm': 0.03253476694226265, 'learning_rate': 0.00015075383629607042, 'epoch': 0.4}\n",
      "{'loss': 0.5912, 'grad_norm': 0.03118041157722473, 'learning_rate': 0.00015000000000000001, 'epoch': 0.4}\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 800/2000 [7:05:29<10:24:23, 31.22s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5881782174110413, 'eval_runtime': 83.3378, 'eval_samples_per_second': 6.888, 'eval_steps_per_second': 0.432, 'epoch': 0.4}\n",
      "{'loss': 0.5595, 'grad_norm': 0.031205518171191216, 'learning_rate': 0.00014924235601034672, 'epoch': 0.4}\n",
      "{'loss': 0.5742, 'grad_norm': 0.03185031935572624, 'learning_rate': 0.00014848096202463372, 'epoch': 0.41}\n",
      "{'loss': 0.5706, 'grad_norm': 0.03068492002785206, 'learning_rate': 0.00014771587602596084, 'epoch': 0.41}\n",
      "{'loss': 0.5898, 'grad_norm': 0.030087944120168686, 'learning_rate': 0.00014694715627858908, 'epoch': 0.41}\n",
      "{'loss': 0.539, 'grad_norm': 0.0373629555106163, 'learning_rate': 0.00014617486132350343, 'epoch': 0.41}\n",
      "{'loss': 0.9235, 'grad_norm': 0.03129191696643829, 'learning_rate': 0.00014539904997395468, 'epoch': 0.41}\n",
      "{'loss': 0.9076, 'grad_norm': 0.03151792287826538, 'learning_rate': 0.00014461978131098088, 'epoch': 0.42}\n",
      "{'loss': 0.9787, 'grad_norm': 0.034512169659137726, 'learning_rate': 0.00014383711467890774, 'epoch': 0.42}\n",
      "{'loss': 0.7981, 'grad_norm': 0.03043893724679947, 'learning_rate': 0.00014305110968082952, 'epoch': 0.42}\n",
      "{'loss': 0.5519, 'grad_norm': 0.030384954065084457, 'learning_rate': 0.00014226182617406996, 'epoch': 0.42}\n",
      "{'loss': 0.5428, 'grad_norm': 0.03660266846418381, 'learning_rate': 0.00014146932426562392, 'epoch': 0.43}\n",
      "{'loss': 0.5951, 'grad_norm': 0.0373799093067646, 'learning_rate': 0.00014067366430758004, 'epoch': 0.43}\n",
      "{'loss': 0.6236, 'grad_norm': 0.034559786319732666, 'learning_rate': 0.00013987490689252463, 'epoch': 0.43}\n",
      "{'loss': 0.6469, 'grad_norm': 0.029126841574907303, 'learning_rate': 0.00013907311284892736, 'epoch': 0.43}\n",
      "{'loss': 0.5796, 'grad_norm': 0.03593658655881882, 'learning_rate': 0.000138268343236509, 'epoch': 0.44}\n",
      "{'loss': 0.5978, 'grad_norm': 0.028489770367741585, 'learning_rate': 0.00013746065934159123, 'epoch': 0.44}\n",
      "{'loss': 0.5887, 'grad_norm': 0.0400129035115242, 'learning_rate': 0.00013665012267242974, 'epoch': 0.44}\n",
      "{'loss': 0.5912, 'grad_norm': 0.03223550692200661, 'learning_rate': 0.00013583679495453, 'epoch': 0.45}\n",
      "{'loss': 0.5565, 'grad_norm': 0.03332335874438286, 'learning_rate': 0.00013502073812594675, 'epoch': 0.45}\n",
      "{'loss': 0.5544, 'grad_norm': 0.031098386272788048, 'learning_rate': 0.00013420201433256689, 'epoch': 0.45}\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 900/2000 [7:58:49<9:32:05, 31.20s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5838550925254822, 'eval_runtime': 81.8858, 'eval_samples_per_second': 7.01, 'eval_steps_per_second': 0.44, 'epoch': 0.45}\n",
      "{'loss': 0.5666, 'grad_norm': 0.03471985086798668, 'learning_rate': 0.0001333806859233771, 'epoch': 0.45}\n",
      "{'loss': 0.5817, 'grad_norm': 0.02915988489985466, 'learning_rate': 0.00013255681544571568, 'epoch': 0.46}\n",
      "{'loss': 0.5831, 'grad_norm': 0.03174344077706337, 'learning_rate': 0.00013173046564050924, 'epoch': 0.46}\n",
      "{'loss': 0.6144, 'grad_norm': 0.0313197560608387, 'learning_rate': 0.00013090169943749476, 'epoch': 0.46}\n",
      "{'loss': 0.5917, 'grad_norm': 0.028940320014953613, 'learning_rate': 0.00013007057995042732, 'epoch': 0.46}\n",
      "{'loss': 0.5679, 'grad_norm': 0.03176621347665787, 'learning_rate': 0.00012923717047227368, 'epoch': 0.47}\n",
      "{'loss': 0.5781, 'grad_norm': 0.0302632637321949, 'learning_rate': 0.00012840153447039228, 'epoch': 0.47}\n",
      "{'loss': 0.5739, 'grad_norm': 0.05342119187116623, 'learning_rate': 0.0001275637355816999, 'epoch': 0.47}\n",
      "{'loss': 0.5797, 'grad_norm': 0.0336737222969532, 'learning_rate': 0.00012672383760782568, 'epoch': 0.47}\n",
      "{'loss': 0.588, 'grad_norm': 0.03469885513186455, 'learning_rate': 0.00012588190451025207, 'epoch': 0.47}\n",
      "{'loss': 0.5849, 'grad_norm': 0.0334843248128891, 'learning_rate': 0.00012503800040544416, 'epoch': 0.48}\n",
      "{'loss': 0.5877, 'grad_norm': 0.032595206052064896, 'learning_rate': 0.00012419218955996676, 'epoch': 0.48}\n",
      "{'loss': 0.5642, 'grad_norm': 0.033184610307216644, 'learning_rate': 0.00012334453638559057, 'epoch': 0.48}\n",
      "{'loss': 0.5709, 'grad_norm': 0.0332832969725132, 'learning_rate': 0.0001224951054343865, 'epoch': 0.48}\n",
      "{'loss': 0.5709, 'grad_norm': 0.03640323504805565, 'learning_rate': 0.00012164396139381029, 'epoch': 0.49}\n",
      "{'loss': 0.5633, 'grad_norm': 0.03853312134742737, 'learning_rate': 0.00012079116908177593, 'epoch': 0.49}\n",
      "{'loss': 0.5697, 'grad_norm': 0.042388662695884705, 'learning_rate': 0.00011993679344171973, 'epoch': 0.49}\n",
      "{'loss': 0.5496, 'grad_norm': 0.0352887287735939, 'learning_rate': 0.00011908089953765449, 'epoch': 0.49}\n",
      "{'loss': 0.5849, 'grad_norm': 0.04099946469068527, 'learning_rate': 0.00011822355254921478, 'epoch': 0.5}\n",
      "{'loss': 0.5653, 'grad_norm': 0.04104343429207802, 'learning_rate': 0.00011736481776669306, 'epoch': 0.5}\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 1000/2000 [8:52:07<8:37:11, 31.03s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5795723795890808, 'eval_runtime': 83.4727, 'eval_samples_per_second': 6.877, 'eval_steps_per_second': 0.431, 'epoch': 0.5}\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 1000/2000 [8:53:31<8:37:11, 31.03s/it]Saving model checkpoint to deepseek-coder-6.7b-base-APR-finetuning/checkpoint-1000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "{'loss': 0.5821, 'grad_norm': 0.03483836352825165, 'learning_rate': 0.00011650476058606777, 'epoch': 0.5}\n",
      "{'loss': 0.5753, 'grad_norm': 0.03185306116938591, 'learning_rate': 0.0001156434465040231, 'epoch': 0.51}\n",
      "{'loss': 0.563, 'grad_norm': 0.033738330006599426, 'learning_rate': 0.00011478094111296109, 'epoch': 0.51}\n",
      "{'loss': 0.5786, 'grad_norm': 0.03815177083015442, 'learning_rate': 0.00011391731009600654, 'epoch': 0.51}\n",
      "{'loss': 0.5584, 'grad_norm': 0.036776915192604065, 'learning_rate': 0.00011305261922200519, 'epoch': 0.51}\n",
      "{'loss': 0.5492, 'grad_norm': 0.036678142845630646, 'learning_rate': 0.00011218693434051475, 'epoch': 0.52}\n",
      "{'loss': 0.5479, 'grad_norm': 0.04249840974807739, 'learning_rate': 0.0001113203213767907, 'epoch': 0.52}\n",
      "{'loss': 0.5728, 'grad_norm': 0.054118428379297256, 'learning_rate': 0.00011045284632676536, 'epoch': 0.52}\n",
      "{'loss': 0.5584, 'grad_norm': 0.035232383757829666, 'learning_rate': 0.00010958457525202241, 'epoch': 0.52}\n",
      "{'loss': 0.5715, 'grad_norm': 0.030707117170095444, 'learning_rate': 0.00010871557427476583, 'epoch': 0.53}\n",
      "{'loss': 0.6072, 'grad_norm': 0.03246069326996803, 'learning_rate': 0.0001078459095727845, 'epoch': 0.53}\n",
      "{'loss': 0.5888, 'grad_norm': 0.03141254559159279, 'learning_rate': 0.00010697564737441252, 'epoch': 0.53}\n",
      "{'loss': 0.5798, 'grad_norm': 0.03231528401374817, 'learning_rate': 0.00010610485395348571, 'epoch': 0.53}\n",
      "{'loss': 0.5816, 'grad_norm': 0.039554715156555176, 'learning_rate': 0.0001052335956242944, 'epoch': 0.54}\n",
      "{'loss': 0.566, 'grad_norm': 0.0331634059548378, 'learning_rate': 0.00010436193873653361, 'epoch': 0.54}\n",
      "{'loss': 0.5958, 'grad_norm': 0.030750444158911705, 'learning_rate': 0.00010348994967025012, 'epoch': 0.54}\n",
      "{'loss': 0.5442, 'grad_norm': 0.03688112273812294, 'learning_rate': 0.00010261769483078733, 'epoch': 0.54}\n",
      "{'loss': 0.5468, 'grad_norm': 0.034900497645139694, 'learning_rate': 0.00010174524064372837, 'epoch': 0.55}\n",
      "{'loss': 0.5573, 'grad_norm': 0.035383373498916626, 'learning_rate': 0.0001008726535498374, 'epoch': 0.55}\n",
      "{'loss': 0.562, 'grad_norm': 0.0359148234128952, 'learning_rate': 0.0001, 'epoch': 0.55}\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 1100/2000 [9:45:28<7:48:19, 31.22s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5781816840171814, 'eval_runtime': 83.1411, 'eval_samples_per_second': 6.904, 'eval_steps_per_second': 0.433, 'epoch': 0.55}\n",
      "{'loss': 0.5646, 'grad_norm': 0.03444800153374672, 'learning_rate': 9.912734645016263e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5639, 'grad_norm': 0.04562484100461006, 'learning_rate': 9.825475935627165e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5748, 'grad_norm': 0.0362754724919796, 'learning_rate': 9.73823051692127e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5689, 'grad_norm': 0.036672066897153854, 'learning_rate': 9.651005032974994e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5533, 'grad_norm': 0.036514535546302795, 'learning_rate': 9.563806126346642e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5704, 'grad_norm': 0.033117908984422684, 'learning_rate': 9.476640437570562e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5822, 'grad_norm': 0.03496066480875015, 'learning_rate': 9.38951460465143e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5849, 'grad_norm': 0.032751601189374924, 'learning_rate': 9.302435262558747e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5698, 'grad_norm': 0.03443493694067001, 'learning_rate': 9.215409042721552e-05, 'epoch': 0.57}\n",
      "{'loss': 0.568, 'grad_norm': 0.03400815278291702, 'learning_rate': 9.128442572523417e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5519, 'grad_norm': 0.036179449409246445, 'learning_rate': 9.04154247479776e-05, 'epoch': 0.58}\n",
      "{'loss': 0.584, 'grad_norm': 0.03350215032696724, 'learning_rate': 8.954715367323468e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5667, 'grad_norm': 0.03822579234838486, 'learning_rate': 8.867967862320934e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5829, 'grad_norm': 0.03412100300192833, 'learning_rate': 8.781306565948528e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5776, 'grad_norm': 0.03730824589729309, 'learning_rate': 8.694738077799488e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5402, 'grad_norm': 0.047602877020835876, 'learning_rate': 8.608268990399349e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5318, 'grad_norm': 0.03517372906208038, 'learning_rate': 8.521905888703893e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5234, 'grad_norm': 0.05320705100893974, 'learning_rate': 8.435655349597689e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5439, 'grad_norm': 0.036361418664455414, 'learning_rate': 8.349523941393224e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5724, 'grad_norm': 0.03951964154839516, 'learning_rate': 8.263518223330697e-05, 'epoch': 0.6}\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 1200/2000 [10:38:47<6:56:00, 31.20s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5757832527160645, 'eval_runtime': 81.7674, 'eval_samples_per_second': 7.02, 'eval_steps_per_second': 0.44, 'epoch': 0.6}\n",
      "{'loss': 0.5569, 'grad_norm': 0.0398821197450161, 'learning_rate': 8.177644745078526e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5712, 'grad_norm': 0.0378047376871109, 'learning_rate': 8.091910046234552e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5879, 'grad_norm': 0.03952109441161156, 'learning_rate': 8.00632065582803e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5547, 'grad_norm': 0.041396550834178925, 'learning_rate': 7.920883091822408e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5682, 'grad_norm': 0.03934174403548241, 'learning_rate': 7.835603860618972e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5866, 'grad_norm': 0.0411599725484848, 'learning_rate': 7.750489456561352e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5888, 'grad_norm': 0.03523790463805199, 'learning_rate': 7.66554636144095e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5759, 'grad_norm': 0.0379929393529892, 'learning_rate': 7.580781044003324e-05, 'epoch': 0.62}\n",
      "{'loss': 0.549, 'grad_norm': 0.03724204748868942, 'learning_rate': 7.496199959455584e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5609, 'grad_norm': 0.04277399554848671, 'learning_rate': 7.411809548974792e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5248, 'grad_norm': 0.041380375623703, 'learning_rate': 7.327616239217431e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5306, 'grad_norm': 0.035562071949243546, 'learning_rate': 7.243626441830009e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5458, 'grad_norm': 0.039636604487895966, 'learning_rate': 7.159846552960774e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5504, 'grad_norm': 0.0407918244600296, 'learning_rate': 7.076282952772633e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5321, 'grad_norm': 0.05268962308764458, 'learning_rate': 6.992942004957271e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5552, 'grad_norm': 0.037479035556316376, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5662, 'grad_norm': 0.039382483810186386, 'learning_rate': 6.826953435949081e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5367, 'grad_norm': 0.047033000737428665, 'learning_rate': 6.744318455428436e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5408, 'grad_norm': 0.03710607439279556, 'learning_rate': 6.661931407662292e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5307, 'grad_norm': 0.04010377451777458, 'learning_rate': 6.579798566743314e-05, 'epoch': 0.65}\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 1300/2000 [11:32:04<6:02:48, 31.10s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5737899541854858, 'eval_runtime': 83.0338, 'eval_samples_per_second': 6.913, 'eval_steps_per_second': 0.434, 'epoch': 0.65}\n",
      "{'loss': 0.5648, 'grad_norm': 0.03378243371844292, 'learning_rate': 6.497926187405326e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5653, 'grad_norm': 0.038368482142686844, 'learning_rate': 6.416320504546997e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5342, 'grad_norm': 0.03952321782708168, 'learning_rate': 6.334987732757029e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5565, 'grad_norm': 0.03983357548713684, 'learning_rate': 6.25393406584088e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5446, 'grad_norm': 0.04212743043899536, 'learning_rate': 6.173165676349103e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5365, 'grad_norm': 0.040227629244327545, 'learning_rate': 6.092688715107264e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5441, 'grad_norm': 0.03610984981060028, 'learning_rate': 6.012509310747538e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5658, 'grad_norm': 0.035894062370061874, 'learning_rate': 5.9326335692419995e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5773, 'grad_norm': 0.037476323544979095, 'learning_rate': 5.853067573437612e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5861, 'grad_norm': 0.03970889002084732, 'learning_rate': 5.773817382593008e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5207, 'grad_norm': 0.04679454490542412, 'learning_rate': 5.694889031917047e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5049, 'grad_norm': 0.0457850880920887, 'learning_rate': 5.616288532109225e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5091, 'grad_norm': 0.04396308586001396, 'learning_rate': 5.5380218689019125e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5537, 'grad_norm': 0.03972550854086876, 'learning_rate': 5.4600950026045326e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5438, 'grad_norm': 0.0395524837076664, 'learning_rate': 5.382513867649663e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5494, 'grad_norm': 0.03998464718461037, 'learning_rate': 5.305284372141095e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5451, 'grad_norm': 0.037496503442525864, 'learning_rate': 5.2284123974039154e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5359, 'grad_norm': 0.041824981570243835, 'learning_rate': 5.15190379753663e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5422, 'grad_norm': 0.04068977013230324, 'learning_rate': 5.07576439896533e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5275, 'grad_norm': 0.0397055521607399, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.7}\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 1400/2000 [12:25:25<5:12:45, 31.28s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5731105208396912, 'eval_runtime': 83.5964, 'eval_samples_per_second': 6.866, 'eval_steps_per_second': 0.431, 'epoch': 0.7}\n",
      "{'loss': 0.5413, 'grad_norm': 0.045265067368745804, 'learning_rate': 4.924616370392961e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5517, 'grad_norm': 0.04012129828333855, 'learning_rate': 4.8496192508994576e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6449, 'grad_norm': 0.0448087640106678, 'learning_rate': 4.7750143528405126e-05, 'epoch': 0.71}\n",
      "{'loss': 0.948, 'grad_norm': 0.04232829809188843, 'learning_rate': 4.700807357667952e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9863, 'grad_norm': 0.038547586649656296, 'learning_rate': 4.6270039165317605e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9121, 'grad_norm': 0.03857323154807091, 'learning_rate': 4.5536096498497295e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5371, 'grad_norm': 0.03900923579931259, 'learning_rate': 4.480630146879419e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5454, 'grad_norm': 0.03896308317780495, 'learning_rate': 4.4080709652925336e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5351, 'grad_norm': 0.0390947125852108, 'learning_rate': 4.335937630751674e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5463, 'grad_norm': 0.04081578925251961, 'learning_rate': 4.264235636489542e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6015, 'grad_norm': 0.03653842955827713, 'learning_rate': 4.1929704428906026e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5589, 'grad_norm': 0.03966478258371353, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5865, 'grad_norm': 0.04184877127408981, 'learning_rate': 4.0517721324865884e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5771, 'grad_norm': 0.03870512545108795, 'learning_rate': 3.981849768479517e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5659, 'grad_norm': 0.037671491503715515, 'learning_rate': 3.9123857099127936e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5698, 'grad_norm': 0.03872383385896683, 'learning_rate': 3.843385246743417e-05, 'epoch': 0.74}\n",
      "{'loss': 0.544, 'grad_norm': 0.03849802911281586, 'learning_rate': 3.774853633623806e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5531, 'grad_norm': 0.036916252225637436, 'learning_rate': 3.7067960895016275e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5411, 'grad_norm': 0.037394504994153976, 'learning_rate': 3.6392177972223594e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5432, 'grad_norm': 0.04611803591251373, 'learning_rate': 3.5721239031346066e-05, 'epoch': 0.75}\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 1500/2000 [13:18:44<4:20:23, 31.25s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5714420080184937, 'eval_runtime': 83.7845, 'eval_samples_per_second': 6.851, 'eval_steps_per_second': 0.43, 'epoch': 0.75}\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 1500/2000 [13:20:08<4:20:23, 31.25s/it]Saving model checkpoint to deepseek-coder-6.7b-base-APR-finetuning/checkpoint-1500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "{'loss': 0.5688, 'grad_norm': 0.03842226415872574, 'learning_rate': 3.5055195166981645e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5606, 'grad_norm': 0.038099952042102814, 'learning_rate': 3.439409710094929e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5492, 'grad_norm': 0.03919156640768051, 'learning_rate': 3.373799517842627e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5628, 'grad_norm': 0.04614035785198212, 'learning_rate': 3.308693936411421e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5509, 'grad_norm': 0.04120911657810211, 'learning_rate': 3.244097923843398e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5654, 'grad_norm': 0.040572941303253174, 'learning_rate': 3.1800163993750166e-05, 'epoch': 0.77}\n",
      "{'loss': 0.571, 'grad_norm': 0.041021350771188736, 'learning_rate': 3.116454243062459e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5874, 'grad_norm': 0.03838110342621803, 'learning_rate': 3.053416295410026e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5315, 'grad_norm': 0.04455128684639931, 'learning_rate': 2.9909073570014912e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5467, 'grad_norm': 0.04099331051111221, 'learning_rate': 2.9289321881345254e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5588, 'grad_norm': 0.05045544356107712, 'learning_rate': 2.8674955084581857e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5375, 'grad_norm': 0.039499636739492416, 'learning_rate': 2.8066019966134904e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5426, 'grad_norm': 0.03953981399536133, 'learning_rate': 2.746256289877126e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5262, 'grad_norm': 0.042625948786735535, 'learning_rate': 2.6864629838082956e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5674, 'grad_norm': 0.0409577377140522, 'learning_rate': 2.6272266318987603e-05, 'epoch': 0.79}\n",
      "{'loss': 0.532, 'grad_norm': 0.043408725410699844, 'learning_rate': 2.5685517452260567e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5414, 'grad_norm': 0.04685100540518761, 'learning_rate': 2.5104427921099782e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5474, 'grad_norm': 0.043521247804164886, 'learning_rate': 2.45290419777228e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5596, 'grad_norm': 0.042300622910261154, 'learning_rate': 2.3959403439996907e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5463, 'grad_norm': 0.04413560777902603, 'learning_rate': 2.339555568810221e-05, 'epoch': 0.8}\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 1600/2000 [14:12:02<3:26:58, 31.05s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5704184770584106, 'eval_runtime': 83.718, 'eval_samples_per_second': 6.856, 'eval_steps_per_second': 0.43, 'epoch': 0.8}\n",
      "{'loss': 0.5318, 'grad_norm': 0.04557863250374794, 'learning_rate': 2.2837541661228025e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5481, 'grad_norm': 0.044327374547719955, 'learning_rate': 2.2285403854302912e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5104, 'grad_norm': 0.045148447155952454, 'learning_rate': 2.173918431475861e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5182, 'grad_norm': 0.04962356761097908, 'learning_rate': 2.119892463932781e-05, 'epoch': 0.81}\n",
      "{'loss': 0.548, 'grad_norm': 0.03936563432216644, 'learning_rate': 2.0664665970876496e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5624, 'grad_norm': 0.037637948989868164, 'learning_rate': 2.013644899527074e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5488, 'grad_norm': 0.03754802048206329, 'learning_rate': 1.9614313938278272e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5902, 'grad_norm': 0.03909704461693764, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5619, 'grad_norm': 0.03668202832341194, 'learning_rate': 1.858844816436809e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5766, 'grad_norm': 0.04143093526363373, 'learning_rate': 1.808479557110081e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5597, 'grad_norm': 0.040375422686338425, 'learning_rate': 1.7587381137798432e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5534, 'grad_norm': 0.04190666601061821, 'learning_rate': 1.7096242744495837e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5546, 'grad_norm': 0.04140929877758026, 'learning_rate': 1.661141779328319e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5455, 'grad_norm': 0.040904197841882706, 'learning_rate': 1.6132943205457606e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5602, 'grad_norm': 0.04083796590566635, 'learning_rate': 1.566085541871145e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5421, 'grad_norm': 0.044982925057411194, 'learning_rate': 1.5195190384357404e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5328, 'grad_norm': 0.041038550436496735, 'learning_rate': 1.4735983564590783e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5291, 'grad_norm': 0.0400874949991703, 'learning_rate': 1.4283269929788779e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5354, 'grad_norm': 0.042133063077926636, 'learning_rate': 1.3837083955847418e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5459, 'grad_norm': 0.04537719488143921, 'learning_rate': 1.339745962155613e-05, 'epoch': 0.85}\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1700/2000 [15:05:23<2:37:00, 31.40s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5698932409286499, 'eval_runtime': 83.6933, 'eval_samples_per_second': 6.858, 'eval_steps_per_second': 0.43, 'epoch': 0.85}\n",
      "{'loss': 0.5356, 'grad_norm': 0.04534155875444412, 'learning_rate': 1.296443040601003e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5446, 'grad_norm': 0.04739878326654434, 'learning_rate': 1.2538029286060426e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5501, 'grad_norm': 0.041398096829652786, 'learning_rate': 1.2118288733803473e-05, 'epoch': 0.86}\n",
      "{'loss': 0.57, 'grad_norm': 0.037472572177648544, 'learning_rate': 1.1705240714107302e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5708, 'grad_norm': 0.03701493889093399, 'learning_rate': 1.129891668217783e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5713, 'grad_norm': 0.04037052392959595, 'learning_rate': 1.0899347581163221e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5565, 'grad_norm': 0.05007153004407883, 'learning_rate': 1.0506563839797501e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5544, 'grad_norm': 0.03969769552350044, 'learning_rate': 1.0120595370083318e-05, 'epoch': 0.87}\n",
      "{'loss': 0.572, 'grad_norm': 0.03942840173840523, 'learning_rate': 9.74147156501396e-06, 'epoch': 0.87}\n",
      "{'loss': 0.5576, 'grad_norm': 0.0366063229739666, 'learning_rate': 9.369221296335006e-06, 'epoch': 0.88}\n",
      "{'loss': 0.5559, 'grad_norm': 0.04114425927400589, 'learning_rate': 9.00387291234569e-06, 'epoch': 0.88}\n",
      "{'loss': 0.5365, 'grad_norm': 0.03674823045730591, 'learning_rate': 8.645454235739903e-06, 'epoch': 0.88}\n",
      "{'loss': 0.5123, 'grad_norm': 0.05634726956486702, 'learning_rate': 8.293992561487596e-06, 'epoch': 0.88}\n",
      "{'loss': 0.4909, 'grad_norm': 0.04438478872179985, 'learning_rate': 7.949514654755962e-06, 'epoch': 0.89}\n",
      "{'loss': 0.5046, 'grad_norm': 0.05044187977910042, 'learning_rate': 7.612046748871327e-06, 'epoch': 0.89}\n",
      "{'loss': 0.5169, 'grad_norm': 0.04465604946017265, 'learning_rate': 7.281614543321269e-06, 'epoch': 0.89}\n",
      "{'loss': 0.5616, 'grad_norm': 0.06022702530026436, 'learning_rate': 6.958243201797554e-06, 'epoch': 0.89}\n",
      "{'loss': 0.5738, 'grad_norm': 0.04178130626678467, 'learning_rate': 6.6419573502798374e-06, 'epoch': 0.9}\n",
      "{'loss': 0.5765, 'grad_norm': 0.04176861047744751, 'learning_rate': 6.332781075160243e-06, 'epoch': 0.9}\n",
      "{'loss': 0.5751, 'grad_norm': 0.0453520305454731, 'learning_rate': 6.030737921409169e-06, 'epoch': 0.9}\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1800/2000 [15:58:42<1:44:07, 31.24s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5695624351501465, 'eval_runtime': 83.7902, 'eval_samples_per_second': 6.85, 'eval_steps_per_second': 0.43, 'epoch': 0.9}\n",
      "{'loss': 0.5448, 'grad_norm': 0.045915208756923676, 'learning_rate': 5.735850890782157e-06, 'epoch': 0.9}\n",
      "{'loss': 0.5508, 'grad_norm': 0.052070021629333496, 'learning_rate': 5.448142440068316e-06, 'epoch': 0.91}\n",
      "{'loss': 0.5464, 'grad_norm': 0.037120625376701355, 'learning_rate': 5.167634479380068e-06, 'epoch': 0.91}\n",
      "{'loss': 0.5549, 'grad_norm': 0.041145697236061096, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.91}\n",
      "{'loss': 0.5662, 'grad_norm': 0.04685737192630768, 'learning_rate': 4.628304925177318e-06, 'epoch': 0.91}\n",
      "{'loss': 0.5313, 'grad_norm': 0.0430963896214962, 'learning_rate': 4.369524403696457e-06, 'epoch': 0.92}\n",
      "{'loss': 0.5295, 'grad_norm': 0.041060637682676315, 'learning_rate': 4.118026513180695e-06, 'epoch': 0.92}\n",
      "{'loss': 0.5177, 'grad_norm': 0.04240086302161217, 'learning_rate': 3.873830406168111e-06, 'epoch': 0.92}\n",
      "{'loss': 0.5307, 'grad_norm': 0.04657292366027832, 'learning_rate': 3.6369546791377052e-06, 'epoch': 0.92}\n",
      "{'loss': 0.5347, 'grad_norm': 0.04239671677350998, 'learning_rate': 3.40741737109318e-06, 'epoch': 0.93}\n",
      "{'loss': 0.5269, 'grad_norm': 0.04263804480433464, 'learning_rate': 3.1852359621892367e-06, 'epoch': 0.93}\n",
      "{'loss': 0.5514, 'grad_norm': 0.042471662163734436, 'learning_rate': 2.970427372400353e-06, 'epoch': 0.93}\n",
      "{'loss': 0.542, 'grad_norm': 0.0450153611600399, 'learning_rate': 2.7630079602323442e-06, 'epoch': 0.93}\n",
      "{'loss': 0.53, 'grad_norm': 0.04453393444418907, 'learning_rate': 2.5629935214764865e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5515, 'grad_norm': 0.042653732001781464, 'learning_rate': 2.3703992880066638e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5295, 'grad_norm': 0.03693718463182449, 'learning_rate': 2.1852399266194314e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5502, 'grad_norm': 0.0403839536011219, 'learning_rate': 2.0075295379170412e-06, 'epoch': 0.94}\n",
      "{'loss': 0.539, 'grad_norm': 0.039712585508823395, 'learning_rate': 1.8372816552336026e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5288, 'grad_norm': 0.03618702292442322, 'learning_rate': 1.6745092436045494e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5057, 'grad_norm': 0.042369674891233444, 'learning_rate': 1.5192246987791981e-06, 'epoch': 0.95}\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1900/2000 [16:52:01<51:36, 30.96s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5696282386779785, 'eval_runtime': 83.7775, 'eval_samples_per_second': 6.851, 'eval_steps_per_second': 0.43, 'epoch': 0.95}\n",
      "{'loss': 0.5417, 'grad_norm': 0.040568795055150986, 'learning_rate': 1.3714398462768563e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5406, 'grad_norm': 0.04467274248600006, 'learning_rate': 1.231165940486234e-06, 'epoch': 0.95}\n",
      "{'loss': 0.55, 'grad_norm': 0.046837978065013885, 'learning_rate': 1.0984136638083177e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5409, 'grad_norm': 0.04006808251142502, 'learning_rate': 9.731931258429638e-07, 'epoch': 0.96}\n",
      "{'loss': 0.5617, 'grad_norm': 0.047663915902376175, 'learning_rate': 8.555138626189618e-07, 'epoch': 0.96}\n",
      "{'loss': 0.5495, 'grad_norm': 0.04133274033665657, 'learning_rate': 7.453848358678017e-07, 'epoch': 0.96}\n",
      "{'loss': 0.561, 'grad_norm': 0.04364743456244469, 'learning_rate': 6.428144323412544e-07, 'epoch': 0.97}\n",
      "{'loss': 0.5389, 'grad_norm': 0.04368449002504349, 'learning_rate': 5.478104631726711e-07, 'epoch': 0.97}\n",
      "{'loss': 0.4992, 'grad_norm': 0.0421704538166523, 'learning_rate': 4.6038016328211476e-07, 'epoch': 0.97}\n",
      "{'loss': 0.4766, 'grad_norm': 0.04527236148715019, 'learning_rate': 3.805301908254455e-07, 'epoch': 0.97}\n",
      "{'loss': 0.5315, 'grad_norm': 0.04468560963869095, 'learning_rate': 3.0826662668720364e-07, 'epoch': 0.98}\n",
      "{'loss': 0.5326, 'grad_norm': 0.042549390345811844, 'learning_rate': 2.4359497401758024e-07, 'epoch': 0.98}\n",
      "{'loss': 0.5321, 'grad_norm': 0.042383283376693726, 'learning_rate': 1.86520157813308e-07, 'epoch': 0.98}\n",
      "{'loss': 0.5289, 'grad_norm': 0.04634900763630867, 'learning_rate': 1.3704652454261668e-07, 'epoch': 0.98}\n",
      "{'loss': 0.5287, 'grad_norm': 0.044857803732156754, 'learning_rate': 9.517784181422019e-08, 'epoch': 0.99}\n",
      "{'loss': 0.5635, 'grad_norm': 0.039749979972839355, 'learning_rate': 6.09172980904238e-08, 'epoch': 0.99}\n",
      "{'loss': 0.5272, 'grad_norm': 0.058062370866537094, 'learning_rate': 3.4267502444274015e-08, 'epoch': 0.99}\n",
      "{'loss': 0.5177, 'grad_norm': 0.043727464973926544, 'learning_rate': 1.5230484360873044e-08, 'epoch': 0.99}\n",
      "{'loss': 0.5313, 'grad_norm': 0.0423637293279171, 'learning_rate': 3.807693582869032e-09, 'epoch': 1.0}\n",
      "{'loss': 0.5485, 'grad_norm': 0.0420888215303421, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [17:45:20<00:00, 31.13s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ardalaan/DeepSeekCoder6.7B_APR_FIM_finetuning\n",
    "\n",
    "%cd DeepSeekCoder6.7B_APR_FIM_finetuning/\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 WANDB_PROJECT=deepseekcoder_apr_non_fim python non_transform_finetune.py \\\n",
    "--seed 11 \\\n",
    "--model_name_or_path \"deepseek-ai/deepseek-coder-6.7b-base\" \\\n",
    "--dataset_name \"ASSERT-KTH/repairllama-datasets\"\\\n",
    "--splits \"train\"\\\n",
    "--max_seq_len 2048 \\\n",
    "--max_steps 2000 \\\n",
    "--save_steps 500 \\\n",
    "--eval_steps 100 \\\n",
    "--logging_steps 5 \\\n",
    "--log_level \"info\" \\\n",
    "--logging_strategy \"steps\" \\\n",
    "--evaluation_strategy \"steps\" \\\n",
    "--save_strategy \"steps\" \\\n",
    "--push_to_hub True\\\n",
    "--hub_private_repo True \\\n",
    "--hub_strategy \"every_save\" \\\n",
    "--bf16 True \\\n",
    "--learning_rate 2e-4 \\\n",
    "--lr_scheduler_type \"cosine\" \\\n",
    "--weight_decay 0.1 \\\n",
    "--warmup_ratio 0.1 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--output_dir \"deepseek-coder-6.7b-base-APR-finetuning\" \\\n",
    "--per_device_train_batch_size 16 \\\n",
    "--per_device_eval_batch_size 16 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--gradient_checkpointing True \\\n",
    "--use_reentrant True \\\n",
    "--dataset_text_field \"text\" \\\n",
    "--test_size 0.1 \\\n",
    "--fim_rate 0 \\\n",
    "--fim_spm_rate 0 \\\n",
    "--use_peft_lora True \\\n",
    "--lora_r 16 \\\n",
    "--lora_alpha 16 \\\n",
    "--lora_dropout 0.1 \\\n",
    "--lora_target_modules \"q_proj,k_proj,v_proj,o_proj,down_proj,up_proj,gate_proj\" \\\n",
    "--use_4bit_quantization True \\\n",
    "--use_nested_quant True \\\n",
    "--bnb_4bit_compute_dtype \"bfloat16\" \\\n",
    "--use_flash_attn True \\\n",
    "--use_unsloth True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4c52a-c709-461d-950f-8ce1f30686f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
