{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feff6428-24c9-44c6-a1bb-3c82e8b95a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers==0.0.23\n",
      "  Downloading xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.23) (1.24.1)\n",
      "Collecting torch==2.1.1 (from xformers==0.0.23)\n",
      "  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1->xformers==0.0.23) (2.1.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1->xformers==0.0.23)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.1->xformers==0.0.23) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.1->xformers==0.0.23) (1.3.0)\n",
      "Downloading xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 2.1.0+cu118\n",
      "    Can't uninstall 'torch'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 torch-2.1.1 xformers-0.0.23\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping ninja as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting ninja\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ninja\n",
      "Successfully installed ninja-1.11.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "1.11.1.git.kitware.jobserver-1\n",
      "0\n",
      "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-enojdysx/unsloth_31601360a28a4bcba373b5ab657b8171\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-enojdysx/unsloth_31601360a28a4bcba373b5ab657b8171\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit ae9e264e33c69b53dd5d533a4c5a264af4141c28\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2)\n",
      "Collecting tyro (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting transformers<4.45.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.16.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tqdm (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.6)\n",
      "Collecting wheel>=0.42.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.24.1)\n",
      "Collecting protobuf<4.0.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting huggingface-hub (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hf-transfer (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4.0)\n",
      "Collecting aiohttp (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiohttp-3.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.0)\n",
      "Collecting regex!=2019.12.17 (from transformers<4.45.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1 (from transformers<4.45.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<4.45.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading rich-13.9.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "INFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2022.12.7)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Using cached hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached tyro-0.8.11-py3-none-any.whl (105 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading aiohttp-3.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-13.9.1-py3-none-any.whl (242 kB)\n",
      "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (447 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.9.post4-py3-none-any.whl size=165293 sha256=73f54081bcb8d830ba1f15756a8d19be81ed2534fc5566f30877197920e961a6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cfvv0mzl/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "\u001b[33mWARNING: Error parsing requirements for requests: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/requests-2.31.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for typing-extensions: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/typing_extensions-4.4.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for wheel: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/wheel-0.41.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sentencepiece, pytz, xxhash, wheel, unsloth, tzdata, typing-extensions, tqdm, shtab, safetensors, requests, regex, pyarrow, protobuf, mdurl, hf-transfer, fsspec, frozenlist, docstring-parser, dill, async-timeout, aiohappyeyeballs, pandas, multiprocess, multidict, markdown-it-py, huggingface-hub, aiosignal, yarl, tokenizers, rich, tyro, transformers, aiohttp, datasets\n",
      "  Attempting uninstall: wheel\n",
      "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: wheel 0.41.3\n",
      "    Can't uninstall 'wheel'. No files were found to uninstall.\n",
      "  Attempting uninstall: typing-extensions\n",
      "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: typing-extensions 4.4.0\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: requests\n",
      "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: requests 2.31.0\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "  Attempting uninstall: fsspec\n",
      "\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: fsspec 2023.4.0\n",
      "    Can't uninstall 'fsspec'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.8 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.4.1 fsspec-2024.6.1 hf-transfer-0.1.8 huggingface-hub-0.25.1 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 protobuf-3.20.3 pyarrow-17.0.0 pytz-2024.2 regex-2024.9.11 requests-2.32.3 rich-13.9.1 safetensors-0.4.5 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2 typing-extensions-4.12.2 tyro-0.8.11 tzdata-2024.2 unsloth-2024.9.post4 wheel-0.44.0 xxhash-3.5.0 yarl-1.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: xformers<0.0.27 in /usr/local/lib/python3.10/dist-packages (0.0.23)\n",
      "Collecting trl<0.9.0\n",
      "  Using cached trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Using cached trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "Using cached peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "Using cached accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, trl, peft, accelerate\n",
      "Successfully installed accelerate-0.34.2 bitsandbytes-0.44.1 peft-0.13.0 trl-0.8.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.15.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached sentry_sdk-2.15.0-py2.py3-none-any.whl (310 kB)\n",
      "Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, click, gitdb, gitpython, wandb\n",
      "Successfully installed click-8.1.7 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.15.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install xformers==0.0.23\n",
    "\n",
    "!pip install packaging\n",
    "!pip uninstall -y ninja && pip install ninja\n",
    "!ninja --version\n",
    "!echo $?\n",
    "\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb42c25-d709-4d24-9348-a449a61ef1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929a9ff7b7e4449abc2012e81864ffb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4cca03-ef74-4c9c-8c67-0c0299ea5967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepSeekCoder6.7B_APR_FIM_finetuning'...\n",
      "remote: Enumerating objects: 71, done.\u001b[K\n",
      "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
      "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
      "remote: Total 71 (delta 37), reused 63 (delta 29), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (71/71), 32.47 KiB | 8.12 MiB/s, done.\n",
      "Resolving deltas: 100% (37/37), done.\n",
      "/workspace/DeepSeekCoder6.7B_APR_FIM_finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "tokenizer_config.json: 100%|███████████████████| 793/793 [00:00<00:00, 11.1MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.37M/1.37M [00:00<00:00, 3.78MB/s]\n",
      "README.md: 100%|███████████████████████████| 2.06k/2.06k [00:00<00:00, 10.1MB/s]\n",
      "train.jsonl: 100%|███████████████████████████| 275M/275M [00:04<00:00, 56.2MB/s]\n",
      "test.jsonl: 100%|██████████████████████████| 4.21M/4.21M [00:00<00:00, 22.7MB/s]\n",
      "Generating train split: 64643 examples [00:00, 126724.92 examples/s]\n",
      "Generating eval split: 1000 examples [00:00, 90531.06 examples/s]\n",
      "Map: 100%|██████████████████████| 64643/64643 [00:03<00:00, 19854.40 examples/s]\n",
      "Map: 100%|████████████████████████| 1000/1000 [00:00<00:00, 19856.01 examples/s]\n",
      "Size of the train set: 64643. Size of the validation set: 1000\n",
      "100%|████████████████████████████████████████| 400/400 [00:00<00:00, 740.62it/s]\n",
      "The character to token ratio of the dataset is: 3.28\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31582 > 16384). Running this sequence through the model will result in indexing errors\n",
      "A sample of valid dataset: {'input_ids': tensor([32013, 32016,  6831,  ...,   334,    72,  2069]), 'labels': tensor([32013, 32016,  6831,  ...,   334,    72,  2069])}\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.097 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.1+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.23. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [01:26<00:00, 43.32s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "deepseek-ai/deepseek-coder-6.7b-base does not have a padding token! Will use pad_token = <pad>.\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2024.9.post4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 39,976,960 || all params: 6,780,489,728 || trainable%: 0.5896\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 128,000 | Num Epochs = 9,223,372,036,854,775,807\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 2,000\n",
      " \"-____-\"     Number of trainable parameters = 39,976,960\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfake0make\u001b[0m (\u001b[33mfake0make-university-of-guilan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/DeepSeekCoder6.7B_APR_FIM_finetuning/wandb/run-20241002_113858-yhd3d95o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeepseek-coder-6.7b-base-APR-FIM-finetuning\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/fake0make-university-of-guilan/deepseekcoder_apr_fim\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/fake0make-university-of-guilan/deepseekcoder_apr_fim/runs/yhd3d95o\u001b[0m\n",
      "{'loss': 0.7521, 'grad_norm': 0.03025900386273861, 'learning_rate': 5e-06, 'epoch': 0.0}\n",
      "{'loss': 0.735, 'grad_norm': 0.027794433757662773, 'learning_rate': 1e-05, 'epoch': 0.01}\n",
      "{'loss': 0.75, 'grad_norm': 0.0323101244866848, 'learning_rate': 1.5e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7459, 'grad_norm': 0.026032444089651108, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7305, 'grad_norm': 0.04718382656574249, 'learning_rate': 2.5e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7569, 'grad_norm': 0.06071649119257927, 'learning_rate': 3e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7377, 'grad_norm': 0.04778270795941353, 'learning_rate': 3.5e-05, 'epoch': 0.02}\n",
      "{'loss': 0.742, 'grad_norm': 0.057370394468307495, 'learning_rate': 4e-05, 'epoch': 0.02}\n",
      "{'loss': 0.723, 'grad_norm': 0.057969797402620316, 'learning_rate': 4.5e-05, 'epoch': 0.02}\n",
      "{'loss': 0.731, 'grad_norm': 0.04770313575863838, 'learning_rate': 5e-05, 'epoch': 0.03}\n",
      "{'loss': 0.7187, 'grad_norm': 0.019759757444262505, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 0.7402, 'grad_norm': 0.01929924450814724, 'learning_rate': 6e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6762, 'grad_norm': 0.01926487684249878, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 0.7073, 'grad_norm': 0.020872779190540314, 'learning_rate': 7e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6708, 'grad_norm': 0.018078936263918877, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6707, 'grad_norm': 0.01640642248094082, 'learning_rate': 8e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6565, 'grad_norm': 0.019888458773493767, 'learning_rate': 8.5e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6309, 'grad_norm': 0.01527598686516285, 'learning_rate': 9e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6606, 'grad_norm': 0.017097607254981995, 'learning_rate': 9.5e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6471, 'grad_norm': 0.016928602010011673, 'learning_rate': 0.0001, 'epoch': 0.05}\n",
      "  5%|█▊                                   | 100/2000 [52:04<16:22:56, 31.04s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.6436533331871033, 'eval_runtime': 84.2855, 'eval_samples_per_second': 6.786, 'eval_steps_per_second': 0.427, 'epoch': 0.05}\n",
      "{'loss': 0.6444, 'grad_norm': 0.015177054330706596, 'learning_rate': 0.000105, 'epoch': 0.05}\n",
      "{'loss': 0.6417, 'grad_norm': 0.01602347195148468, 'learning_rate': 0.00011000000000000002, 'epoch': 0.06}\n",
      "{'loss': 0.6512, 'grad_norm': 0.017317866906523705, 'learning_rate': 0.00011499999999999999, 'epoch': 0.06}\n",
      "{'loss': 0.649, 'grad_norm': 0.016632873564958572, 'learning_rate': 0.00012, 'epoch': 0.06}\n",
      "{'loss': 0.6398, 'grad_norm': 0.017016159370541573, 'learning_rate': 0.000125, 'epoch': 0.06}\n",
      "{'loss': 0.6506, 'grad_norm': 0.03033963218331337, 'learning_rate': 0.00013000000000000002, 'epoch': 0.07}\n",
      "{'loss': 0.6534, 'grad_norm': 0.021177468821406364, 'learning_rate': 0.00013500000000000003, 'epoch': 0.07}\n",
      "{'loss': 0.6454, 'grad_norm': 0.021127142012119293, 'learning_rate': 0.00014, 'epoch': 0.07}\n",
      "{'loss': 0.6199, 'grad_norm': 0.01916959136724472, 'learning_rate': 0.000145, 'epoch': 0.07}\n",
      "{'loss': 0.6284, 'grad_norm': 0.02089708298444748, 'learning_rate': 0.00015000000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.6268, 'grad_norm': 0.023830102756619453, 'learning_rate': 0.000155, 'epoch': 0.08}\n",
      "{'loss': 0.6286, 'grad_norm': 0.023700354620814323, 'learning_rate': 0.00016, 'epoch': 0.08}\n",
      "{'loss': 0.6553, 'grad_norm': 0.021574990823864937, 'learning_rate': 0.000165, 'epoch': 0.08}\n",
      "{'loss': 0.6437, 'grad_norm': 0.02362539991736412, 'learning_rate': 0.00017, 'epoch': 0.09}\n",
      "{'loss': 0.6196, 'grad_norm': 0.023560086265206337, 'learning_rate': 0.000175, 'epoch': 0.09}\n",
      "{'loss': 0.6758, 'grad_norm': 0.024339614436030388, 'learning_rate': 0.00018, 'epoch': 0.09}\n",
      "{'loss': 0.6273, 'grad_norm': 0.02429940178990364, 'learning_rate': 0.00018500000000000002, 'epoch': 0.09}\n",
      "{'loss': 0.6393, 'grad_norm': 0.021832438185811043, 'learning_rate': 0.00019, 'epoch': 0.1}\n",
      "{'loss': 0.6063, 'grad_norm': 0.023643258959054947, 'learning_rate': 0.000195, 'epoch': 0.1}\n",
      "{'loss': 0.6132, 'grad_norm': 0.020641375333070755, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      " 10%|███▌                               | 200/2000 [1:45:30<15:31:31, 31.05s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.6207927465438843, 'eval_runtime': 84.2086, 'eval_samples_per_second': 6.793, 'eval_steps_per_second': 0.428, 'epoch': 0.1}\n",
      "{'loss': 0.6109, 'grad_norm': 0.034362517297267914, 'learning_rate': 0.00019999619230641713, 'epoch': 0.1}\n",
      "{'loss': 0.6237, 'grad_norm': 0.030335448682308197, 'learning_rate': 0.00019998476951563915, 'epoch': 0.1}\n",
      "{'loss': 0.6144, 'grad_norm': 0.025168217718601227, 'learning_rate': 0.00019996573249755572, 'epoch': 0.11}\n",
      "{'loss': 0.6285, 'grad_norm': 0.02257300354540348, 'learning_rate': 0.0001999390827019096, 'epoch': 0.11}\n",
      "{'loss': 0.6215, 'grad_norm': 0.02870318479835987, 'learning_rate': 0.0001999048221581858, 'epoch': 0.11}\n",
      "{'loss': 0.6275, 'grad_norm': 0.022377600893378258, 'learning_rate': 0.0001998629534754574, 'epoch': 0.12}\n",
      "{'loss': 0.6281, 'grad_norm': 0.02094024233520031, 'learning_rate': 0.0001998134798421867, 'epoch': 0.12}\n",
      "{'loss': 0.6157, 'grad_norm': 0.024943070486187935, 'learning_rate': 0.00019975640502598244, 'epoch': 0.12}\n",
      "{'loss': 0.627, 'grad_norm': 0.033543359488248825, 'learning_rate': 0.0001996917333733128, 'epoch': 0.12}\n",
      "{'loss': 0.6332, 'grad_norm': 0.02265988662838936, 'learning_rate': 0.00019961946980917456, 'epoch': 0.12}\n",
      "{'loss': 0.8697, 'grad_norm': 0.045366041362285614, 'learning_rate': 0.00019953961983671788, 'epoch': 0.13}\n",
      "{'loss': 1.0365, 'grad_norm': 0.15238656103610992, 'learning_rate': 0.00019945218953682734, 'epoch': 0.13}\n",
      "{'loss': 1.0068, 'grad_norm': 0.028992945328354836, 'learning_rate': 0.00019935718556765876, 'epoch': 0.13}\n",
      "{'loss': 0.8593, 'grad_norm': 0.044670090079307556, 'learning_rate': 0.00019925461516413223, 'epoch': 0.14}\n",
      "{'loss': 0.6311, 'grad_norm': 0.039634205400943756, 'learning_rate': 0.00019914448613738106, 'epoch': 0.14}\n",
      "{'loss': 0.6047, 'grad_norm': 0.02474699355661869, 'learning_rate': 0.00019902680687415705, 'epoch': 0.14}\n",
      "{'loss': 0.6359, 'grad_norm': 0.02461034618318081, 'learning_rate': 0.0001989015863361917, 'epoch': 0.14}\n",
      "{'loss': 0.6183, 'grad_norm': 0.02262524701654911, 'learning_rate': 0.00019876883405951377, 'epoch': 0.14}\n",
      "{'loss': 0.6818, 'grad_norm': 0.02713720314204693, 'learning_rate': 0.00019862856015372317, 'epoch': 0.15}\n",
      "{'loss': 0.6719, 'grad_norm': 0.025703292340040207, 'learning_rate': 0.00019848077530122083, 'epoch': 0.15}\n",
      " 15%|█████▎                             | 300/2000 [2:38:58<14:47:58, 31.34s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.6141491532325745, 'eval_runtime': 84.1113, 'eval_samples_per_second': 6.801, 'eval_steps_per_second': 0.428, 'epoch': 0.15}\n",
      "{'loss': 0.6005, 'grad_norm': 0.030042657628655434, 'learning_rate': 0.0001983254907563955, 'epoch': 0.15}\n",
      "{'loss': 0.6197, 'grad_norm': 0.024838292971253395, 'learning_rate': 0.00019816271834476642, 'epoch': 0.15}\n",
      "{'loss': 0.613, 'grad_norm': 0.024794060736894608, 'learning_rate': 0.00019799247046208297, 'epoch': 0.16}\n",
      "{'loss': 0.6393, 'grad_norm': 0.023759931325912476, 'learning_rate': 0.00019781476007338058, 'epoch': 0.16}\n",
      "{'loss': 0.6423, 'grad_norm': 0.030791861936450005, 'learning_rate': 0.00019762960071199333, 'epoch': 0.16}\n",
      "{'loss': 0.6329, 'grad_norm': 0.026065032929182053, 'learning_rate': 0.00019743700647852354, 'epoch': 0.17}\n",
      "{'loss': 0.6268, 'grad_norm': 0.023838067427277565, 'learning_rate': 0.00019723699203976766, 'epoch': 0.17}\n",
      "{'loss': 0.6299, 'grad_norm': 0.02732304111123085, 'learning_rate': 0.00019702957262759965, 'epoch': 0.17}\n",
      "{'loss': 0.6015, 'grad_norm': 0.027288131415843964, 'learning_rate': 0.0001968147640378108, 'epoch': 0.17}\n",
      "{'loss': 0.6179, 'grad_norm': 0.02902989834547043, 'learning_rate': 0.00019659258262890683, 'epoch': 0.17}\n",
      "{'loss': 0.6127, 'grad_norm': 0.023101208731532097, 'learning_rate': 0.0001963630453208623, 'epoch': 0.18}\n",
      "{'loss': 0.638, 'grad_norm': 0.028394464403390884, 'learning_rate': 0.0001961261695938319, 'epoch': 0.18}\n",
      "{'loss': 0.6324, 'grad_norm': 0.02783978544175625, 'learning_rate': 0.0001958819734868193, 'epoch': 0.18}\n",
      "{'loss': 0.6275, 'grad_norm': 0.031889528036117554, 'learning_rate': 0.00019563047559630357, 'epoch': 0.18}\n",
      "{'loss': 0.6204, 'grad_norm': 0.028202136978507042, 'learning_rate': 0.0001953716950748227, 'epoch': 0.19}\n",
      "{'loss': 0.633, 'grad_norm': 0.027349332347512245, 'learning_rate': 0.00019510565162951537, 'epoch': 0.19}\n",
      "{'loss': 0.6254, 'grad_norm': 0.02491542138159275, 'learning_rate': 0.00019483236552061994, 'epoch': 0.19}\n",
      "{'loss': 0.6352, 'grad_norm': 0.024039166048169136, 'learning_rate': 0.0001945518575599317, 'epoch': 0.2}\n",
      "{'loss': 0.6276, 'grad_norm': 0.02520841360092163, 'learning_rate': 0.00019426414910921787, 'epoch': 0.2}\n",
      "{'loss': 0.6325, 'grad_norm': 0.0323062464594841, 'learning_rate': 0.00019396926207859084, 'epoch': 0.2}\n",
      " 20%|███████                            | 400/2000 [3:32:23<13:47:28, 31.03s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.608863115310669, 'eval_runtime': 84.137, 'eval_samples_per_second': 6.798, 'eval_steps_per_second': 0.428, 'epoch': 0.2}\n",
      "{'loss': 0.6313, 'grad_norm': 0.025009123608469963, 'learning_rate': 0.00019366721892483978, 'epoch': 0.2}\n",
      "{'loss': 0.6384, 'grad_norm': 0.026303645223379135, 'learning_rate': 0.00019335804264972018, 'epoch': 0.2}\n",
      "{'loss': 0.6135, 'grad_norm': 0.02765989862382412, 'learning_rate': 0.00019304175679820247, 'epoch': 0.21}\n",
      "{'loss': 0.6378, 'grad_norm': 0.03155754879117012, 'learning_rate': 0.00019271838545667876, 'epoch': 0.21}\n",
      "{'loss': 0.6029, 'grad_norm': 0.02808675542473793, 'learning_rate': 0.0001923879532511287, 'epoch': 0.21}\n",
      "{'loss': 0.6038, 'grad_norm': 0.024645181372761726, 'learning_rate': 0.00019205048534524406, 'epoch': 0.21}\n",
      "{'loss': 0.6133, 'grad_norm': 0.028481842949986458, 'learning_rate': 0.0001917060074385124, 'epoch': 0.22}\n",
      "{'loss': 0.6019, 'grad_norm': 0.02708921767771244, 'learning_rate': 0.0001913545457642601, 'epoch': 0.22}\n",
      "{'loss': 0.616, 'grad_norm': 0.025460265576839447, 'learning_rate': 0.00019099612708765434, 'epoch': 0.22}\n",
      "{'loss': 0.6121, 'grad_norm': 0.02940065972507, 'learning_rate': 0.000190630778703665, 'epoch': 0.23}\n",
      "{'loss': 0.6024, 'grad_norm': 0.026568803936243057, 'learning_rate': 0.00019025852843498607, 'epoch': 0.23}\n",
      "{'loss': 0.6392, 'grad_norm': 0.026299044489860535, 'learning_rate': 0.0001898794046299167, 'epoch': 0.23}\n",
      "{'loss': 0.6207, 'grad_norm': 0.025251973420381546, 'learning_rate': 0.00018949343616020252, 'epoch': 0.23}\n",
      "{'loss': 0.6301, 'grad_norm': 0.029813751578330994, 'learning_rate': 0.0001891006524188368, 'epoch': 0.23}\n",
      "{'loss': 0.6148, 'grad_norm': 0.025867216289043427, 'learning_rate': 0.00018870108331782217, 'epoch': 0.24}\n",
      "{'loss': 0.6185, 'grad_norm': 0.02786979451775551, 'learning_rate': 0.00018829475928589271, 'epoch': 0.24}\n",
      "{'loss': 0.6306, 'grad_norm': 0.025986550375819206, 'learning_rate': 0.00018788171126619653, 'epoch': 0.24}\n",
      "{'loss': 0.6216, 'grad_norm': 0.027831081300973892, 'learning_rate': 0.00018746197071393958, 'epoch': 0.24}\n",
      "{'loss': 0.6035, 'grad_norm': 0.026571327820420265, 'learning_rate': 0.00018703556959398998, 'epoch': 0.25}\n",
      "{'loss': 0.6124, 'grad_norm': 0.027052395045757294, 'learning_rate': 0.00018660254037844388, 'epoch': 0.25}\n",
      " 25%|████████▊                          | 500/2000 [4:25:49<12:56:52, 31.07s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.605444610118866, 'eval_runtime': 82.8048, 'eval_samples_per_second': 6.908, 'eval_steps_per_second': 0.435, 'epoch': 0.25}\n",
      " 25%|████████▊                          | 500/2000 [4:27:12<12:56:52, 31.07s/it]Saving model checkpoint to deepseek-coder-6.7b-base-APR-FIM-finetuning/checkpoint-500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "{'loss': 0.5997, 'grad_norm': 0.030659915879368782, 'learning_rate': 0.00018616291604415258, 'epoch': 0.25}\n",
      "{'loss': 0.6044, 'grad_norm': 0.024661090224981308, 'learning_rate': 0.00018571673007021123, 'epoch': 0.26}\n",
      "{'loss': 0.6497, 'grad_norm': 0.029955079779028893, 'learning_rate': 0.00018526401643540922, 'epoch': 0.26}\n",
      "{'loss': 0.6408, 'grad_norm': 0.02835782803595066, 'learning_rate': 0.0001848048096156426, 'epoch': 0.26}\n",
      "{'loss': 0.5839, 'grad_norm': 0.029232019558548927, 'learning_rate': 0.0001843391445812886, 'epoch': 0.26}\n",
      "{'loss': 0.5888, 'grad_norm': 0.027616864070296288, 'learning_rate': 0.00018386705679454242, 'epoch': 0.27}\n",
      "{'loss': 0.5846, 'grad_norm': 0.028385840356349945, 'learning_rate': 0.00018338858220671682, 'epoch': 0.27}\n",
      "{'loss': 0.6118, 'grad_norm': 0.030704360455274582, 'learning_rate': 0.00018290375725550417, 'epoch': 0.27}\n",
      "{'loss': 0.6237, 'grad_norm': 0.027278173714876175, 'learning_rate': 0.00018241261886220154, 'epoch': 0.27}\n",
      "{'loss': 0.6077, 'grad_norm': 0.026368124410510063, 'learning_rate': 0.0001819152044288992, 'epoch': 0.28}\n",
      "{'loss': 0.6033, 'grad_norm': 0.02680114656686783, 'learning_rate': 0.00018141155183563193, 'epoch': 0.28}\n",
      "{'loss': 0.6325, 'grad_norm': 0.03119836188852787, 'learning_rate': 0.00018090169943749476, 'epoch': 0.28}\n",
      "{'loss': 0.6285, 'grad_norm': 0.02921101823449135, 'learning_rate': 0.00018038568606172173, 'epoch': 0.28}\n",
      "{'loss': 0.6344, 'grad_norm': 0.027950188145041466, 'learning_rate': 0.00017986355100472928, 'epoch': 0.28}\n",
      "{'loss': 0.6061, 'grad_norm': 0.02646767534315586, 'learning_rate': 0.00017933533402912354, 'epoch': 0.29}\n",
      "{'loss': 0.6133, 'grad_norm': 0.03207600489258766, 'learning_rate': 0.00017880107536067218, 'epoch': 0.29}\n",
      "{'loss': 0.6137, 'grad_norm': 0.02909110300242901, 'learning_rate': 0.0001782608156852414, 'epoch': 0.29}\n",
      "{'loss': 0.6083, 'grad_norm': 0.02773224376142025, 'learning_rate': 0.0001777145961456971, 'epoch': 0.29}\n",
      "{'loss': 0.5916, 'grad_norm': 0.029825616627931595, 'learning_rate': 0.00017716245833877201, 'epoch': 0.3}\n",
      "{'loss': 0.5842, 'grad_norm': 0.0329488106071949, 'learning_rate': 0.0001766044443118978, 'epoch': 0.3}\n",
      " 30%|██████████▌                        | 600/2000 [5:19:16<12:13:08, 31.42s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.6023371815681458, 'eval_runtime': 84.1154, 'eval_samples_per_second': 6.8, 'eval_steps_per_second': 0.428, 'epoch': 0.3}\n",
      "{'loss': 0.5878, 'grad_norm': 0.03682061657309532, 'learning_rate': 0.0001760405965600031, 'epoch': 0.3}\n",
      "{'loss': 0.5988, 'grad_norm': 0.030116165056824684, 'learning_rate': 0.00017547095802227723, 'epoch': 0.3}\n",
      "{'loss': 0.6019, 'grad_norm': 0.0346134714782238, 'learning_rate': 0.00017489557207890023, 'epoch': 0.31}\n",
      "{'loss': 0.6362, 'grad_norm': 0.02611209824681282, 'learning_rate': 0.00017431448254773944, 'epoch': 0.31}\n",
      "{'loss': 0.6039, 'grad_norm': 0.028185009956359863, 'learning_rate': 0.0001737277336810124, 'epoch': 0.31}\n",
      "{'loss': 0.6276, 'grad_norm': 0.02927064523100853, 'learning_rate': 0.00017313537016191706, 'epoch': 0.32}\n",
      "{'loss': 0.6232, 'grad_norm': 0.0332147553563118, 'learning_rate': 0.00017253743710122875, 'epoch': 0.32}\n",
      "{'loss': 0.6197, 'grad_norm': 0.028362615033984184, 'learning_rate': 0.0001719339800338651, 'epoch': 0.32}\n",
      "{'loss': 0.6365, 'grad_norm': 0.027688421308994293, 'learning_rate': 0.00017132504491541818, 'epoch': 0.32}\n",
      "{'loss': 0.6184, 'grad_norm': 0.02715304121375084, 'learning_rate': 0.00017071067811865476, 'epoch': 0.33}\n",
      "{'loss': 0.6036, 'grad_norm': 0.028925493359565735, 'learning_rate': 0.0001700909264299851, 'epoch': 0.33}\n",
      "{'loss': 0.6157, 'grad_norm': 0.03217480331659317, 'learning_rate': 0.00016946583704589973, 'epoch': 0.33}\n",
      "{'loss': 0.592, 'grad_norm': 0.02999591827392578, 'learning_rate': 0.0001688354575693754, 'epoch': 0.33}\n",
      "{'loss': 0.5793, 'grad_norm': 0.02783593162894249, 'learning_rate': 0.00016819983600624986, 'epoch': 0.34}\n",
      "{'loss': 0.585, 'grad_norm': 0.030453400686383247, 'learning_rate': 0.00016755902076156604, 'epoch': 0.34}\n",
      "{'loss': 0.5945, 'grad_norm': 0.029435111209750175, 'learning_rate': 0.00016691306063588583, 'epoch': 0.34}\n",
      "{'loss': 0.6003, 'grad_norm': 0.028991417959332466, 'learning_rate': 0.00016626200482157378, 'epoch': 0.34}\n",
      "{'loss': 0.6064, 'grad_norm': 0.029760558158159256, 'learning_rate': 0.00016560590289905073, 'epoch': 0.34}\n",
      "{'loss': 0.5884, 'grad_norm': 0.03067578189074993, 'learning_rate': 0.00016494480483301836, 'epoch': 0.35}\n",
      "{'loss': 0.5537, 'grad_norm': 0.029335133731365204, 'learning_rate': 0.00016427876096865394, 'epoch': 0.35}\n",
      " 35%|████████████▎                      | 700/2000 [6:12:41<11:13:47, 31.10s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5982220768928528, 'eval_runtime': 84.0172, 'eval_samples_per_second': 6.808, 'eval_steps_per_second': 0.428, 'epoch': 0.35}\n",
      "{'loss': 0.6022, 'grad_norm': 0.03296176344156265, 'learning_rate': 0.0001636078220277764, 'epoch': 0.35}\n",
      "{'loss': 0.5765, 'grad_norm': 0.031950533390045166, 'learning_rate': 0.00016293203910498376, 'epoch': 0.35}\n",
      "{'loss': 0.6196, 'grad_norm': 0.041161082684993744, 'learning_rate': 0.00016225146366376198, 'epoch': 0.36}\n",
      "{'loss': 0.5956, 'grad_norm': 0.03766920790076256, 'learning_rate': 0.0001615661475325658, 'epoch': 0.36}\n",
      "{'loss': 0.6019, 'grad_norm': 0.03710737079381943, 'learning_rate': 0.00016087614290087208, 'epoch': 0.36}\n",
      "{'loss': 0.5909, 'grad_norm': 0.03405090048909187, 'learning_rate': 0.00016018150231520486, 'epoch': 0.36}\n",
      "{'loss': 0.594, 'grad_norm': 0.03456863760948181, 'learning_rate': 0.00015948227867513415, 'epoch': 0.37}\n",
      "{'loss': 0.5824, 'grad_norm': 0.03002367913722992, 'learning_rate': 0.00015877852522924732, 'epoch': 0.37}\n",
      "{'loss': 0.601, 'grad_norm': 0.03383505716919899, 'learning_rate': 0.00015807029557109398, 'epoch': 0.37}\n",
      "{'loss': 0.6007, 'grad_norm': 0.02983241155743599, 'learning_rate': 0.0001573576436351046, 'epoch': 0.38}\n",
      "{'loss': 0.6054, 'grad_norm': 0.029615651816129684, 'learning_rate': 0.00015664062369248328, 'epoch': 0.38}\n",
      "{'loss': 0.6217, 'grad_norm': 0.031193725764751434, 'learning_rate': 0.0001559192903470747, 'epoch': 0.38}\n",
      "{'loss': 0.5998, 'grad_norm': 0.030537094920873642, 'learning_rate': 0.0001551936985312058, 'epoch': 0.38}\n",
      "{'loss': 0.5966, 'grad_norm': 0.033870719373226166, 'learning_rate': 0.00015446390350150273, 'epoch': 0.39}\n",
      "{'loss': 0.571, 'grad_norm': 0.0374566912651062, 'learning_rate': 0.0001537299608346824, 'epoch': 0.39}\n",
      "{'loss': 0.5638, 'grad_norm': 0.03822759911417961, 'learning_rate': 0.0001529919264233205, 'epoch': 0.39}\n",
      "{'loss': 0.5644, 'grad_norm': 0.0373539924621582, 'learning_rate': 0.0001522498564715949, 'epoch': 0.39}\n",
      "{'loss': 0.5605, 'grad_norm': 0.03243827447295189, 'learning_rate': 0.00015150380749100545, 'epoch': 0.4}\n",
      "{'loss': 0.5737, 'grad_norm': 0.03232165426015854, 'learning_rate': 0.00015075383629607042, 'epoch': 0.4}\n",
      "{'loss': 0.5966, 'grad_norm': 0.03128918260335922, 'learning_rate': 0.00015000000000000001, 'epoch': 0.4}\n",
      " 40%|██████████████                     | 800/2000 [7:06:09<10:23:05, 31.15s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5950639843940735, 'eval_runtime': 84.0028, 'eval_samples_per_second': 6.809, 'eval_steps_per_second': 0.429, 'epoch': 0.4}\n",
      "{'loss': 0.5894, 'grad_norm': 0.03459317237138748, 'learning_rate': 0.00014924235601034672, 'epoch': 0.4}\n",
      "{'loss': 0.5987, 'grad_norm': 0.03125156834721565, 'learning_rate': 0.00014848096202463372, 'epoch': 0.41}\n",
      "{'loss': 0.5745, 'grad_norm': 0.03700503706932068, 'learning_rate': 0.00014771587602596084, 'epoch': 0.41}\n",
      "{'loss': 0.6033, 'grad_norm': 0.03579164668917656, 'learning_rate': 0.00014694715627858908, 'epoch': 0.41}\n",
      "{'loss': 0.9724, 'grad_norm': 0.03529178723692894, 'learning_rate': 0.00014617486132350343, 'epoch': 0.41}\n",
      "{'loss': 1.0107, 'grad_norm': 0.03199468180537224, 'learning_rate': 0.00014539904997395468, 'epoch': 0.41}\n",
      "{'loss': 0.9098, 'grad_norm': 0.030621076002717018, 'learning_rate': 0.00014461978131098088, 'epoch': 0.42}\n",
      "{'loss': 0.7124, 'grad_norm': 0.03767227381467819, 'learning_rate': 0.00014383711467890774, 'epoch': 0.42}\n",
      "{'loss': 0.5881, 'grad_norm': 0.03173090144991875, 'learning_rate': 0.00014305110968082952, 'epoch': 0.42}\n",
      "{'loss': 0.573, 'grad_norm': 0.034021977335214615, 'learning_rate': 0.00014226182617406996, 'epoch': 0.42}\n",
      "{'loss': 0.6043, 'grad_norm': 0.03397063538432121, 'learning_rate': 0.00014146932426562392, 'epoch': 0.43}\n",
      "{'loss': 0.5886, 'grad_norm': 0.03279218077659607, 'learning_rate': 0.00014067366430758004, 'epoch': 0.43}\n",
      "{'loss': 0.5721, 'grad_norm': 0.03149474784731865, 'learning_rate': 0.00013987490689252463, 'epoch': 0.43}\n",
      "{'loss': 0.5869, 'grad_norm': 0.0341956727206707, 'learning_rate': 0.00013907311284892736, 'epoch': 0.43}\n",
      "{'loss': 0.6581, 'grad_norm': 0.03454296290874481, 'learning_rate': 0.000138268343236509, 'epoch': 0.44}\n",
      "{'loss': 0.6216, 'grad_norm': 0.03021078184247017, 'learning_rate': 0.00013746065934159123, 'epoch': 0.44}\n",
      "{'loss': 0.6279, 'grad_norm': 0.04071759432554245, 'learning_rate': 0.00013665012267242974, 'epoch': 0.44}\n",
      "{'loss': 0.5837, 'grad_norm': 0.03392937406897545, 'learning_rate': 0.00013583679495453, 'epoch': 0.45}\n",
      "{'loss': 0.5727, 'grad_norm': 0.033330634236335754, 'learning_rate': 0.00013502073812594675, 'epoch': 0.45}\n",
      "{'loss': 0.5757, 'grad_norm': 0.03006824664771557, 'learning_rate': 0.00013420201433256689, 'epoch': 0.45}\n",
      " 45%|████████████████▏                   | 900/2000 [7:59:37<9:38:07, 31.53s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5921048521995544, 'eval_runtime': 83.9859, 'eval_samples_per_second': 6.811, 'eval_steps_per_second': 0.429, 'epoch': 0.45}\n",
      "{'loss': 0.6026, 'grad_norm': 0.03387388959527016, 'learning_rate': 0.0001333806859233771, 'epoch': 0.45}\n",
      "{'loss': 0.6128, 'grad_norm': 0.034703463315963745, 'learning_rate': 0.00013255681544571568, 'epoch': 0.46}\n",
      "{'loss': 0.6126, 'grad_norm': 0.03798701614141464, 'learning_rate': 0.00013173046564050924, 'epoch': 0.46}\n",
      "{'loss': 0.5774, 'grad_norm': 0.03347621113061905, 'learning_rate': 0.00013090169943749476, 'epoch': 0.46}\n",
      "{'loss': 0.5807, 'grad_norm': 0.034569162875413895, 'learning_rate': 0.00013007057995042732, 'epoch': 0.46}\n",
      "{'loss': 0.5909, 'grad_norm': 0.037523653358221054, 'learning_rate': 0.00012923717047227368, 'epoch': 0.47}\n",
      "{'loss': 0.582, 'grad_norm': 0.03980953246355057, 'learning_rate': 0.00012840153447039228, 'epoch': 0.47}\n",
      "{'loss': 0.5979, 'grad_norm': 0.03437073156237602, 'learning_rate': 0.0001275637355816999, 'epoch': 0.47}\n",
      "{'loss': 0.5893, 'grad_norm': 0.038108404725790024, 'learning_rate': 0.00012672383760782568, 'epoch': 0.47}\n",
      "{'loss': 0.6129, 'grad_norm': 0.034631963819265366, 'learning_rate': 0.00012588190451025207, 'epoch': 0.47}\n",
      "{'loss': 0.5992, 'grad_norm': 0.03608188033103943, 'learning_rate': 0.00012503800040544416, 'epoch': 0.48}\n",
      "{'loss': 0.5755, 'grad_norm': 0.03740844875574112, 'learning_rate': 0.00012419218955996676, 'epoch': 0.48}\n",
      "{'loss': 0.584, 'grad_norm': 0.03985723480582237, 'learning_rate': 0.00012334453638559057, 'epoch': 0.48}\n",
      "{'loss': 0.5918, 'grad_norm': 0.0394943542778492, 'learning_rate': 0.0001224951054343865, 'epoch': 0.48}\n",
      "{'loss': 0.5749, 'grad_norm': 0.038605544716119766, 'learning_rate': 0.00012164396139381029, 'epoch': 0.49}\n",
      "{'loss': 0.6004, 'grad_norm': 0.04355549067258835, 'learning_rate': 0.00012079116908177593, 'epoch': 0.49}\n",
      "{'loss': 0.5889, 'grad_norm': 0.034324318170547485, 'learning_rate': 0.00011993679344171973, 'epoch': 0.49}\n",
      "{'loss': 0.5923, 'grad_norm': 0.04203961417078972, 'learning_rate': 0.00011908089953765449, 'epoch': 0.49}\n",
      "{'loss': 0.5966, 'grad_norm': 0.04025917500257492, 'learning_rate': 0.00011822355254921478, 'epoch': 0.5}\n",
      "{'loss': 0.5856, 'grad_norm': 0.04144919291138649, 'learning_rate': 0.00011736481776669306, 'epoch': 0.5}\n",
      " 50%|█████████████████▌                 | 1000/2000 [8:53:03<8:40:13, 31.21s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5878980159759521, 'eval_runtime': 81.535, 'eval_samples_per_second': 7.015, 'eval_steps_per_second': 0.442, 'epoch': 0.5}\n",
      " 50%|█████████████████▌                 | 1000/2000 [8:54:24<8:40:13, 31.21s/it]Saving model checkpoint to deepseek-coder-6.7b-base-APR-FIM-finetuning/checkpoint-1000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "{'loss': 0.5971, 'grad_norm': 0.04190243408083916, 'learning_rate': 0.00011650476058606777, 'epoch': 0.5}\n",
      "{'loss': 0.5667, 'grad_norm': 0.03991072252392769, 'learning_rate': 0.0001156434465040231, 'epoch': 0.51}\n",
      "{'loss': 0.558, 'grad_norm': 0.04129510745406151, 'learning_rate': 0.00011478094111296109, 'epoch': 0.51}\n",
      "{'loss': 0.576, 'grad_norm': 0.03701101616024971, 'learning_rate': 0.00011391731009600654, 'epoch': 0.51}\n",
      "{'loss': 0.5578, 'grad_norm': 0.03732290118932724, 'learning_rate': 0.00011305261922200519, 'epoch': 0.51}\n",
      "{'loss': 0.5742, 'grad_norm': 0.036756739020347595, 'learning_rate': 0.00011218693434051475, 'epoch': 0.52}\n",
      "{'loss': 0.5798, 'grad_norm': 0.03626762330532074, 'learning_rate': 0.0001113203213767907, 'epoch': 0.52}\n",
      "{'loss': 0.5599, 'grad_norm': 0.04180879518389702, 'learning_rate': 0.00011045284632676536, 'epoch': 0.52}\n",
      "{'loss': 0.5928, 'grad_norm': 0.03295349329710007, 'learning_rate': 0.00010958457525202241, 'epoch': 0.52}\n",
      "{'loss': 0.6047, 'grad_norm': 0.03504738584160805, 'learning_rate': 0.00010871557427476583, 'epoch': 0.53}\n",
      "{'loss': 0.6097, 'grad_norm': 0.031837429851293564, 'learning_rate': 0.0001078459095727845, 'epoch': 0.53}\n",
      "{'loss': 0.604, 'grad_norm': 0.03291244059801102, 'learning_rate': 0.00010697564737441252, 'epoch': 0.53}\n",
      "{'loss': 0.6062, 'grad_norm': 0.04053889960050583, 'learning_rate': 0.00010610485395348571, 'epoch': 0.53}\n",
      "{'loss': 0.5963, 'grad_norm': 0.03525613248348236, 'learning_rate': 0.0001052335956242944, 'epoch': 0.54}\n",
      "{'loss': 0.5828, 'grad_norm': 0.037451960146427155, 'learning_rate': 0.00010436193873653361, 'epoch': 0.54}\n",
      "{'loss': 0.5664, 'grad_norm': 0.0387127511203289, 'learning_rate': 0.00010348994967025012, 'epoch': 0.54}\n",
      "{'loss': 0.5695, 'grad_norm': 0.03635520115494728, 'learning_rate': 0.00010261769483078733, 'epoch': 0.54}\n",
      "{'loss': 0.5655, 'grad_norm': 0.03698529675602913, 'learning_rate': 0.00010174524064372837, 'epoch': 0.55}\n",
      "{'loss': 0.5938, 'grad_norm': 0.03334248438477516, 'learning_rate': 0.0001008726535498374, 'epoch': 0.55}\n",
      "{'loss': 0.6049, 'grad_norm': 0.037417344748973846, 'learning_rate': 0.0001, 'epoch': 0.55}\n",
      " 55%|███████████████████▎               | 1100/2000 [9:46:28<7:47:37, 31.18s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5863940715789795, 'eval_runtime': 83.8734, 'eval_samples_per_second': 6.82, 'eval_steps_per_second': 0.429, 'epoch': 0.55}\n",
      "{'loss': 0.5755, 'grad_norm': 0.035559359937906265, 'learning_rate': 9.912734645016263e-05, 'epoch': 0.55}\n",
      "{'loss': 0.547, 'grad_norm': 0.043762341141700745, 'learning_rate': 9.825475935627165e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5551, 'grad_norm': 0.03902880847454071, 'learning_rate': 9.73823051692127e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5572, 'grad_norm': 0.03963469713926315, 'learning_rate': 9.651005032974994e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5816, 'grad_norm': 0.03589979559183121, 'learning_rate': 9.563806126346642e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5914, 'grad_norm': 0.03757263720035553, 'learning_rate': 9.476640437570562e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5776, 'grad_norm': 0.04177180677652359, 'learning_rate': 9.38951460465143e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6132, 'grad_norm': 0.034495335072278976, 'learning_rate': 9.302435262558747e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5785, 'grad_norm': 0.043467096984386444, 'learning_rate': 9.215409042721552e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6035, 'grad_norm': 0.037436094135046005, 'learning_rate': 9.128442572523417e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5948, 'grad_norm': 0.03558404743671417, 'learning_rate': 9.04154247479776e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5766, 'grad_norm': 0.03349806368350983, 'learning_rate': 8.954715367323468e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5752, 'grad_norm': 0.034493930637836456, 'learning_rate': 8.867967862320934e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5794, 'grad_norm': 0.041911546140909195, 'learning_rate': 8.781306565948528e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5604, 'grad_norm': 0.041695963591337204, 'learning_rate': 8.694738077799488e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5672, 'grad_norm': 0.03568476438522339, 'learning_rate': 8.608268990399349e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5674, 'grad_norm': 0.04275992885231972, 'learning_rate': 8.521905888703893e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5566, 'grad_norm': 0.04129667952656746, 'learning_rate': 8.435655349597689e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5459, 'grad_norm': 0.04630057141184807, 'learning_rate': 8.349523941393224e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5611, 'grad_norm': 0.04342995211482048, 'learning_rate': 8.263518223330697e-05, 'epoch': 0.6}\n",
      " 60%|████████████████████▍             | 1200/2000 [10:39:56<7:01:20, 31.60s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5840713381767273, 'eval_runtime': 83.7417, 'eval_samples_per_second': 6.831, 'eval_steps_per_second': 0.43, 'epoch': 0.6}\n",
      "{'loss': 0.589, 'grad_norm': 0.038070227950811386, 'learning_rate': 8.177644745078526e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6034, 'grad_norm': 0.037665508687496185, 'learning_rate': 8.091910046234552e-05, 'epoch': 0.6}\n",
      "{'loss': 0.587, 'grad_norm': 0.038497962057590485, 'learning_rate': 8.00632065582803e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5951, 'grad_norm': 0.04229388013482094, 'learning_rate': 7.920883091822408e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5972, 'grad_norm': 0.04395042359828949, 'learning_rate': 7.835603860618972e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5963, 'grad_norm': 0.039137545973062515, 'learning_rate': 7.750489456561352e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5819, 'grad_norm': 0.03737778589129448, 'learning_rate': 7.66554636144095e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5517, 'grad_norm': 0.037759486585855484, 'learning_rate': 7.580781044003324e-05, 'epoch': 0.62}\n",
      "{'loss': 0.57, 'grad_norm': 0.038593146950006485, 'learning_rate': 7.496199959455584e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5409, 'grad_norm': 0.04151453077793121, 'learning_rate': 7.411809548974792e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5752, 'grad_norm': 0.03601555898785591, 'learning_rate': 7.327616239217431e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5586, 'grad_norm': 0.03728308901190758, 'learning_rate': 7.243626441830009e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5704, 'grad_norm': 0.053420618176460266, 'learning_rate': 7.159846552960774e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5695, 'grad_norm': 0.0450621172785759, 'learning_rate': 7.076282952772633e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5574, 'grad_norm': 0.04355716332793236, 'learning_rate': 6.992942004957271e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5194, 'grad_norm': 0.0459415540099144, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5559, 'grad_norm': 0.03878011554479599, 'learning_rate': 6.826953435949081e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5482, 'grad_norm': 0.041655126959085464, 'learning_rate': 6.744318455428436e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5608, 'grad_norm': 0.04372106492519379, 'learning_rate': 6.661931407662292e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5753, 'grad_norm': 0.039374686777591705, 'learning_rate': 6.579798566743314e-05, 'epoch': 0.65}\n",
      " 65%|██████████████████████            | 1300/2000 [11:33:23<6:05:02, 31.29s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5821101069450378, 'eval_runtime': 82.8534, 'eval_samples_per_second': 6.904, 'eval_steps_per_second': 0.435, 'epoch': 0.65}\n",
      "{'loss': 0.5854, 'grad_norm': 0.040573105216026306, 'learning_rate': 6.497926187405326e-05, 'epoch': 0.65}\n",
      "{'loss': 0.576, 'grad_norm': 0.042558763176202774, 'learning_rate': 6.416320504546997e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5646, 'grad_norm': 0.03534049913287163, 'learning_rate': 6.334987732757029e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5502, 'grad_norm': 0.03912234306335449, 'learning_rate': 6.25393406584088e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5547, 'grad_norm': 0.04148564487695694, 'learning_rate': 6.173165676349103e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5865, 'grad_norm': 0.045459046959877014, 'learning_rate': 6.092688715107264e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5828, 'grad_norm': 0.04125037416815758, 'learning_rate': 6.012509310747538e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5865, 'grad_norm': 0.04120956361293793, 'learning_rate': 5.9326335692419995e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5814, 'grad_norm': 0.04062920808792114, 'learning_rate': 5.853067573437612e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5294, 'grad_norm': 0.045581452548503876, 'learning_rate': 5.773817382593008e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5411, 'grad_norm': 0.041988782584667206, 'learning_rate': 5.694889031917047e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5337, 'grad_norm': 0.04672941938042641, 'learning_rate': 5.616288532109225e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5434, 'grad_norm': 0.04726540297269821, 'learning_rate': 5.5380218689019125e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5484, 'grad_norm': 0.038256917148828506, 'learning_rate': 5.4600950026045326e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5762, 'grad_norm': 0.04239872097969055, 'learning_rate': 5.382513867649663e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5517, 'grad_norm': 0.03831189498305321, 'learning_rate': 5.305284372141095e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5452, 'grad_norm': 0.04316778481006622, 'learning_rate': 5.2284123974039154e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5682, 'grad_norm': 0.0427091047167778, 'learning_rate': 5.15190379753663e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5434, 'grad_norm': 0.03815707936882973, 'learning_rate': 5.07576439896533e-05, 'epoch': 0.7}\n",
      "{'loss': 0.541, 'grad_norm': 0.05052545294165611, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.7}\n",
      " 70%|███████████████████████▊          | 1400/2000 [12:26:48<5:12:08, 31.21s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5809974670410156, 'eval_runtime': 83.8446, 'eval_samples_per_second': 6.822, 'eval_steps_per_second': 0.429, 'epoch': 0.7}\n",
      "{'loss': 0.601, 'grad_norm': 0.036974430084228516, 'learning_rate': 4.924616370392961e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9838, 'grad_norm': 0.04639462009072304, 'learning_rate': 4.8496192508994576e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8965, 'grad_norm': 0.039555035531520844, 'learning_rate': 4.7750143528405126e-05, 'epoch': 0.71}\n",
      "{'loss': 0.966, 'grad_norm': 0.03743605688214302, 'learning_rate': 4.700807357667952e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6601, 'grad_norm': 0.04133858159184456, 'learning_rate': 4.6270039165317605e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5382, 'grad_norm': 0.038232315331697464, 'learning_rate': 4.5536096498497295e-05, 'epoch': 0.71}\n",
      "{'loss': 0.545, 'grad_norm': 0.03773307427763939, 'learning_rate': 4.480630146879419e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5975, 'grad_norm': 0.04675038903951645, 'learning_rate': 4.4080709652925336e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6249, 'grad_norm': 0.044152773916721344, 'learning_rate': 4.335937630751674e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5921, 'grad_norm': 0.039776165038347244, 'learning_rate': 4.264235636489542e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6106, 'grad_norm': 0.041248273104429245, 'learning_rate': 4.1929704428906026e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5709, 'grad_norm': 0.0439278744161129, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5682, 'grad_norm': 0.042406924068927765, 'learning_rate': 4.0517721324865884e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5933, 'grad_norm': 0.04284318536520004, 'learning_rate': 3.981849768479517e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5394, 'grad_norm': 0.03838198259472847, 'learning_rate': 3.9123857099127936e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5484, 'grad_norm': 0.04045974463224411, 'learning_rate': 3.843385246743417e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5441, 'grad_norm': 0.04137693718075752, 'learning_rate': 3.774853633623806e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5889, 'grad_norm': 0.03942141309380531, 'learning_rate': 3.7067960895016275e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5926, 'grad_norm': 0.041002288460731506, 'learning_rate': 3.6392177972223594e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5838, 'grad_norm': 0.0470283143222332, 'learning_rate': 3.5721239031346066e-05, 'epoch': 0.75}\n",
      " 75%|█████████████████████████▌        | 1500/2000 [13:20:15<4:16:57, 30.84s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5795415043830872, 'eval_runtime': 83.7429, 'eval_samples_per_second': 6.83, 'eval_steps_per_second': 0.43, 'epoch': 0.75}\n",
      " 75%|█████████████████████████▌        | 1500/2000 [13:21:38<4:16:57, 30.84s/it]Saving model checkpoint to deepseek-coder-6.7b-base-APR-FIM-finetuning/checkpoint-1500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "{'loss': 0.5849, 'grad_norm': 0.03950074315071106, 'learning_rate': 3.5055195166981645e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5569, 'grad_norm': 0.040262412279844284, 'learning_rate': 3.439409710094929e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5561, 'grad_norm': 0.041864849627017975, 'learning_rate': 3.373799517842627e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5815, 'grad_norm': 0.04014619439840317, 'learning_rate': 3.308693936411421e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5773, 'grad_norm': 0.03654221072793007, 'learning_rate': 3.244097923843398e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5783, 'grad_norm': 0.03780192509293556, 'learning_rate': 3.1800163993750166e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5691, 'grad_norm': 0.04053491726517677, 'learning_rate': 3.116454243062459e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5573, 'grad_norm': 0.04248134046792984, 'learning_rate': 3.053416295410026e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5708, 'grad_norm': 0.042511578649282455, 'learning_rate': 2.9909073570014912e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5592, 'grad_norm': 0.04399676248431206, 'learning_rate': 2.9289321881345254e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5633, 'grad_norm': 0.04396047070622444, 'learning_rate': 2.8674955084581857e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5585, 'grad_norm': 0.04450216144323349, 'learning_rate': 2.8066019966134904e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5609, 'grad_norm': 0.0455886535346508, 'learning_rate': 2.746256289877126e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5661, 'grad_norm': 0.043601419776678085, 'learning_rate': 2.6864629838082956e-05, 'epoch': 0.79}\n",
      "{'loss': 0.58, 'grad_norm': 0.046590980142354965, 'learning_rate': 2.6272266318987603e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5645, 'grad_norm': 0.04047902673482895, 'learning_rate': 2.5685517452260567e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5611, 'grad_norm': 0.045594435185194016, 'learning_rate': 2.5104427921099782e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5579, 'grad_norm': 0.046092305332422256, 'learning_rate': 2.45290419777228e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5711, 'grad_norm': 0.04509654641151428, 'learning_rate': 2.3959403439996907e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5326, 'grad_norm': 0.04733772203326225, 'learning_rate': 2.339555568810221e-05, 'epoch': 0.8}\n",
      " 80%|███████████████████████████▏      | 1600/2000 [14:13:44<3:28:48, 31.32s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5789250731468201, 'eval_runtime': 83.6055, 'eval_samples_per_second': 6.842, 'eval_steps_per_second': 0.431, 'epoch': 0.8}\n",
      "{'loss': 0.5571, 'grad_norm': 0.051052313297986984, 'learning_rate': 2.2837541661228025e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5135, 'grad_norm': 0.05031363666057587, 'learning_rate': 2.2285403854302912e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5654, 'grad_norm': 0.04296620190143585, 'learning_rate': 2.173918431475861e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5671, 'grad_norm': 0.042444437742233276, 'learning_rate': 2.119892463932781e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5552, 'grad_norm': 0.04235220327973366, 'learning_rate': 2.0664665970876496e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5896, 'grad_norm': 0.040049877017736435, 'learning_rate': 2.013644899527074e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5939, 'grad_norm': 0.0387832336127758, 'learning_rate': 1.9614313938278272e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5872, 'grad_norm': 0.03896363824605942, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.82}\n",
      "{'loss': 0.589, 'grad_norm': 0.04218792915344238, 'learning_rate': 1.858844816436809e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5803, 'grad_norm': 0.041848961263895035, 'learning_rate': 1.808479557110081e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5818, 'grad_norm': 0.040962301194667816, 'learning_rate': 1.7587381137798432e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5865, 'grad_norm': 0.04068642482161522, 'learning_rate': 1.7096242744495837e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5639, 'grad_norm': 0.04455775022506714, 'learning_rate': 1.661141779328319e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5254, 'grad_norm': 0.040635377168655396, 'learning_rate': 1.6132943205457606e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5375, 'grad_norm': 0.043387848883867264, 'learning_rate': 1.566085541871145e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5784, 'grad_norm': 0.04286482185125351, 'learning_rate': 1.5195190384357404e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5548, 'grad_norm': 0.03871019184589386, 'learning_rate': 1.4735983564590783e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5609, 'grad_norm': 0.04102785885334015, 'learning_rate': 1.4283269929788779e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5607, 'grad_norm': 0.04284467175602913, 'learning_rate': 1.3837083955847418e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5292, 'grad_norm': 0.04819435253739357, 'learning_rate': 1.339745962155613e-05, 'epoch': 0.85}\n",
      " 85%|████████████████████████████▉     | 1700/2000 [15:07:08<2:35:59, 31.20s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5784437656402588, 'eval_runtime': 83.7527, 'eval_samples_per_second': 6.83, 'eval_steps_per_second': 0.43, 'epoch': 0.85}\n",
      "{'loss': 0.5461, 'grad_norm': 0.054155174642801285, 'learning_rate': 1.296443040601003e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5855, 'grad_norm': 0.03919841721653938, 'learning_rate': 1.2538029286060426e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5875, 'grad_norm': 0.040882132947444916, 'learning_rate': 1.2118288733803473e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5762, 'grad_norm': 0.03547467291355133, 'learning_rate': 1.1705240714107302e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5712, 'grad_norm': 0.04035647585988045, 'learning_rate': 1.129891668217783e-05, 'epoch': 0.86}\n",
      "{'loss': 0.569, 'grad_norm': 0.03872836381196976, 'learning_rate': 1.0899347581163221e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5739, 'grad_norm': 0.044923797249794006, 'learning_rate': 1.0506563839797501e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5722, 'grad_norm': 0.03877276927232742, 'learning_rate': 1.0120595370083318e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5829, 'grad_norm': 0.03806367516517639, 'learning_rate': 9.74147156501396e-06, 'epoch': 0.87}\n",
      "{'loss': 0.5603, 'grad_norm': 0.04147129878401756, 'learning_rate': 9.369221296335006e-06, 'epoch': 0.88}\n",
      "{'loss': 0.5591, 'grad_norm': 0.039943449199199677, 'learning_rate': 9.00387291234569e-06, 'epoch': 0.88}\n",
      "{'loss': 0.5388, 'grad_norm': 0.03930211067199707, 'learning_rate': 8.645454235739903e-06, 'epoch': 0.88}\n",
      "{'loss': 0.5338, 'grad_norm': 0.039225559681653976, 'learning_rate': 8.293992561487596e-06, 'epoch': 0.88}\n",
      "{'loss': 0.553, 'grad_norm': 0.05043797567486763, 'learning_rate': 7.949514654755962e-06, 'epoch': 0.89}\n",
      "{'loss': 0.5335, 'grad_norm': 0.04067070037126541, 'learning_rate': 7.612046748871327e-06, 'epoch': 0.89}\n",
      "{'loss': 0.5183, 'grad_norm': 0.047715455293655396, 'learning_rate': 7.281614543321269e-06, 'epoch': 0.89}\n",
      "{'loss': 0.5683, 'grad_norm': 0.037606965750455856, 'learning_rate': 6.958243201797554e-06, 'epoch': 0.89}\n",
      "{'loss': 0.5747, 'grad_norm': 0.04372773692011833, 'learning_rate': 6.6419573502798374e-06, 'epoch': 0.9}\n",
      "{'loss': 0.5951, 'grad_norm': 0.04654604569077492, 'learning_rate': 6.332781075160243e-06, 'epoch': 0.9}\n",
      "{'loss': 0.5548, 'grad_norm': 0.05056043341755867, 'learning_rate': 6.030737921409169e-06, 'epoch': 0.9}\n",
      " 90%|██████████████████████████████▌   | 1800/2000 [16:00:36<1:45:19, 31.60s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5779799818992615, 'eval_runtime': 83.741, 'eval_samples_per_second': 6.831, 'eval_steps_per_second': 0.43, 'epoch': 0.9}\n",
      "{'loss': 0.573, 'grad_norm': 0.04037856310606003, 'learning_rate': 5.448142440068316e-06, 'epoch': 0.91}\n",
      "{'loss': 0.5874, 'grad_norm': 0.040690530091524124, 'learning_rate': 5.167634479380068e-06, 'epoch': 0.91}\n",
      "{'loss': 0.5625, 'grad_norm': 0.04218929633498192, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.91}\n",
      "{'loss': 0.5416, 'grad_norm': 0.04418003559112549, 'learning_rate': 4.628304925177318e-06, 'epoch': 0.91}\n",
      "{'loss': 0.5519, 'grad_norm': 0.041087329387664795, 'learning_rate': 4.369524403696457e-06, 'epoch': 0.92}\n",
      "{'loss': 0.5339, 'grad_norm': 0.038743987679481506, 'learning_rate': 4.118026513180695e-06, 'epoch': 0.92}\n",
      "{'loss': 0.5697, 'grad_norm': 0.04221908375620842, 'learning_rate': 3.873830406168111e-06, 'epoch': 0.92}\n",
      "{'loss': 0.5493, 'grad_norm': 0.046997927129268646, 'learning_rate': 3.6369546791377052e-06, 'epoch': 0.92}\n",
      "{'loss': 0.5583, 'grad_norm': 0.04302177578210831, 'learning_rate': 3.40741737109318e-06, 'epoch': 0.93}\n",
      "{'loss': 0.554, 'grad_norm': 0.04269618168473244, 'learning_rate': 3.1852359621892367e-06, 'epoch': 0.93}\n",
      "{'loss': 0.5549, 'grad_norm': 0.04312707483768463, 'learning_rate': 2.970427372400353e-06, 'epoch': 0.93}\n",
      "{'loss': 0.5368, 'grad_norm': 0.03967849910259247, 'learning_rate': 2.7630079602323442e-06, 'epoch': 0.93}\n",
      "{'loss': 0.5351, 'grad_norm': 0.04365628957748413, 'learning_rate': 2.5629935214764865e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5391, 'grad_norm': 0.04267082363367081, 'learning_rate': 2.3703992880066638e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5485, 'grad_norm': 0.04291988164186478, 'learning_rate': 2.1852399266194314e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5657, 'grad_norm': 0.040556102991104126, 'learning_rate': 2.0075295379170412e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5528, 'grad_norm': 0.0422477051615715, 'learning_rate': 1.8372816552336026e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5425, 'grad_norm': 0.04402048885822296, 'learning_rate': 1.6745092436045494e-06, 'epoch': 0.95}\n",
      "{'loss': 0.552, 'grad_norm': 0.041938282549381256, 'learning_rate': 1.5192246987791981e-06, 'epoch': 0.95}\n",
      " 95%|██████████████████████████████████▏ | 1900/2000 [16:54:03<52:09, 31.29s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.5778846740722656, 'eval_runtime': 83.7715, 'eval_samples_per_second': 6.828, 'eval_steps_per_second': 0.43, 'epoch': 0.95}\n",
      "{'loss': 0.5415, 'grad_norm': 0.045094411820173264, 'learning_rate': 1.3714398462768563e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5588, 'grad_norm': 0.040920209139585495, 'learning_rate': 1.231165940486234e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5657, 'grad_norm': 0.041297849267721176, 'learning_rate': 1.0984136638083177e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5828, 'grad_norm': 0.04145597666501999, 'learning_rate': 9.731931258429638e-07, 'epoch': 0.96}\n",
      "{'loss': 0.5988, 'grad_norm': 0.043584201484918594, 'learning_rate': 8.555138626189618e-07, 'epoch': 0.96}\n",
      "{'loss': 0.5139, 'grad_norm': 0.04556921124458313, 'learning_rate': 7.453848358678017e-07, 'epoch': 0.96}\n",
      "{'loss': 0.5159, 'grad_norm': 0.04594379663467407, 'learning_rate': 6.428144323412544e-07, 'epoch': 0.97}\n",
      "{'loss': 0.5143, 'grad_norm': 0.05297262966632843, 'learning_rate': 5.478104631726711e-07, 'epoch': 0.97}\n",
      "{'loss': 0.5336, 'grad_norm': 0.03855856880545616, 'learning_rate': 4.6038016328211476e-07, 'epoch': 0.97}\n",
      "{'loss': 0.541, 'grad_norm': 0.047385815531015396, 'learning_rate': 3.805301908254455e-07, 'epoch': 0.97}\n",
      "{'loss': 0.5502, 'grad_norm': 0.04461260139942169, 'learning_rate': 3.0826662668720364e-07, 'epoch': 0.98}\n",
      "{'loss': 0.5465, 'grad_norm': 0.043133120983839035, 'learning_rate': 2.4359497401758024e-07, 'epoch': 0.98}\n",
      "{'loss': 0.5799, 'grad_norm': 0.0422358438372612, 'learning_rate': 1.86520157813308e-07, 'epoch': 0.98}\n",
      "{'loss': 0.545, 'grad_norm': 0.04261227324604988, 'learning_rate': 1.3704652454261668e-07, 'epoch': 0.98}\n",
      "{'loss': 0.5352, 'grad_norm': 0.044363997876644135, 'learning_rate': 9.517784181422019e-08, 'epoch': 0.99}\n",
      "{'loss': 0.5532, 'grad_norm': 0.04218252748250961, 'learning_rate': 6.09172980904238e-08, 'epoch': 0.99}\n",
      "{'loss': 0.549, 'grad_norm': 0.04055728390812874, 'learning_rate': 3.4267502444274015e-08, 'epoch': 0.99}\n",
      "{'loss': 0.6117, 'grad_norm': 0.040004417300224304, 'learning_rate': 1.5230484360873044e-08, 'epoch': 0.99}\n",
      "{'loss': 1.0344, 'grad_norm': 0.0428578183054924, 'learning_rate': 3.807693582869032e-09, 'epoch': 1.0}\n",
      "{'loss': 0.9524, 'grad_norm': 0.03901001065969467, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "100%|████████████████████████████████████| 2000/2000 [17:47:05<00:00, 24.66s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 16\n",
      "{'eval_loss': 0.577882707118988, 'eval_runtime': 39.0692, 'eval_samples_per_second': 14.641, 'eval_steps_per_second': 0.921, 'epoch': 1.0}\n",
      "100%|████████████████████████████████████| 2000/2000 [17:47:44<00:00, 24.66s/it]Saving model checkpoint to deepseek-coder-6.7b-base-APR-FIM-finetuning/checkpoint-2000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 64066.5753, 'train_samples_per_second': 1.998, 'train_steps_per_second': 0.031, 'train_loss': 0.6051359320878983, 'epoch': 1.0}\n",
      "100%|████████████████████████████████████| 2000/2000 [17:47:45<00:00, 32.03s/it]\n",
      "Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n",
      "Saving model checkpoint to deepseek-coder-6.7b-base-APR-FIM-finetuning\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "Saving model checkpoint to deepseek-coder-6.7b-base-APR-FIM-finetuning\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "Saving model checkpoint to deepseek-coder-6.7b-base-APR-FIM-finetuning\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\n",
      "}\n",
      "\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ardalaan/DeepSeekCoder6.7B_APR_FIM_finetuning\n",
    "\n",
    "%cd DeepSeekCoder6.7B_APR_FIM_finetuning/\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 WANDB_PROJECT=deepseekcoder_apr_fim python fim_transform_finetune.py \\\n",
    "--seed 11 \\\n",
    "--model_name_or_path \"deepseek-ai/deepseek-coder-6.7b-base\" \\\n",
    "--dataset_name \"ASSERT-KTH/repairllama-datasets\"\\\n",
    "--splits \"train\"\\\n",
    "--max_seq_len 2048 \\\n",
    "--max_steps 2000 \\\n",
    "--save_steps 500 \\\n",
    "--eval_steps 100 \\\n",
    "--logging_steps 5 \\\n",
    "--log_level \"info\" \\\n",
    "--logging_strategy \"steps\" \\\n",
    "--evaluation_strategy \"steps\" \\\n",
    "--save_strategy \"steps\" \\\n",
    "--push_to_hub True\\\n",
    "--hub_private_repo True \\\n",
    "--hub_strategy \"every_save\" \\\n",
    "--bf16 True \\\n",
    "--learning_rate 2e-4 \\\n",
    "--lr_scheduler_type \"cosine\" \\\n",
    "--weight_decay 0.1 \\\n",
    "--warmup_ratio 0.1 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--output_dir \"deepseek-coder-6.7b-base-APR-FIM-finetuning\" \\\n",
    "--per_device_train_batch_size 16 \\\n",
    "--per_device_eval_batch_size 16 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--gradient_checkpointing True \\\n",
    "--use_reentrant True \\\n",
    "--dataset_text_field \"prefix\" \\\n",
    "--test_size 0.1 \\\n",
    "--fim_rate 0.9 \\\n",
    "--fim_spm_rate 0.5 \\\n",
    "--use_peft_lora True \\\n",
    "--lora_r 16 \\\n",
    "--lora_alpha 16 \\\n",
    "--lora_dropout 0.1 \\\n",
    "--lora_target_modules \"q_proj,k_proj,v_proj,o_proj,down_proj,up_proj,gate_proj\" \\\n",
    "--use_4bit_quantization True \\\n",
    "--use_nested_quant True \\\n",
    "--bnb_4bit_compute_dtype \"bfloat16\" \\\n",
    "--use_flash_attn True \\\n",
    "--use_unsloth True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1e5e9-3846-4c3d-9859-3721f296c125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
