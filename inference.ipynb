{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc26292d-e835-40fa-91ce-650b0ca8dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/peft\n",
      "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-0dqfw8uu\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-0dqfw8uu\n",
      "  Resolved https://github.com/huggingface/peft to commit 162d7e57ee0088f42eb0f26150bd9170d30f3637\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.13.3.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.13.3.dev0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.13.3.dev0) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.13.3.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.13.3.dev0) (2.1.0+cu118)\n",
      "Collecting transformers (from peft==0.13.3.dev0)\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m823.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from peft==0.13.3.dev0)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.21.0 (from peft==0.13.3.dev0)\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting safetensors (from peft==0.13.3.dev0)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting huggingface-hub>=0.25.0 (from peft==0.13.3.dev0)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft==0.13.3.dev0) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.25.0->peft==0.13.3.dev0)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft==0.13.3.dev0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft==0.13.3.dev0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.13.3.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.13.3.dev0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.13.3.dev0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.13.3.dev0) (2.1.0)\n",
      "Collecting regex!=2019.12.17 (from transformers->peft==0.13.3.dev0)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20 (from transformers->peft==0.13.3.dev0)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.13.3.dev0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft==0.13.3.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft==0.13.3.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft==0.13.3.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft==0.13.3.dev0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.13.3.dev0) (1.3.0)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.13.3.dev0-py3-none-any.whl size=353031 sha256=4a7ac5c68b2d73f26885b9869e9a2058a19512d53f288e177f2d371ed2ff28a0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v2hk_fr6/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\n",
      "Successfully built peft\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, accelerate, transformers, peft\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.1.1 fsspec-2024.10.0 huggingface-hub-0.26.2 peft-0.13.3.dev0 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 tqdm-4.67.0 transformers-4.46.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Cloning into 'DeepSeekCoder6.7B_APR_FIM_finetuning'...\n",
      "remote: Enumerating objects: 77, done.\u001b[K\n",
      "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 77 (delta 42), reused 66 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (77/77), 33.87 KiB | 619.00 KiB/s, done.\n",
      "Resolving deltas: 100% (42/42), done.\n",
      "/workspace/DeepSeekCoder6.7B_APR_FIM_finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|███████████████████| 793/793 [00:00<00:00, 5.66MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.37M/1.37M [00:00<00:00, 3.29MB/s]\n",
      "config.json: 100%|█████████████████████████████| 632/632 [00:00<00:00, 6.53MB/s]\n",
      "model.safetensors.index.json: 100%|████████| 25.1k/25.1k [00:00<00:00, 97.4MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 10.5M/9.98G [00:00<06:44, 24.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 21.0M/9.98G [00:00<05:44, 28.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 31.5M/9.98G [00:01<05:27, 30.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 41.9M/9.98G [00:01<04:41, 35.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 52.4M/9.98G [00:01<04:17, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 62.9M/9.98G [00:01<04:02, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 73.4M/9.98G [00:01<03:53, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 83.9M/9.98G [00:02<03:46, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 94.4M/9.98G [00:02<03:42, 44.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 105M/9.98G [00:02<04:31, 36.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 115M/9.98G [00:03<04:19, 37.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 126M/9.98G [00:03<04:05, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 136M/9.98G [00:03<03:56, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 147M/9.98G [00:03<04:07, 39.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 157M/9.98G [00:04<03:56, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 168M/9.98G [00:04<04:30, 36.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 178M/9.98G [00:04<04:19, 37.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 189M/9.98G [00:04<04:05, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 199M/9.98G [00:05<04:51, 33.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 210M/9.98G [00:05<04:29, 36.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 220M/9.98G [00:05<04:11, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 231M/9.98G [00:06<03:59, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 241M/9.98G [00:06<03:50, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 252M/9.98G [00:06<03:44, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 262M/9.98G [00:06<03:40, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 273M/9.98G [00:06<03:36, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 283M/9.98G [00:07<03:34, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 294M/9.98G [00:07<03:32, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 304M/9.98G [00:07<04:33, 35.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 315M/9.98G [00:08<04:13, 38.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 325M/9.98G [00:08<03:59, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 336M/9.98G [00:08<04:21, 36.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 346M/9.98G [00:08<04:21, 36.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 357M/9.98G [00:09<04:18, 37.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 367M/9.98G [00:09<04:40, 34.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 377M/9.98G [00:09<04:18, 37.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 388M/9.98G [00:10<04:03, 39.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 398M/9.98G [00:10<04:25, 36.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 409M/9.98G [00:10<04:26, 36.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 419M/9.98G [00:11<04:59, 32.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 430M/9.98G [00:11<04:42, 33.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 440M/9.98G [00:11<04:20, 36.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 451M/9.98G [00:11<04:39, 34.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 461M/9.98G [00:12<04:38, 34.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 472M/9.98G [00:12<04:17, 37.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 482M/9.98G [00:12<04:01, 39.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 493M/9.98G [00:12<03:50, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 503M/9.98G [00:13<03:42, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 514M/9.98G [00:13<03:36, 43.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 524M/9.98G [00:13<03:33, 44.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 535M/9.98G [00:13<03:30, 44.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 545M/9.98G [00:14<03:28, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 556M/9.98G [00:14<03:27, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 566M/9.98G [00:14<03:28, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 577M/9.98G [00:14<04:09, 37.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 587M/9.98G [00:15<04:45, 32.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 598M/9.98G [00:15<04:27, 35.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 608M/9.98G [00:15<04:40, 33.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 619M/9.98G [00:16<04:17, 36.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 629M/9.98G [00:16<04:00, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 640M/9.98G [00:16<03:48, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 650M/9.98G [00:16<03:40, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 661M/9.98G [00:17<03:34, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 671M/9.98G [00:17<03:30, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 682M/9.98G [00:17<03:27, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 692M/9.98G [00:17<03:25, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 703M/9.98G [00:17<03:24, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 713M/9.98G [00:18<03:22, 45.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 724M/9.98G [00:18<03:38, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 734M/9.98G [00:18<04:22, 35.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 744M/9.98G [00:19<04:03, 37.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 755M/9.98G [00:19<03:50, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 765M/9.98G [00:19<04:50, 31.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 776M/9.98G [00:20<04:23, 35.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 786M/9.98G [00:20<04:03, 37.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 797M/9.98G [00:20<03:49, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 807M/9.98G [00:20<03:40, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 818M/9.98G [00:21<04:30, 33.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 828M/9.98G [00:21<04:09, 36.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 839M/9.98G [00:21<04:26, 34.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 849M/9.98G [00:21<04:05, 37.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 860M/9.98G [00:22<05:32, 27.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 870M/9.98G [00:22<04:58, 30.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 881M/9.98G [00:23<04:27, 34.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 891M/9.98G [00:23<04:06, 36.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 902M/9.98G [00:23<03:51, 39.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 912M/9.98G [00:23<03:40, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 923M/9.98G [00:23<03:32, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 933M/9.98G [00:24<03:28, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 944M/9.98G [00:24<03:23, 44.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 954M/9.98G [00:24<03:21, 44.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 965M/9.98G [00:24<03:19, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 975M/9.98G [00:25<03:19, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 986M/9.98G [00:25<03:23, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 996M/9.98G [00:25<03:25, 43.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍   | 1.01G/9.98G [00:25<03:27, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍   | 1.02G/9.98G [00:26<03:29, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍   | 1.03G/9.98G [00:26<04:24, 33.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍   | 1.04G/9.98G [00:26<04:05, 36.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.05G/9.98G [00:27<03:50, 38.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.06G/9.98G [00:27<03:38, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.07G/9.98G [00:27<03:30, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.08G/9.98G [00:27<03:25, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.09G/9.98G [00:28<03:38, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.10G/9.98G [00:28<04:02, 36.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.11G/9.98G [00:28<03:46, 39.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.12G/9.98G [00:28<03:36, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.13G/9.98G [00:29<03:28, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.14G/9.98G [00:29<03:23, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.15G/9.98G [00:29<03:19, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.16G/9.98G [00:29<03:28, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.17G/9.98G [00:30<04:14, 34.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.18G/9.98G [00:30<04:08, 35.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.20G/9.98G [00:30<03:50, 38.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.21G/9.98G [00:30<03:38, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.22G/9.98G [00:31<03:29, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.23G/9.98G [00:31<03:34, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.24G/9.98G [00:31<04:08, 35.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.25G/9.98G [00:32<03:51, 37.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.26G/9.98G [00:32<03:54, 37.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.27G/9.98G [00:32<05:09, 28.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.28G/9.98G [00:33<04:51, 29.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.29G/9.98G [00:33<04:56, 29.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.30G/9.98G [00:33<04:31, 32.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.31G/9.98G [00:34<04:40, 30.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.32G/9.98G [00:34<04:25, 32.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.33G/9.98G [00:34<04:02, 35.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.34G/9.98G [00:35<03:45, 38.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.35G/9.98G [00:35<03:33, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.36G/9.98G [00:35<03:26, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.37G/9.98G [00:35<03:19, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.38G/9.98G [00:35<03:15, 43.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.39G/9.98G [00:36<03:12, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.41G/9.98G [00:36<03:25, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.42G/9.98G [00:36<03:36, 39.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.43G/9.98G [00:36<03:26, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.44G/9.98G [00:37<03:20, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.45G/9.98G [00:37<03:55, 36.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.46G/9.98G [00:37<03:39, 38.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.47G/9.98G [00:38<03:57, 35.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.48G/9.98G [00:38<03:57, 35.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.49G/9.98G [00:38<03:40, 38.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.50G/9.98G [00:38<03:29, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.51G/9.98G [00:39<03:21, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.52G/9.98G [00:39<03:15, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.53G/9.98G [00:39<03:27, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.54G/9.98G [00:39<03:33, 39.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▌   | 1.55G/9.98G [00:40<03:38, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.56G/9.98G [00:40<03:28, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.57G/9.98G [00:40<03:50, 36.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.58G/9.98G [00:41<03:34, 39.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.59G/9.98G [00:41<04:08, 33.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.60G/9.98G [00:41<03:53, 35.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.61G/9.98G [00:41<03:37, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.63G/9.98G [00:42<03:26, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.64G/9.98G [00:42<03:18, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▋   | 1.65G/9.98G [00:42<03:12, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.66G/9.98G [00:42<03:18, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.67G/9.98G [00:43<03:59, 34.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.68G/9.98G [00:43<04:49, 28.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.69G/9.98G [00:44<04:52, 28.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.70G/9.98G [00:44<04:18, 32.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.71G/9.98G [00:44<03:54, 35.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.72G/9.98G [00:44<03:37, 38.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.73G/9.98G [00:45<03:25, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▋   | 1.74G/9.98G [00:45<03:17, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.75G/9.98G [00:45<03:52, 35.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.76G/9.98G [00:45<03:44, 36.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.77G/9.98G [00:46<04:07, 33.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.78G/9.98G [00:46<03:53, 35.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.79G/9.98G [00:46<03:36, 37.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.80G/9.98G [00:47<03:51, 35.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.81G/9.98G [00:47<04:02, 33.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.82G/9.98G [00:47<04:24, 30.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.84G/9.98G [00:48<04:12, 32.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▋   | 1.85G/9.98G [00:48<03:49, 35.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▋   | 1.86G/9.98G [00:48<03:32, 38.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▋   | 1.87G/9.98G [00:48<03:21, 40.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▊   | 1.88G/9.98G [00:49<03:13, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▊   | 1.89G/9.98G [00:49<03:07, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▊   | 1.90G/9.98G [00:49<03:03, 43.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▊   | 1.91G/9.98G [00:49<03:00, 44.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▊   | 1.92G/9.98G [00:50<03:12, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▊   | 1.93G/9.98G [00:50<03:26, 39.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▊   | 1.94G/9.98G [00:50<03:39, 36.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.95G/9.98G [00:51<03:49, 35.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.96G/9.98G [00:51<03:48, 35.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.97G/9.98G [00:51<03:31, 37.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.98G/9.98G [00:51<03:20, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.99G/9.98G [00:52<03:11, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 2.00G/9.98G [00:52<03:05, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 2.01G/9.98G [00:52<03:01, 43.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 2.02G/9.98G [00:52<02:58, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 2.03G/9.98G [00:52<02:56, 45.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 2.04G/9.98G [00:53<02:54, 45.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.06G/9.98G [00:53<03:06, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.07G/9.98G [00:54<04:28, 29.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.08G/9.98G [00:54<03:59, 33.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.09G/9.98G [00:54<03:38, 36.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.10G/9.98G [00:54<03:23, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.11G/9.98G [00:54<03:13, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.12G/9.98G [00:55<03:06, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.13G/9.98G [00:55<03:01, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 2.14G/9.98G [00:55<02:57, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▊   | 2.15G/9.98G [00:55<02:55, 44.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▊   | 2.16G/9.98G [00:56<02:53, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▊   | 2.17G/9.98G [00:56<02:54, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▊   | 2.18G/9.98G [00:56<02:58, 43.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 2.19G/9.98G [00:56<03:00, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 2.20G/9.98G [00:57<03:36, 35.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 2.21G/9.98G [00:57<03:23, 38.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 2.22G/9.98G [00:57<03:12, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 2.23G/9.98G [00:57<03:04, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 2.24G/9.98G [00:58<02:59, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.25G/9.98G [00:58<02:55, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.26G/9.98G [00:58<02:52, 44.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.28G/9.98G [00:58<03:01, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.29G/9.98G [00:59<03:27, 37.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.30G/9.98G [00:59<03:14, 39.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.31G/9.98G [00:59<03:05, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.32G/9.98G [00:59<02:59, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.33G/9.98G [01:00<02:55, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 2.34G/9.98G [01:00<02:52, 44.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.35G/9.98G [01:00<02:50, 44.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.36G/9.98G [01:00<02:48, 45.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.37G/9.98G [01:01<03:11, 39.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.38G/9.98G [01:01<03:06, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.39G/9.98G [01:01<03:20, 37.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.40G/9.98G [01:02<03:32, 35.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.41G/9.98G [01:02<03:17, 38.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.42G/9.98G [01:02<03:22, 37.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.43G/9.98G [01:02<03:29, 36.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 2.44G/9.98G [01:03<03:20, 37.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|▉   | 2.45G/9.98G [01:03<03:40, 34.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|▉   | 2.46G/9.98G [01:03<03:26, 36.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|▉   | 2.47G/9.98G [01:04<03:34, 35.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|▉   | 2.49G/9.98G [01:04<03:52, 32.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█   | 2.50G/9.98G [01:04<03:31, 35.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█   | 2.51G/9.98G [01:05<03:30, 35.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█   | 2.52G/9.98G [01:05<03:16, 38.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█   | 2.53G/9.98G [01:05<03:10, 39.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█   | 2.54G/9.98G [01:05<03:23, 36.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.55G/9.98G [01:06<03:10, 39.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.56G/9.98G [01:06<03:22, 36.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.57G/9.98G [01:06<03:23, 36.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.58G/9.98G [01:06<03:10, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.59G/9.98G [01:07<03:00, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.60G/9.98G [01:07<02:54, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.61G/9.98G [01:07<02:49, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.62G/9.98G [01:07<02:46, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.63G/9.98G [01:08<02:43, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█   | 2.64G/9.98G [01:08<02:42, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.65G/9.98G [01:08<02:44, 44.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.66G/9.98G [01:08<03:11, 38.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.67G/9.98G [01:09<03:10, 38.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.68G/9.98G [01:09<03:30, 34.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.69G/9.98G [01:09<03:14, 37.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.71G/9.98G [01:10<03:11, 38.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.72G/9.98G [01:10<03:19, 36.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.73G/9.98G [01:10<03:07, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█   | 2.74G/9.98G [01:10<02:57, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 2.75G/9.98G [01:11<02:51, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 2.76G/9.98G [01:11<02:46, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 2.77G/9.98G [01:11<02:56, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 2.78G/9.98G [01:11<03:03, 39.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 2.79G/9.98G [01:12<03:05, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█   | 2.80G/9.98G [01:12<03:12, 37.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▏  | 2.81G/9.98G [01:12<03:01, 39.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▏  | 2.82G/9.98G [01:12<02:53, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▏  | 2.83G/9.98G [01:13<02:47, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▏  | 2.84G/9.98G [01:13<03:02, 39.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.85G/9.98G [01:13<03:25, 34.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.86G/9.98G [01:14<03:11, 37.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.87G/9.98G [01:14<03:27, 34.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.88G/9.98G [01:14<03:26, 34.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.89G/9.98G [01:14<03:23, 34.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.90G/9.98G [01:15<03:21, 35.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.92G/9.98G [01:15<03:06, 37.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.93G/9.98G [01:15<03:03, 38.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 2.94G/9.98G [01:16<03:06, 37.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 2.95G/9.98G [01:16<03:38, 32.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 2.96G/9.98G [01:16<03:46, 31.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 2.97G/9.98G [01:17<03:34, 32.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 2.98G/9.98G [01:17<03:31, 33.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 2.99G/9.98G [01:17<03:13, 36.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 3.00G/9.98G [01:18<03:24, 34.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 3.01G/9.98G [01:18<03:21, 34.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 3.02G/9.98G [01:18<04:10, 27.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 3.03G/9.98G [01:19<03:52, 29.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 3.04G/9.98G [01:19<03:27, 33.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 3.05G/9.98G [01:19<03:11, 36.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 3.06G/9.98G [01:19<02:59, 38.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 3.07G/9.98G [01:20<03:35, 32.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 3.08G/9.98G [01:20<03:23, 33.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 3.09G/9.98G [01:20<03:07, 36.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 3.10G/9.98G [01:21<02:57, 38.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 3.11G/9.98G [01:21<02:48, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▎  | 3.12G/9.98G [01:21<02:41, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▎  | 3.14G/9.98G [01:21<02:37, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.15G/9.98G [01:21<02:34, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.16G/9.98G [01:22<02:32, 44.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.17G/9.98G [01:22<02:31, 45.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.18G/9.98G [01:22<02:29, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.19G/9.98G [01:22<02:50, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.20G/9.98G [01:23<03:05, 36.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.21G/9.98G [01:23<02:53, 39.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.22G/9.98G [01:23<02:45, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.23G/9.98G [01:24<02:42, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▎  | 3.24G/9.98G [01:24<02:37, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.25G/9.98G [01:24<02:33, 43.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.26G/9.98G [01:24<03:17, 34.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.27G/9.98G [01:25<03:03, 36.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.28G/9.98G [01:25<02:51, 39.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.29G/9.98G [01:25<02:43, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.30G/9.98G [01:25<02:39, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.31G/9.98G [01:26<02:37, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.32G/9.98G [01:26<03:07, 35.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▎  | 3.33G/9.98G [01:26<02:54, 38.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.34G/9.98G [01:26<02:45, 40.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.36G/9.98G [01:27<02:39, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.37G/9.98G [01:27<02:34, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.38G/9.98G [01:27<02:30, 43.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.39G/9.98G [01:27<02:28, 44.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.40G/9.98G [01:28<02:26, 45.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.41G/9.98G [01:28<02:25, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.42G/9.98G [01:28<02:24, 45.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▎  | 3.43G/9.98G [01:28<02:23, 45.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▍  | 3.44G/9.98G [01:29<02:35, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.45G/9.98G [01:29<02:42, 40.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.46G/9.98G [01:29<02:36, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.47G/9.98G [01:29<02:48, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.48G/9.98G [01:30<03:00, 36.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.49G/9.98G [01:30<02:57, 36.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.50G/9.98G [01:30<02:59, 36.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.51G/9.98G [01:31<02:48, 38.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.52G/9.98G [01:31<02:40, 40.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▍  | 3.53G/9.98G [01:31<02:34, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.54G/9.98G [01:31<02:29, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.55G/9.98G [01:32<02:51, 37.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.57G/9.98G [01:32<02:55, 36.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.58G/9.98G [01:32<02:43, 39.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.59G/9.98G [01:32<02:35, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.60G/9.98G [01:33<02:31, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.61G/9.98G [01:33<02:26, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.62G/9.98G [01:33<02:23, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.63G/9.98G [01:33<02:22, 44.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▍  | 3.64G/9.98G [01:34<02:22, 44.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.65G/9.98G [01:34<02:20, 45.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.66G/9.98G [01:34<02:19, 45.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.67G/9.98G [01:34<02:18, 45.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.68G/9.98G [01:34<02:21, 44.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.69G/9.98G [01:35<02:26, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.70G/9.98G [01:35<02:26, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.71G/9.98G [01:35<03:09, 33.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.72G/9.98G [01:36<03:03, 34.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▍  | 3.73G/9.98G [01:36<03:02, 34.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.74G/9.98G [01:36<02:47, 37.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.75G/9.98G [01:37<03:09, 32.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.76G/9.98G [01:37<03:20, 30.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.77G/9.98G [01:37<03:02, 34.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.79G/9.98G [01:38<02:48, 36.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.80G/9.98G [01:38<02:37, 39.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.81G/9.98G [01:38<02:30, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.82G/9.98G [01:38<02:25, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.83G/9.98G [01:39<02:46, 37.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▌  | 3.84G/9.98G [01:39<02:57, 34.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.85G/9.98G [01:39<02:44, 37.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.86G/9.98G [01:39<02:34, 39.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.87G/9.98G [01:40<02:27, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.88G/9.98G [01:40<03:27, 29.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.89G/9.98G [01:40<03:04, 32.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.90G/9.98G [01:41<02:56, 34.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.91G/9.98G [01:41<03:07, 32.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.92G/9.98G [01:41<02:49, 35.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▌  | 3.93G/9.98G [01:42<02:38, 38.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 3.94G/9.98G [01:42<02:29, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 3.95G/9.98G [01:42<02:24, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 3.96G/9.98G [01:42<02:30, 39.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 3.97G/9.98G [01:43<02:34, 38.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 3.98G/9.98G [01:43<02:26, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 4.00G/9.98G [01:43<02:30, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 4.01G/9.98G [01:43<02:34, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 4.02G/9.98G [01:44<02:29, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 4.03G/9.98G [01:44<02:22, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▌  | 4.04G/9.98G [01:44<02:18, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▌  | 4.05G/9.98G [01:44<02:15, 43.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 4.06G/9.98G [01:45<02:12, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 4.07G/9.98G [01:45<02:11, 45.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 4.08G/9.98G [01:45<02:09, 45.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 4.09G/9.98G [01:45<02:08, 45.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 4.10G/9.98G [01:45<02:08, 45.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 4.11G/9.98G [01:46<02:07, 46.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 4.12G/9.98G [01:46<02:07, 46.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|█▋  | 4.13G/9.98G [01:46<02:06, 46.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.14G/9.98G [01:46<02:07, 45.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.15G/9.98G [01:47<02:26, 39.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.16G/9.98G [01:47<02:33, 38.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.17G/9.98G [01:47<02:24, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.18G/9.98G [01:47<02:18, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.19G/9.98G [01:48<02:16, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.20G/9.98G [01:48<02:18, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.22G/9.98G [01:48<02:17, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.23G/9.98G [01:48<02:19, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|█▋  | 4.24G/9.98G [01:49<02:27, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.25G/9.98G [01:49<02:20, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.26G/9.98G [01:49<02:34, 37.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.27G/9.98G [01:50<02:24, 39.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.28G/9.98G [01:50<02:18, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.29G/9.98G [01:50<02:13, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.30G/9.98G [01:50<02:50, 33.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.31G/9.98G [01:51<02:54, 32.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.32G/9.98G [01:51<02:38, 35.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|█▋  | 4.33G/9.98G [01:51<02:44, 34.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▋  | 4.34G/9.98G [01:52<02:33, 36.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▋  | 4.35G/9.98G [01:52<02:23, 39.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▋  | 4.36G/9.98G [01:52<02:35, 36.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 4.37G/9.98G [01:52<02:25, 38.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 4.38G/9.98G [01:53<02:27, 38.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 4.39G/9.98G [01:53<02:19, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 4.40G/9.98G [01:53<02:13, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 4.41G/9.98G [01:53<02:28, 37.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 4.42G/9.98G [01:54<02:19, 39.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|█▊  | 4.44G/9.98G [01:54<02:16, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.45G/9.98G [01:54<02:12, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.46G/9.98G [01:54<02:17, 40.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.47G/9.98G [01:55<02:15, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.48G/9.98G [01:55<02:10, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.49G/9.98G [01:55<02:07, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.50G/9.98G [01:55<02:05, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.51G/9.98G [01:56<02:04, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.52G/9.98G [01:56<02:01, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|█▊  | 4.53G/9.98G [01:56<02:11, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.54G/9.98G [01:56<02:07, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.55G/9.98G [01:57<02:04, 43.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.56G/9.98G [01:57<02:10, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.57G/9.98G [01:57<02:07, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.58G/9.98G [01:57<02:13, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.59G/9.98G [01:58<02:08, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.60G/9.98G [01:58<02:04, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.61G/9.98G [01:58<02:01, 44.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.62G/9.98G [01:58<01:59, 44.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|█▊  | 4.63G/9.98G [01:59<01:58, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▊  | 4.65G/9.98G [01:59<01:57, 45.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▊  | 4.66G/9.98G [01:59<01:56, 45.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▊  | 4.67G/9.98G [01:59<02:05, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▊  | 4.68G/9.98G [02:00<02:02, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 4.69G/9.98G [02:00<01:59, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 4.70G/9.98G [02:00<02:44, 32.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 4.71G/9.98G [02:01<02:29, 35.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 4.72G/9.98G [02:01<02:55, 29.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 4.73G/9.98G [02:01<02:36, 33.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|█▉  | 4.74G/9.98G [02:01<02:23, 36.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.75G/9.98G [02:02<02:14, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.76G/9.98G [02:02<02:08, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.77G/9.98G [02:02<02:20, 37.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.78G/9.98G [02:02<02:12, 39.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.79G/9.98G [02:03<02:06, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.80G/9.98G [02:03<02:01, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.81G/9.98G [02:03<01:58, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.82G/9.98G [02:03<01:56, 44.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 4.83G/9.98G [02:04<01:54, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.84G/9.98G [02:04<01:53, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.85G/9.98G [02:04<01:52, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.87G/9.98G [02:04<01:52, 45.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.88G/9.98G [02:05<01:54, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.89G/9.98G [02:05<01:56, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.90G/9.98G [02:05<01:58, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.91G/9.98G [02:05<01:59, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.92G/9.98G [02:06<02:00, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.93G/9.98G [02:06<01:59, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|█▉  | 4.94G/9.98G [02:06<01:59, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█▉  | 4.95G/9.98G [02:06<02:09, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█▉  | 4.96G/9.98G [02:07<02:03, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█▉  | 4.97G/9.98G [02:07<01:58, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█▉  | 4.98G/9.98G [02:07<02:03, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 4.99G/9.98G [02:07<01:58, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 5.00G/9.98G [02:08<01:55, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 5.01G/9.98G [02:08<01:52, 44.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 5.02G/9.98G [02:08<01:54, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██  | 5.03G/9.98G [02:08<01:55, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.04G/9.98G [02:09<01:56, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.05G/9.98G [02:09<01:56, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.06G/9.98G [02:09<01:56, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.08G/9.98G [02:09<01:56, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.09G/9.98G [02:10<01:55, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.10G/9.98G [02:10<01:56, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.11G/9.98G [02:10<01:56, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.12G/9.98G [02:10<01:55, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.13G/9.98G [02:11<01:55, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██  | 5.14G/9.98G [02:11<01:54, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.15G/9.98G [02:11<01:55, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.16G/9.98G [02:11<01:54, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.17G/9.98G [02:12<01:55, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.18G/9.98G [02:12<02:02, 39.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.19G/9.98G [02:12<02:32, 31.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.20G/9.98G [02:13<02:17, 34.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.21G/9.98G [02:13<02:06, 37.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.22G/9.98G [02:13<01:59, 39.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██  | 5.23G/9.98G [02:13<01:54, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 5.24G/9.98G [02:13<01:50, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 5.25G/9.98G [02:14<01:47, 43.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 5.26G/9.98G [02:14<01:46, 44.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 5.27G/9.98G [02:14<01:44, 45.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 5.28G/9.98G [02:14<01:43, 45.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██  | 5.30G/9.98G [02:15<01:42, 45.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▏ | 5.31G/9.98G [02:15<01:43, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▏ | 5.32G/9.98G [02:15<01:44, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▏ | 5.33G/9.98G [02:15<01:46, 43.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▏ | 5.34G/9.98G [02:16<01:47, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.35G/9.98G [02:16<01:47, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.36G/9.98G [02:16<01:49, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.37G/9.98G [02:16<01:48, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.38G/9.98G [02:17<01:49, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.39G/9.98G [02:17<01:48, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.40G/9.98G [02:17<01:49, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.41G/9.98G [02:17<01:48, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.42G/9.98G [02:18<01:47, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▏ | 5.43G/9.98G [02:18<01:46, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.44G/9.98G [02:18<01:47, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.45G/9.98G [02:18<01:46, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.46G/9.98G [02:19<01:46, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.47G/9.98G [02:19<01:46, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.48G/9.98G [02:19<01:46, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.49G/9.98G [02:19<01:46, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.51G/9.98G [02:20<01:47, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.52G/9.98G [02:20<01:47, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.53G/9.98G [02:20<01:46, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▏ | 5.54G/9.98G [02:20<01:45, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 5.55G/9.98G [02:21<01:44, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 5.56G/9.98G [02:21<01:44, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 5.57G/9.98G [02:21<01:44, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 5.58G/9.98G [02:21<01:44, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 5.59G/9.98G [02:22<01:43, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 5.60G/9.98G [02:22<01:51, 39.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▏ | 5.61G/9.98G [02:22<01:53, 38.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▎ | 5.62G/9.98G [02:22<01:47, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▎ | 5.63G/9.98G [02:23<01:43, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.64G/9.98G [02:23<01:40, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.65G/9.98G [02:23<01:40, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.66G/9.98G [02:23<01:40, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.67G/9.98G [02:24<01:40, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.68G/9.98G [02:24<01:41, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.69G/9.98G [02:24<02:05, 34.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.70G/9.98G [02:25<01:56, 36.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.71G/9.98G [02:25<01:48, 39.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.73G/9.98G [02:25<01:43, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▎ | 5.74G/9.98G [02:25<01:39, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.75G/9.98G [02:25<01:37, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.76G/9.98G [02:26<01:35, 44.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.77G/9.98G [02:26<01:33, 44.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.78G/9.98G [02:26<01:32, 45.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.79G/9.98G [02:26<01:32, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.80G/9.98G [02:27<01:33, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.81G/9.98G [02:27<01:35, 43.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.82G/9.98G [02:27<01:35, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▎ | 5.83G/9.98G [02:27<01:36, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.84G/9.98G [02:28<01:36, 42.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.85G/9.98G [02:28<01:55, 35.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.86G/9.98G [02:28<01:53, 36.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.87G/9.98G [02:29<01:45, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.88G/9.98G [02:29<01:40, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.89G/9.98G [02:29<01:36, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.90G/9.98G [02:29<01:33, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.91G/9.98G [02:29<01:31, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▎ | 5.92G/9.98G [02:30<01:30, 44.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▍ | 5.93G/9.98G [02:30<01:29, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 5.95G/9.98G [02:30<01:28, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 5.96G/9.98G [02:30<01:29, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 5.97G/9.98G [02:31<01:30, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 5.98G/9.98G [02:31<01:31, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 5.99G/9.98G [02:31<01:39, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 6.00G/9.98G [02:31<01:37, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 6.01G/9.98G [02:32<01:36, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 6.02G/9.98G [02:32<01:35, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▍ | 6.03G/9.98G [02:32<02:00, 32.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.04G/9.98G [02:33<01:49, 36.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.05G/9.98G [02:33<01:41, 38.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.06G/9.98G [02:33<01:36, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.07G/9.98G [02:33<01:32, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.08G/9.98G [02:33<01:30, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.09G/9.98G [02:34<01:28, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.10G/9.98G [02:34<01:26, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.11G/9.98G [02:34<01:25, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.12G/9.98G [02:34<01:24, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██▍ | 6.13G/9.98G [02:35<01:25, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.14G/9.98G [02:35<01:27, 44.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.16G/9.98G [02:35<01:27, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.17G/9.98G [02:35<01:27, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.18G/9.98G [02:36<01:28, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.19G/9.98G [02:36<01:28, 43.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.20G/9.98G [02:36<01:28, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.21G/9.98G [02:36<01:28, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.22G/9.98G [02:37<01:28, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██▍ | 6.23G/9.98G [02:37<01:27, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.24G/9.98G [02:37<01:30, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.25G/9.98G [02:37<01:29, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.26G/9.98G [02:38<01:28, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.27G/9.98G [02:38<01:28, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.28G/9.98G [02:38<01:27, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.29G/9.98G [02:38<01:28, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.30G/9.98G [02:39<01:27, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.31G/9.98G [02:39<01:27, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.32G/9.98G [02:39<01:27, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██▌ | 6.33G/9.98G [02:39<01:26, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.34G/9.98G [02:40<01:26, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.35G/9.98G [02:40<01:26, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.36G/9.98G [02:40<01:26, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.38G/9.98G [02:40<01:25, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.39G/9.98G [02:41<01:25, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.40G/9.98G [02:41<01:25, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.41G/9.98G [02:41<01:25, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.42G/9.98G [02:41<01:24, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██▌ | 6.43G/9.98G [02:42<01:24, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.44G/9.98G [02:42<01:24, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.45G/9.98G [02:42<01:24, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.46G/9.98G [02:42<01:23, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.47G/9.98G [02:43<01:23, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.48G/9.98G [02:43<01:22, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.49G/9.98G [02:43<01:22, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.50G/9.98G [02:43<01:22, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.51G/9.98G [02:44<01:22, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.52G/9.98G [02:44<01:45, 32.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██▌ | 6.53G/9.98G [02:44<01:35, 35.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▌ | 6.54G/9.98G [02:45<01:29, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 6.55G/9.98G [02:45<01:24, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 6.56G/9.98G [02:45<01:21, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 6.57G/9.98G [02:45<01:18, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 6.59G/9.98G [02:45<01:16, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 6.60G/9.98G [02:46<01:15, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 6.61G/9.98G [02:46<01:14, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 6.62G/9.98G [02:46<01:13, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██▋ | 6.63G/9.98G [02:46<01:13, 45.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.64G/9.98G [02:47<01:13, 45.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.65G/9.98G [02:47<01:14, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.66G/9.98G [02:47<01:16, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.67G/9.98G [02:47<01:17, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.68G/9.98G [02:48<01:18, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.69G/9.98G [02:48<01:18, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.70G/9.98G [02:48<01:19, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.71G/9.98G [02:48<01:19, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.72G/9.98G [02:49<01:18, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██▋ | 6.73G/9.98G [02:49<01:20, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.74G/9.98G [02:49<01:20, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.75G/9.98G [02:49<01:20, 40.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.76G/9.98G [02:50<01:18, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.77G/9.98G [02:50<01:17, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.78G/9.98G [02:50<01:17, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.79G/9.98G [02:50<01:17, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.81G/9.98G [02:51<01:18, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.82G/9.98G [02:51<01:18, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██▋ | 6.83G/9.98G [02:51<01:17, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▋ | 6.84G/9.98G [02:51<01:16, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▋ | 6.85G/9.98G [02:52<01:15, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▋ | 6.86G/9.98G [02:52<01:15, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 6.87G/9.98G [02:52<01:14, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 6.88G/9.98G [02:52<01:13, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 6.89G/9.98G [02:53<01:13, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 6.90G/9.98G [02:53<01:13, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 6.91G/9.98G [02:53<01:13, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 6.92G/9.98G [02:53<01:13, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██▊ | 6.93G/9.98G [02:54<01:13, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 6.94G/9.98G [02:54<01:12, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 6.95G/9.98G [02:54<01:11, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 6.96G/9.98G [02:54<01:11, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 6.97G/9.98G [02:55<01:10, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 6.98G/9.98G [02:55<01:10, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 6.99G/9.98G [02:55<01:30, 33.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 7.00G/9.98G [02:56<01:22, 36.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 7.01G/9.98G [02:56<01:16, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██▊ | 7.03G/9.98G [02:56<01:12, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.04G/9.98G [02:56<01:09, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.05G/9.98G [02:57<01:07, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.06G/9.98G [02:57<01:06, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.07G/9.98G [02:57<01:05, 44.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.08G/9.98G [02:57<01:04, 45.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.09G/9.98G [02:57<01:03, 45.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.10G/9.98G [02:58<01:03, 45.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.11G/9.98G [02:58<01:02, 45.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.12G/9.98G [02:58<01:04, 44.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|██▊ | 7.13G/9.98G [02:58<01:05, 43.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▊ | 7.14G/9.98G [02:59<01:06, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▊ | 7.15G/9.98G [02:59<01:06, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▊ | 7.16G/9.98G [02:59<01:06, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 7.17G/9.98G [02:59<01:06, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 7.18G/9.98G [03:00<01:06, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 7.19G/9.98G [03:00<01:06, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 7.20G/9.98G [03:00<01:06, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 7.21G/9.98G [03:00<01:05, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|██▉ | 7.22G/9.98G [03:01<01:08, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.24G/9.98G [03:01<01:06, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.25G/9.98G [03:01<01:06, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.26G/9.98G [03:01<01:05, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.27G/9.98G [03:02<01:05, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.28G/9.98G [03:02<01:04, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.29G/9.98G [03:02<01:03, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.30G/9.98G [03:02<01:03, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.31G/9.98G [03:03<01:03, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.32G/9.98G [03:03<01:03, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|██▉ | 7.33G/9.98G [03:03<01:02, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.34G/9.98G [03:03<01:03, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.35G/9.98G [03:04<01:02, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.36G/9.98G [03:04<01:02, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.37G/9.98G [03:04<01:02, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.38G/9.98G [03:04<01:02, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.39G/9.98G [03:05<01:01, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.40G/9.98G [03:05<01:01, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.41G/9.98G [03:05<01:02, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|██▉ | 7.42G/9.98G [03:05<01:02, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 7.43G/9.98G [03:06<01:01, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 7.44G/9.98G [03:06<01:01, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 7.46G/9.98G [03:06<01:00, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 7.47G/9.98G [03:06<00:59, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|██▉ | 7.48G/9.98G [03:07<00:59, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 7.49G/9.98G [03:07<00:59, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 7.50G/9.98G [03:07<00:58, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 7.51G/9.98G [03:07<00:58, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 7.52G/9.98G [03:08<00:58, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███ | 7.53G/9.98G [03:08<00:58, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.54G/9.98G [03:08<01:08, 35.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.55G/9.98G [03:09<01:05, 37.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.56G/9.98G [03:09<01:03, 38.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.57G/9.98G [03:09<01:01, 39.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.58G/9.98G [03:09<01:00, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.59G/9.98G [03:10<00:58, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.60G/9.98G [03:10<00:57, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.61G/9.98G [03:10<00:56, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.62G/9.98G [03:10<00:56, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███ | 7.63G/9.98G [03:11<00:55, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.64G/9.98G [03:11<00:55, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.65G/9.98G [03:11<00:55, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.67G/9.98G [03:11<00:54, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.68G/9.98G [03:12<00:54, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.69G/9.98G [03:12<00:54, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.70G/9.98G [03:12<00:54, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.71G/9.98G [03:12<00:53, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.72G/9.98G [03:13<00:53, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███ | 7.73G/9.98G [03:13<01:08, 33.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 7.74G/9.98G [03:13<01:01, 36.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 7.75G/9.98G [03:14<00:57, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 7.76G/9.98G [03:14<00:54, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 7.77G/9.98G [03:14<00:52, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 7.78G/9.98G [03:14<00:50, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███ | 7.79G/9.98G [03:14<00:49, 44.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▏| 7.80G/9.98G [03:15<00:48, 44.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▏| 7.81G/9.98G [03:15<00:47, 45.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▏| 7.82G/9.98G [03:15<00:47, 45.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▏| 7.83G/9.98G [03:15<00:47, 45.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.84G/9.98G [03:16<00:48, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.85G/9.98G [03:16<00:49, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.86G/9.98G [03:16<00:49, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.87G/9.98G [03:16<00:51, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.89G/9.98G [03:17<00:50, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.90G/9.98G [03:17<00:50, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.91G/9.98G [03:17<00:49, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.92G/9.98G [03:17<00:49, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▏| 7.93G/9.98G [03:18<00:48, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 7.94G/9.98G [03:18<00:48, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 7.95G/9.98G [03:18<00:48, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 7.96G/9.98G [03:18<00:48, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 7.97G/9.98G [03:19<00:48, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 7.98G/9.98G [03:19<00:47, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 7.99G/9.98G [03:19<00:47, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 8.00G/9.98G [03:19<00:46, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 8.01G/9.98G [03:20<00:46, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 8.02G/9.98G [03:20<00:46, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▏| 8.03G/9.98G [03:20<00:46, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 8.04G/9.98G [03:20<00:46, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 8.05G/9.98G [03:21<00:45, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 8.06G/9.98G [03:21<00:45, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 8.07G/9.98G [03:21<00:47, 39.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 8.08G/9.98G [03:21<00:47, 39.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 8.10G/9.98G [03:22<00:45, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▏| 8.11G/9.98G [03:22<00:43, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▎| 8.12G/9.98G [03:22<00:42, 43.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|███▎| 8.13G/9.98G [03:22<00:42, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.14G/9.98G [03:23<00:42, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.15G/9.98G [03:23<00:42, 42.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.16G/9.98G [03:23<00:43, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.17G/9.98G [03:23<00:42, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.18G/9.98G [03:24<00:42, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.19G/9.98G [03:24<00:42, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.20G/9.98G [03:24<00:42, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.21G/9.98G [03:24<00:42, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.22G/9.98G [03:25<00:41, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|███▎| 8.23G/9.98G [03:25<00:41, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.24G/9.98G [03:25<00:41, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.25G/9.98G [03:25<00:40, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.26G/9.98G [03:26<00:40, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.27G/9.98G [03:26<00:40, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.28G/9.98G [03:26<00:40, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.29G/9.98G [03:26<00:40, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.30G/9.98G [03:27<00:39, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.32G/9.98G [03:27<00:39, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|███▎| 8.33G/9.98G [03:27<00:39, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 8.34G/9.98G [03:27<00:38, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 8.35G/9.98G [03:28<00:38, 42.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 8.36G/9.98G [03:28<00:38, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 8.37G/9.98G [03:28<00:38, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 8.38G/9.98G [03:28<00:37, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 8.39G/9.98G [03:29<00:37, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 8.40G/9.98G [03:29<00:37, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▎| 8.41G/9.98G [03:29<00:37, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▍| 8.42G/9.98G [03:29<00:38, 40.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|███▍| 8.43G/9.98G [03:30<00:37, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.44G/9.98G [03:30<00:37, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.45G/9.98G [03:30<00:36, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.46G/9.98G [03:30<00:36, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.47G/9.98G [03:31<00:35, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.48G/9.98G [03:31<00:35, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.49G/9.98G [03:31<00:35, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.50G/9.98G [03:32<00:44, 33.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.51G/9.98G [03:32<00:40, 36.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|███▍| 8.52G/9.98G [03:32<00:37, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.54G/9.98G [03:32<00:35, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.55G/9.98G [03:32<00:33, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.56G/9.98G [03:33<00:32, 43.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.57G/9.98G [03:33<00:34, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.58G/9.98G [03:33<00:33, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.59G/9.98G [03:34<00:59, 23.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.60G/9.98G [03:35<01:04, 21.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.61G/9.98G [03:35<00:57, 23.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.62G/9.98G [03:36<00:56, 23.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|███▍| 8.63G/9.98G [03:36<00:54, 24.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.64G/9.98G [03:36<00:48, 27.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.65G/9.98G [03:37<00:47, 28.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.66G/9.98G [03:37<00:42, 31.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.67G/9.98G [03:37<00:38, 33.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.68G/9.98G [03:37<00:36, 35.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.69G/9.98G [03:38<00:34, 37.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.70G/9.98G [03:38<00:33, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.71G/9.98G [03:38<00:32, 39.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|███▍| 8.72G/9.98G [03:38<00:34, 36.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.73G/9.98G [03:39<00:33, 37.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.75G/9.98G [03:39<00:32, 38.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.76G/9.98G [03:39<00:31, 39.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.77G/9.98G [03:39<00:32, 37.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.78G/9.98G [03:40<00:31, 38.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.79G/9.98G [03:40<00:30, 39.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.80G/9.98G [03:40<00:29, 40.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.81G/9.98G [03:41<00:31, 37.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.82G/9.98G [03:41<00:30, 38.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.83G/9.98G [03:41<00:32, 35.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.84G/9.98G [03:41<00:31, 36.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.85G/9.98G [03:42<00:29, 38.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.86G/9.98G [03:42<00:30, 36.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.87G/9.98G [03:42<00:30, 36.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.88G/9.98G [03:43<00:29, 37.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.89G/9.98G [03:43<00:28, 38.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.90G/9.98G [03:43<00:35, 30.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.91G/9.98G [03:44<00:31, 34.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|███▌| 8.92G/9.98G [03:44<00:28, 36.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 8.93G/9.98G [03:44<00:26, 39.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 8.94G/9.98G [03:44<00:25, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 8.95G/9.98G [03:44<00:24, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 8.97G/9.98G [03:45<00:24, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 8.98G/9.98G [03:45<00:24, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 8.99G/9.98G [03:45<00:23, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 9.00G/9.98G [03:45<00:23, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 9.01G/9.98G [03:46<00:23, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 9.02G/9.98G [03:46<00:23, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|███▌| 9.03G/9.98G [03:46<00:22, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▌| 9.04G/9.98G [03:46<00:22, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 9.05G/9.98G [03:47<00:22, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 9.06G/9.98G [03:47<00:22, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 9.07G/9.98G [03:47<00:21, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 9.08G/9.98G [03:48<00:22, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 9.09G/9.98G [03:48<00:26, 33.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 9.10G/9.98G [03:48<00:24, 35.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 9.11G/9.98G [03:48<00:23, 36.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|███▋| 9.12G/9.98G [03:49<00:22, 37.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.13G/9.98G [03:49<00:21, 39.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.14G/9.98G [03:49<00:21, 39.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.15G/9.98G [03:49<00:20, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.16G/9.98G [03:50<00:20, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.18G/9.98G [03:50<00:19, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.19G/9.98G [03:50<00:19, 40.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.20G/9.98G [03:51<00:19, 39.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.21G/9.98G [03:51<00:19, 40.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.22G/9.98G [03:51<00:18, 40.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.23G/9.98G [03:51<00:18, 40.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.24G/9.98G [03:52<00:18, 41.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.25G/9.98G [03:52<00:17, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.26G/9.98G [03:52<00:17, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.27G/9.98G [03:52<00:16, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.28G/9.98G [03:53<00:16, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.29G/9.98G [03:53<00:16, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.30G/9.98G [03:53<00:16, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.31G/9.98G [03:53<00:16, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.32G/9.98G [03:54<00:15, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▋| 9.33G/9.98G [03:54<00:15, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▋| 9.34G/9.98G [03:54<00:15, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▋| 9.35G/9.98G [03:54<00:15, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 9.36G/9.98G [03:55<00:14, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 9.37G/9.98G [03:55<00:14, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 9.38G/9.98G [03:55<00:14, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 9.40G/9.98G [03:55<00:14, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 9.41G/9.98G [03:56<00:17, 32.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 9.42G/9.98G [03:56<00:15, 35.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|███▊| 9.43G/9.98G [03:56<00:14, 38.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.44G/9.98G [03:56<00:13, 40.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.45G/9.98G [03:57<00:12, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.46G/9.98G [03:57<00:12, 43.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.47G/9.98G [03:57<00:11, 44.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.48G/9.98G [03:57<00:11, 43.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.49G/9.98G [03:58<00:11, 43.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.50G/9.98G [03:58<00:11, 42.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.51G/9.98G [03:58<00:11, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.52G/9.98G [03:58<00:10, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.53G/9.98G [03:59<00:10, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.54G/9.98G [03:59<00:10, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.55G/9.98G [03:59<00:10, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.56G/9.98G [03:59<00:09, 42.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.57G/9.98G [04:00<00:09, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.58G/9.98G [04:00<00:09, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.59G/9.98G [04:00<00:09, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.60G/9.98G [04:00<00:08, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.62G/9.98G [04:01<00:08, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.63G/9.98G [04:01<00:08, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▊| 9.64G/9.98G [04:01<00:08, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▊| 9.65G/9.98G [04:01<00:07, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▊| 9.66G/9.98G [04:02<00:07, 41.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 9.67G/9.98G [04:02<00:07, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 9.68G/9.98G [04:02<00:07, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 9.69G/9.98G [04:02<00:06, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 9.70G/9.98G [04:03<00:06, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 9.71G/9.98G [04:03<00:06, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 9.72G/9.98G [04:03<00:06, 41.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.73G/9.98G [04:03<00:05, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.74G/9.98G [04:04<00:05, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.75G/9.98G [04:04<00:05, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.76G/9.98G [04:04<00:05, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.77G/9.98G [04:04<00:04, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.78G/9.98G [04:05<00:04, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.79G/9.98G [04:05<00:04, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.80G/9.98G [04:05<00:04, 42.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.81G/9.98G [04:05<00:03, 42.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|███▉| 9.83G/9.98G [04:06<00:03, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.84G/9.98G [04:06<00:03, 42.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.85G/9.98G [04:06<00:03, 42.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.86G/9.98G [04:06<00:02, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.87G/9.98G [04:07<00:02, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.88G/9.98G [04:07<00:02, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.89G/9.98G [04:07<00:02, 41.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.90G/9.98G [04:07<00:01, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.91G/9.98G [04:08<00:01, 41.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.92G/9.98G [04:08<00:01, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 9.93G/9.98G [04:08<00:01, 41.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 9.94G/9.98G [04:08<00:00, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 9.95G/9.98G [04:09<00:00, 41.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 9.96G/9.98G [04:09<00:00, 40.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|███▉| 9.97G/9.98G [04:09<00:00, 40.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|████| 9.98G/9.98G [04:09<00:00, 39.9MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████            | 1/2 [04:10<04:10, 250.45s/it]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0%|    | 10.5M/3.50G [00:00<01:21, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 21.0M/3.50G [00:00<01:21, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 31.5M/3.50G [00:00<01:21, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 41.9M/3.50G [00:00<01:20, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 52.4M/3.50G [00:01<01:21, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 62.9M/3.50G [00:01<01:22, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 73.4M/3.50G [00:01<01:21, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 83.9M/3.50G [00:02<01:44, 32.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|    | 94.4M/3.50G [00:02<01:34, 36.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|▏    | 105M/3.50G [00:02<01:27, 38.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|▏    | 115M/3.50G [00:02<01:23, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏    | 126M/3.50G [00:03<01:20, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏    | 136M/3.50G [00:03<01:23, 40.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏    | 147M/3.50G [00:03<01:19, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏    | 157M/3.50G [00:03<01:17, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▏    | 168M/3.50G [00:04<01:15, 44.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎    | 178M/3.50G [00:04<01:14, 44.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎    | 189M/3.50G [00:04<01:13, 45.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 199M/3.50G [00:04<01:12, 45.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 210M/3.50G [00:04<01:12, 45.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 220M/3.50G [00:05<01:14, 44.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 231M/3.50G [00:05<01:16, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 241M/3.50G [00:05<01:16, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 252M/3.50G [00:05<01:16, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 262M/3.50G [00:06<01:16, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 273M/3.50G [00:06<01:17, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 283M/3.50G [00:06<01:16, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 294M/3.50G [00:07<01:16, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 304M/3.50G [00:07<01:15, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 315M/3.50G [00:07<01:16, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 325M/3.50G [00:07<01:16, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▍    | 336M/3.50G [00:08<01:15, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▍    | 346M/3.50G [00:08<01:15, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌    | 357M/3.50G [00:08<01:15, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌    | 367M/3.50G [00:08<01:14, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 377M/3.50G [00:09<01:14, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 388M/3.50G [00:09<01:14, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 398M/3.50G [00:09<01:17, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▌    | 409M/3.50G [00:09<01:15, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▌    | 419M/3.50G [00:10<01:15, 41.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▌    | 430M/3.50G [00:10<01:13, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 440M/3.50G [00:10<01:14, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 451M/3.50G [00:10<01:14, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 461M/3.50G [00:11<01:13, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 472M/3.50G [00:11<01:12, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 482M/3.50G [00:11<01:12, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 493M/3.50G [00:11<01:11, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 503M/3.50G [00:12<01:11, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▋    | 514M/3.50G [00:12<01:11, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▋    | 524M/3.50G [00:12<01:10, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▊    | 535M/3.50G [00:12<01:10, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 545M/3.50G [00:13<01:10, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 556M/3.50G [00:13<01:10, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 566M/3.50G [00:13<01:11, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 577M/3.50G [00:13<01:11, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 587M/3.50G [00:14<01:30, 32.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 598M/3.50G [00:14<01:22, 35.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 608M/3.50G [00:14<01:16, 38.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 619M/3.50G [00:15<01:11, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 629M/3.50G [00:15<01:08, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 640M/3.50G [00:15<01:06, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 650M/3.50G [00:15<01:04, 44.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 661M/3.50G [00:15<01:10, 40.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 671M/3.50G [00:16<01:14, 38.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 682M/3.50G [00:16<01:10, 40.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|▉    | 692M/3.50G [00:16<01:07, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█    | 703M/3.50G [00:16<01:05, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█    | 713M/3.50G [00:17<01:03, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█    | 724M/3.50G [00:17<01:02, 44.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█    | 734M/3.50G [00:17<01:01, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█    | 744M/3.50G [00:17<01:00, 45.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 755M/3.50G [00:18<01:00, 45.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 765M/3.50G [00:18<01:00, 45.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 776M/3.50G [00:18<01:01, 44.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 786M/3.50G [00:18<01:01, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 797M/3.50G [00:19<01:02, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 807M/3.50G [00:19<01:03, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 818M/3.50G [00:19<01:03, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 828M/3.50G [00:19<01:03, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 839M/3.50G [00:20<01:05, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 849M/3.50G [00:20<01:04, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▏   | 860M/3.50G [00:20<01:03, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▏   | 870M/3.50G [00:20<01:05, 40.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▎   | 881M/3.50G [00:21<01:06, 39.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▎   | 891M/3.50G [00:21<01:08, 37.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 902M/3.50G [00:21<01:06, 39.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 912M/3.50G [00:21<01:04, 40.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 923M/3.50G [00:22<01:03, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 933M/3.50G [00:22<01:02, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 944M/3.50G [00:22<01:01, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 954M/3.50G [00:22<01:00, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 965M/3.50G [00:23<01:00, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 975M/3.50G [00:23<01:00, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 986M/3.50G [00:23<01:02, 40.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 996M/3.50G [00:24<01:01, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▏  | 1.01G/3.50G [00:24<01:00, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▏  | 1.02G/3.50G [00:24<00:59, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▏  | 1.03G/3.50G [00:24<01:01, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▏  | 1.04G/3.50G [00:25<01:01, 40.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▏  | 1.05G/3.50G [00:25<01:00, 40.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▏  | 1.06G/3.50G [00:25<01:00, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▏  | 1.07G/3.50G [00:25<01:00, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▏  | 1.08G/3.50G [00:26<01:16, 31.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▏  | 1.09G/3.50G [00:26<01:08, 35.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▎  | 1.10G/3.50G [00:26<01:03, 37.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▎  | 1.11G/3.50G [00:27<00:59, 40.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▎  | 1.12G/3.50G [00:27<00:57, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▎  | 1.13G/3.50G [00:27<00:55, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▎  | 1.14G/3.50G [00:27<00:53, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▎  | 1.15G/3.50G [00:27<00:52, 44.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▎  | 1.16G/3.50G [00:28<00:51, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▎  | 1.17G/3.50G [00:28<00:51, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▎  | 1.18G/3.50G [00:28<00:52, 43.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▎  | 1.20G/3.50G [00:28<00:53, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▍  | 1.21G/3.50G [00:29<00:53, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▍  | 1.22G/3.50G [00:29<00:53, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▍  | 1.23G/3.50G [00:29<00:54, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▍  | 1.24G/3.50G [00:29<00:54, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▍  | 1.25G/3.50G [00:30<00:55, 41.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▍  | 1.26G/3.50G [00:30<00:55, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▍  | 1.27G/3.50G [00:30<00:54, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▍  | 1.28G/3.50G [00:30<00:53, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▍  | 1.29G/3.50G [00:31<00:53, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▍  | 1.30G/3.50G [00:31<00:52, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▍  | 1.31G/3.50G [00:31<00:53, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▌  | 1.32G/3.50G [00:32<01:05, 33.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▌  | 1.33G/3.50G [00:32<00:59, 36.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▌  | 1.34G/3.50G [00:32<00:55, 38.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▌  | 1.35G/3.50G [00:32<00:52, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▌  | 1.36G/3.50G [00:33<00:50, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▌  | 1.37G/3.50G [00:33<00:49, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▌  | 1.38G/3.50G [00:33<00:47, 44.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▌  | 1.39G/3.50G [00:33<00:47, 44.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▌  | 1.41G/3.50G [00:33<00:46, 45.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▌  | 1.42G/3.50G [00:34<00:45, 45.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|█▋  | 1.43G/3.50G [00:34<00:46, 44.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|█▋  | 1.44G/3.50G [00:34<00:47, 43.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|█▋  | 1.45G/3.50G [00:35<00:51, 39.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|█▋  | 1.46G/3.50G [00:35<00:51, 39.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|█▋  | 1.47G/3.50G [00:35<00:50, 40.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|█▋  | 1.48G/3.50G [00:35<00:49, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|█▋  | 1.49G/3.50G [00:36<00:48, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|█▋  | 1.50G/3.50G [00:36<00:50, 39.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|█▋  | 1.51G/3.50G [00:36<01:01, 32.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|█▋  | 1.52G/3.50G [00:37<00:57, 34.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|█▋  | 1.53G/3.50G [00:37<00:52, 37.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|█▊  | 1.54G/3.50G [00:37<00:49, 39.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|█▊  | 1.55G/3.50G [00:37<00:47, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|█▊  | 1.56G/3.50G [00:37<00:45, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|█▊  | 1.57G/3.50G [00:38<00:44, 43.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|█▊  | 1.58G/3.50G [00:38<00:43, 44.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.59G/3.50G [00:38<00:43, 43.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.60G/3.50G [00:38<00:43, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.61G/3.50G [00:39<00:44, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.63G/3.50G [00:39<00:44, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|█▊  | 1.64G/3.50G [00:39<00:44, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|█▉  | 1.65G/3.50G [00:39<00:43, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|█▉  | 1.66G/3.50G [00:40<00:43, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|█▉  | 1.67G/3.50G [00:40<00:43, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|█▉  | 1.68G/3.50G [00:40<00:44, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|█▉  | 1.69G/3.50G [00:40<00:43, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.70G/3.50G [00:41<00:43, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.71G/3.50G [00:41<00:43, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.72G/3.50G [00:41<00:43, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.73G/3.50G [00:42<00:54, 32.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|█▉  | 1.74G/3.50G [00:42<00:49, 35.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|█▉  | 1.75G/3.50G [00:42<00:45, 38.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██  | 1.76G/3.50G [00:42<00:43, 40.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.77G/3.50G [00:43<00:41, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.78G/3.50G [00:43<00:39, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.79G/3.50G [00:43<00:38, 44.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.80G/3.50G [00:43<00:38, 44.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██  | 1.81G/3.50G [00:43<00:37, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██  | 1.82G/3.50G [00:44<00:36, 45.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██  | 1.84G/3.50G [00:44<00:36, 45.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██  | 1.85G/3.50G [00:44<00:37, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██  | 1.86G/3.50G [00:44<00:37, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██▏ | 1.87G/3.50G [00:45<00:38, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▏ | 1.88G/3.50G [00:45<00:38, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▏ | 1.89G/3.50G [00:45<00:38, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▏ | 1.90G/3.50G [00:45<00:37, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▏ | 1.91G/3.50G [00:46<00:37, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▏ | 1.92G/3.50G [00:46<00:37, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▏ | 1.93G/3.50G [00:46<00:37, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▏ | 1.94G/3.50G [00:46<00:36, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▏ | 1.95G/3.50G [00:47<00:37, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▏ | 1.96G/3.50G [00:47<00:37, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▎ | 1.97G/3.50G [00:47<00:36, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 1.98G/3.50G [00:47<00:36, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 1.99G/3.50G [00:48<00:36, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 2.00G/3.50G [00:48<00:35, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 2.01G/3.50G [00:48<00:35, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▎ | 2.02G/3.50G [00:48<00:35, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▎ | 2.03G/3.50G [00:49<00:35, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▎ | 2.04G/3.50G [00:49<00:35, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▎ | 2.06G/3.50G [00:49<00:34, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▎ | 2.07G/3.50G [00:49<00:34, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▎ | 2.08G/3.50G [00:50<00:34, 41.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 2.09G/3.50G [00:50<00:34, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 2.10G/3.50G [00:50<00:33, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 2.11G/3.50G [00:50<00:33, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 2.12G/3.50G [00:51<00:33, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|██▍ | 2.13G/3.50G [00:51<00:33, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|██▍ | 2.14G/3.50G [00:51<00:32, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|██▍ | 2.15G/3.50G [00:51<00:32, 41.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|██▍ | 2.16G/3.50G [00:52<00:32, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|██▍ | 2.17G/3.50G [00:52<00:32, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|██▍ | 2.18G/3.50G [00:52<00:40, 32.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██▌ | 2.19G/3.50G [00:53<00:36, 35.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██▌ | 2.20G/3.50G [00:53<00:33, 38.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██▌ | 2.21G/3.50G [00:53<00:31, 40.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██▌ | 2.22G/3.50G [00:53<00:30, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|██▌ | 2.23G/3.50G [00:54<00:29, 43.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|██▌ | 2.24G/3.50G [00:54<00:28, 44.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|██▌ | 2.25G/3.50G [00:54<00:27, 44.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|██▌ | 2.26G/3.50G [00:54<00:27, 45.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|██▌ | 2.28G/3.50G [00:54<00:27, 45.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|██▌ | 2.29G/3.50G [00:55<00:26, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▌ | 2.30G/3.50G [00:55<00:27, 43.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▋ | 2.31G/3.50G [00:55<00:27, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▋ | 2.32G/3.50G [00:55<00:27, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▋ | 2.33G/3.50G [00:56<00:27, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|██▋ | 2.34G/3.50G [00:56<00:27, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|██▋ | 2.35G/3.50G [00:56<00:27, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|██▋ | 2.36G/3.50G [00:56<00:27, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|██▋ | 2.37G/3.50G [00:57<00:27, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|██▋ | 2.38G/3.50G [00:57<00:27, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|██▋ | 2.39G/3.50G [00:57<00:27, 41.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▋ | 2.40G/3.50G [00:58<00:26, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▊ | 2.41G/3.50G [00:58<00:26, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▊ | 2.42G/3.50G [00:58<00:25, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▊ | 2.43G/3.50G [00:58<00:25, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|██▊ | 2.44G/3.50G [00:59<00:25, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|██▊ | 2.45G/3.50G [00:59<00:24, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|██▊ | 2.46G/3.50G [00:59<00:24, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|██▊ | 2.47G/3.50G [00:59<00:24, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|██▊ | 2.49G/3.50G [01:00<00:24, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|██▊ | 2.50G/3.50G [01:00<00:24, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|██▊ | 2.51G/3.50G [01:00<00:24, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|██▊ | 2.52G/3.50G [01:00<00:23, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|██▉ | 2.53G/3.50G [01:01<00:23, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|██▉ | 2.54G/3.50G [01:01<00:23, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|██▉ | 2.55G/3.50G [01:01<00:22, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|██▉ | 2.56G/3.50G [01:01<00:22, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|██▉ | 2.57G/3.50G [01:02<00:22, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|██▉ | 2.58G/3.50G [01:02<00:22, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|██▉ | 2.59G/3.50G [01:02<00:21, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|██▉ | 2.60G/3.50G [01:02<00:21, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|██▉ | 2.61G/3.50G [01:03<00:22, 40.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|██▉ | 2.62G/3.50G [01:03<00:21, 41.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|███ | 2.63G/3.50G [01:03<00:21, 41.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|███ | 2.64G/3.50G [01:03<00:21, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███ | 2.65G/3.50G [01:04<00:20, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███ | 2.66G/3.50G [01:04<00:21, 39.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███ | 2.67G/3.50G [01:04<00:20, 39.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███ | 2.68G/3.50G [01:04<00:20, 39.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███ | 2.69G/3.50G [01:05<00:20, 40.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███ | 2.71G/3.50G [01:05<00:19, 40.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███ | 2.72G/3.50G [01:05<00:19, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███ | 2.73G/3.50G [01:05<00:19, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███▏| 2.74G/3.50G [01:06<00:18, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███▏| 2.75G/3.50G [01:06<00:18, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▏| 2.76G/3.50G [01:06<00:17, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▏| 2.77G/3.50G [01:06<00:17, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▏| 2.78G/3.50G [01:07<00:17, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▏| 2.79G/3.50G [01:07<00:17, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▏| 2.80G/3.50G [01:07<00:16, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▏| 2.81G/3.50G [01:07<00:16, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|███▏| 2.82G/3.50G [01:08<00:16, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|███▏| 2.83G/3.50G [01:08<00:20, 32.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|███▏| 2.84G/3.50G [01:08<00:18, 35.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|███▎| 2.85G/3.50G [01:09<00:16, 38.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|███▎| 2.86G/3.50G [01:09<00:15, 40.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|███▎| 2.87G/3.50G [01:09<00:14, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|███▎| 2.88G/3.50G [01:09<00:14, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|███▎| 2.89G/3.50G [01:10<00:13, 43.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|███▎| 2.90G/3.50G [01:10<00:13, 44.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|███▎| 2.92G/3.50G [01:10<00:13, 45.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|███▎| 2.93G/3.50G [01:10<00:12, 45.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|███▎| 2.94G/3.50G [01:10<00:12, 45.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|███▎| 2.95G/3.50G [01:11<00:12, 45.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|███▍| 2.96G/3.50G [01:11<00:12, 44.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|███▍| 2.97G/3.50G [01:11<00:12, 43.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|███▍| 2.98G/3.50G [01:11<00:12, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|███▍| 2.99G/3.50G [01:12<00:11, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|███▍| 3.00G/3.50G [01:12<00:11, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|███▍| 3.01G/3.50G [01:12<00:11, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|███▍| 3.02G/3.50G [01:12<00:11, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|███▍| 3.03G/3.50G [01:13<00:11, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|███▍| 3.04G/3.50G [01:13<00:11, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|███▍| 3.05G/3.50G [01:13<00:10, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|███▍| 3.06G/3.50G [01:13<00:10, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|███▌| 3.07G/3.50G [01:14<00:10, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|███▌| 3.08G/3.50G [01:14<00:09, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|███▌| 3.09G/3.50G [01:14<00:09, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|███▌| 3.10G/3.50G [01:14<00:09, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|███▌| 3.11G/3.50G [01:15<00:09, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|███▌| 3.12G/3.50G [01:15<00:08, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|███▌| 3.14G/3.50G [01:15<00:08, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|███▌| 3.15G/3.50G [01:15<00:08, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|███▌| 3.16G/3.50G [01:16<00:08, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|███▌| 3.17G/3.50G [01:16<00:08, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|███▋| 3.18G/3.50G [01:16<00:07, 41.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|███▋| 3.19G/3.50G [01:16<00:07, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|███▋| 3.20G/3.50G [01:17<00:07, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|███▋| 3.21G/3.50G [01:17<00:08, 33.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|███▋| 3.22G/3.50G [01:17<00:07, 35.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|███▋| 3.23G/3.50G [01:18<00:07, 38.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|███▋| 3.24G/3.50G [01:18<00:06, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|███▋| 3.25G/3.50G [01:18<00:06, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|███▋| 3.26G/3.50G [01:18<00:05, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|███▋| 3.27G/3.50G [01:19<00:05, 44.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|███▋| 3.28G/3.50G [01:19<00:04, 44.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|███▊| 3.29G/3.50G [01:19<00:04, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|███▊| 3.30G/3.50G [01:19<00:04, 45.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|███▊| 3.31G/3.50G [01:19<00:04, 45.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|███▊| 3.32G/3.50G [01:20<00:03, 45.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|███▊| 3.33G/3.50G [01:20<00:03, 44.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|███▊| 3.34G/3.50G [01:20<00:03, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|███▊| 3.36G/3.50G [01:20<00:03, 43.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|███▊| 3.37G/3.50G [01:21<00:03, 43.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|███▊| 3.38G/3.50G [01:21<00:02, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|███▊| 3.39G/3.50G [01:21<00:02, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|███▉| 3.40G/3.50G [01:21<00:02, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|███▉| 3.41G/3.50G [01:22<00:02, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|███▉| 3.42G/3.50G [01:22<00:01, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|███▉| 3.43G/3.50G [01:22<00:01, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|███▉| 3.44G/3.50G [01:22<00:01, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|███▉| 3.45G/3.50G [01:23<00:01, 39.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|███▉| 3.46G/3.50G [01:23<00:01, 40.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|███▉| 3.47G/3.50G [01:23<00:00, 41.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|███▉| 3.48G/3.50G [01:23<00:00, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|███▉| 3.49G/3.50G [01:24<00:00, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|████| 3.50G/3.50G [01:24<00:00, 41.5MB/s]\u001b[A\n",
      "Downloading shards: 100%|████████████████████████| 2/2 [05:35<00:00, 167.70s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  9.92it/s]\n",
      "generation_config.json: 100%|██████████████████| 119/119 [00:00<00:00, 1.46MB/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class ADD {\n",
      "    public static int add(int x, int y) {\n",
      "        return x+y;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class ADD {\n",
      "    public static int add(int x, int y) {\n",
      "        return x+y;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class ADD_ELEMENTS {\n",
      "    public static int add_elements(ArrayList<Integer> arr, int k) {\n",
      "        int result = 0;\n",
      "        for(int i = 0; i < k; i++) {\n",
      "            result += arr.get(i);\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "\n",
      "    public static int add_elements_2(ArrayList<Integer> arr, int k) {\n",
      "        int result = 0;\n",
      "        for(int i = 0; i < arr.size(); i++) {\n",
      "            if(i < k) {\n",
      "                result += arr.get(i);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class ADD_ELEMENTS {\n",
      "    public static int add_elements(ArrayList<Integer> arr, int k) {\n",
      "        int result = 0;\n",
      "        for(int i = 0; i < k; i++) {\n",
      "            result += arr.get(i);\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "\n",
      "    public static int add_elements_2(ArrayList<Integer> arr, int k) {\n",
      "        int result = 0;\n",
      "        for(int i = 0; i < arr.size(); i++) {\n",
      "            if(i < k) {\n",
      "                result += arr.get(i);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class ADD_EVEN_AT_ODD {\n",
      "    public static int add_even_at_odd(int[] lst) {\n",
      "        int sum = 0;\n",
      "        for (int i = 0; i < lst.length; i++) {\n",
      "            if (lst[i] % 2 == 0)\n",
      "                sum += lst[i];\n",
      "        }\n",
      "        return sum;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class ADD_EVEN_AT_ODD {\n",
      "    public static int add_even_at_odd(int[] lst) {\n",
      "        int sum = 0;\n",
      "        for (int i = 0; i < lst.length; i++) {\n",
      "            if (lst[i] % 2 == 0)\n",
      "                sum += lst[i];\n",
      "        }\n",
      "        return sum;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class ALL_PREFIXES {\n",
      "    public static List<String> all_prefixes(String string){\n",
      "        List<String> result = new ArrayList<String>();\n",
      "\n",
      "        for (int i = 0; i < string.length(); i += 1){\n",
      "            result.add(string.substring(0, i + 1));\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class ALL_PREFIXES {\n",
      "    public static List<String> all_prefixes(String string){\n",
      "        List<String> result = new ArrayList<String>();\n",
      "\n",
      "        for (int i = 0; i < string.length(); i += 1){\n",
      "            result.add(string.substring(0, i + 1));\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class ANTI_SHUFFLE {\n",
      "    public static String anti_shuffle(String s) {\n",
      "        String result = \"\";\n",
      "        for (String str : s.split(\" \")) {\n",
      "            char[] chars = str.toCharArray();\n",
      "            Arrays.sort(chars);\n",
      "            result += String.valueOf(chars) + \" \";\n",
      "        }\n",
      "        return result.trim();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class ANTI_SHUFFLE {\n",
      "    public static String anti_shuffle(String s) {\n",
      "        String result = \"\";\n",
      "        for (String str : s.split(\" \")) {\n",
      "            char[] chars = str.toCharArray();\n",
      "            Arrays.sort(chars);\n",
      "            result += new String(chars) + \" \";\n",
      "        }\n",
      "        return result.trim();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class ANY_INT {\n",
      "    public static boolean any_int(double x, double y, double z) {\n",
      "        if (x > 0) {\n",
      "            if (y > 0)\n",
      "                if (z > 0)\n",
      "                    return true;\n",
      "            if (z > 0)\n",
      "                return true;\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class ANY_INT {\n",
      "    public static boolean any_int(double x, double y, double z) {\n",
      "        if (x > 0)\n",
      "            return true;\n",
      "        if (y > 0)\n",
      "            return true;\n",
      "        if (z > 0) {\n",
      "            if (x > 0)\n",
      "                return true;\n",
      "            if (y > 0)\n",
      "                return true;\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class BELOW_THRESHOLD {\n",
      "    public static boolean below_threshold(int[] l, int t) {\n",
      "        for (int i = 0; i < l.length; i += 1) {\n",
      "            if (l[i] < t) {\n",
      "                return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class BELOW_THRESHOLD {\n",
      "    public static boolean below_threshold(int[] l, int t) {\n",
      "        for (int i = 0; i < l.length; i += 1) {\n",
      "            if (l[i] < t) {\n",
      "                return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class BELOW_ZERO {\n",
      "    public static boolean below_zero(List<Integer> operations) {\n",
      "        int balance = 0;\n",
      "        for (Integer op : operations){\n",
      "            balance += op;\n",
      "            if (balance < 0) return true;\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class BELOW_ZERO {\n",
      "    public static boolean below_zero(List<Integer> operations) {\n",
      "        int balance = 0;\n",
      "        for (Integer op : operations){\n",
      "            balance += op;\n",
      "            if (balance < 0)\n",
      "                return true;\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class BF {\n",
      "    public static ArrayList<String> bf(String planet1, String planet2) {\n",
      "        ArrayList<String> planet_names = new ArrayList<String>(Arrays.asList(\n",
      "            \"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"\n",
      "        ));\n",
      "\n",
      "        ArrayList<String> result = new ArrayList<String>();\n",
      "        if ((! planet_names.contains(planet1)) || (! planet_names.contains(planet2)))\n",
      "            return result;\n",
      "        int planet1_index = planet_names.indexOf(planet1);\n",
      "        int planet2_index = planet_names.indexOf(planet2);\n",
      "        if (planet1_index > planet2_index) {\n",
      "            int temp = planet1_index;\n",
      "            planet1_index = planet2_index;\n",
      "            planet2_index = temp;\n",
      "        }\n",
      "        for (int i = planet1_index; i <= planet2_index; i++)\n",
      "            result.add(planet_names.get(i));\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class BF {\n",
      "    public static ArrayList<String> bf(String planet1, String planet2) {\n",
      "        ArrayList<String> planet_names = new ArrayList<String>(Arrays.asList(\n",
      "            \"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"\n",
      "        ));\n",
      "\n",
      "        ArrayList<String> result = new ArrayList<String>();\n",
      "        if ((! planet_names.contains(planet1)) || (! planet_names.contains(planet2)))\n",
      "            return result;\n",
      "        int planet1_index = planet_names.indexOf(planet1);\n",
      "        int planet2_index = planet_names.indexOf(planet2);\n",
      "        int diff = Math.abs(planet1_index - planet2_index);\n",
      "        if (diff == 0)\n",
      "            return result;\n",
      "        if (diff == 1) {\n",
      "            result.add(planet1);\n",
      "            result.add(planet2);\n",
      "            return result;\n",
      "        }\n",
      "        if (planet1_index < planet2_index) {\n",
      "            for (int i = planet1_index + 1; i < planet2_index; i++)\n",
      "                result.add(planet_names.get(i));\n",
      "        } else {\n",
      "            for (int i = planet2_index + 1; i < planet1_index; i++)\n",
      "                result.add(planet_names.get(i));\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class BY_LENGTH {\n",
      "    public static ArrayList<String> by_length(ArrayList<Integer> a) {\n",
      "        Map<Integer, String> digit_map = new HashMap<>();\n",
      "        digit_map.put(1, \"One\");\n",
      "        digit_map.put(2, \"Two\");\n",
      "        digit_map.put(3, \"Three\");\n",
      "        digit_map.put(4, \"Four\");\n",
      "        digit_map.put(5, \"Five\");\n",
      "        digit_map.put(6, \"Six\");\n",
      "        digit_map.put(7, \"Seven\");\n",
      "        digit_map.put(8, \"Eight\");\n",
      "        digit_map.put(9, \"Nine\");\n",
      "        Collections.sort(a, Collections.reverseOrder());\n",
      "        ArrayList<String> result = new ArrayList<>();\n",
      "        for(int num : a) {\n",
      "            if(num == 0) {\n",
      "                result.add(\"Zero\");\n",
      "            } else {\n",
      "                result.add(digit_map.get(num));\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class BY_LENGTH {\n",
      "    public static ArrayList<String> by_length(ArrayList<Integer> a) {\n",
      "        Map<Integer, String> digit_map = new HashMap<>();\n",
      "        digit_map.put(1, \"One\");\n",
      "        digit_map.put(2, \"Two\");\n",
      "        digit_map.put(3, \"Three\");\n",
      "        digit_map.put(4, \"Four\");\n",
      "        digit_map.put(5, \"Five\");\n",
      "        digit_map.put(6, \"Six\");\n",
      "        digit_map.put(7, \"Seven\");\n",
      "        digit_map.put(8, \"Eight\");\n",
      "        digit_map.put(9, \"Nine\");\n",
      "        Collections.sort(a, Collections.reverseOrder());\n",
      "        ArrayList<String> result = new ArrayList<>();\n",
      "        for(int num : a) {\n",
      "            if(num == 0) {\n",
      "                result.add(\"Zero\");\n",
      "            } else {\n",
      "                result.add(digit_map.get(num));\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CAN_ARRANGE {\n",
      "    public static int can_arrange(int[] arr) {\n",
      "        int ind = 0;\n",
      "        for (int i = 0; i < arr.length; i++) {\n",
      "            if (arr[i] != i + 1) {\n",
      "                ind = i + 1;\n",
      "            }\n",
      "        }\n",
      "        return ind;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CAN_ARRANGE {\n",
      "    public static int can_arrange(int[] arr) {\n",
      "        int ind = 0;\n",
      "        for (int i = 0; i < arr.length; i++) {\n",
      "            if (arr[i] == i + 1) {\n",
      "                ind++;\n",
      "            }\n",
      "        }\n",
      "        return ind;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CAR_RACE_COLLISION {\n",
      "    public static int car_race_collision(int n) {\n",
      "        return n * n;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CAR_RACE_COLLISION {\n",
      "    public static int car_race_collision(int n) {\n",
      "        if (n == 1) {\n",
      "            return 1;\n",
      "        }\n",
      "        return car_race_collision(n - 1) + n - 1;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CHANGE_BASE {\n",
      "    public static String change_base(int x, int base) {\n",
      "        String result = \"\";\n",
      "\n",
      "        while (x > 0) {\n",
      "            result = x % base + result;\n",
      "            x /= base;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CHANGE_BASE {\n",
      "    public static String change_base(int x, int base) {\n",
      "        String result = \"\";\n",
      "\n",
      "        while (x > 0) {\n",
      "            int r = x % base;\n",
      "            result = r + result;\n",
      "            x /= base;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CHECK_DICT_CASE {\n",
      "    public static boolean check_dict_case(HashMap<Object, Object> dict) {\n",
      "        if (dict.size() == 0)\n",
      "            return false;\n",
      "        else {\n",
      "            String state = \"start\";\n",
      "            for (Object key : dict.keySet()) {\n",
      "                if (!(key instanceof String)) {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                }\n",
      "                String str_key = (String) key;\n",
      "                if (state.equals(\"start\")) {\n",
      "                    if (str_key.toUpperCase().equals(str_key))\n",
      "                        state = \"upper\";\n",
      "                    else if (str_key.toLowerCase().equals(str_key))\n",
      "                        state = \"lower\";\n",
      "                    else {\n",
      "                        state = \"mixed\";\n",
      "                        break;\n",
      "                    }\n",
      "                } else if (!(str_key.toLowerCase().equals(str_key) && state.equals(\"upper\")) && !(str_key.toUpperCase().equals(str_key) && state.equals(\"lower\"))) {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                }\n",
      "            }\n",
      "            return (state.equals(\"upper\")) || (state.equals(\"lower\"));\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CHECK_DICT_CASE {\n",
      "    public static boolean check_dict_case(HashMap<Object, Object> dict) {\n",
      "        if (dict.size() == 0)\n",
      "            return false;\n",
      "        else {\n",
      "            String state = \"start\";\n",
      "            for (Object key : dict.keySet()) {\n",
      "                if (!(key instanceof String)) {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                }\n",
      "                String str_key = (String) key;\n",
      "                if (state.equals(\"start\")) {\n",
      "                    if (str_key.toUpperCase().equals(str_key))\n",
      "                        state = \"upper\";\n",
      "                    else if (str_key.toLowerCase().equals(str_key))\n",
      "                        state = \"lower\";\n",
      "                    else {\n",
      "                        state = \"mixed\";\n",
      "                        break;\n",
      "                    }\n",
      "                } else if (state.equals(\"upper\")) {\n",
      "                    if (str_key.toLowerCase().equals(str_key)) {\n",
      "                        state = \"mixed\";\n",
      "                        break;\n",
      "                    }\n",
      "                } else if (state.equals(\"lower\")) {\n",
      "                    if (str_key.toUpperCase().equals(str_key)) {\n",
      "                        state = \"mixed\";\n",
      "                        break;\n",
      "                    }\n",
      "                } else {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                }\n",
      "            }\n",
      "            return (state.equals(\"upper\")) || (state.equals(\"lower\"));\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CHECK_IF_LAST_CHAR_IS_A_LETTER {\n",
      "    public static boolean check_if_last_char_is_a_letter(String txt) {\n",
      "        String[] parts = txt.split(\" \");\n",
      "        String last = \" \";\n",
      "        for (int i = 0; i < parts.length; i++) {\n",
      "            last = parts[i];\n",
      "        }\n",
      "        return last.matches(\"[a-zA-Z]+\");\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CHECK_IF_LAST_CHAR_IS_A_LETTER {\n",
      "    public static boolean check_if_last_char_is_a_letter(String txt) {\n",
      "        String[] parts = txt.split(\" \");\n",
      "        String last = \" \";\n",
      "        for (int i = 0; i < parts.length; i++) {\n",
      "            last = parts[i];\n",
      "        }\n",
      "        return Character.isLetter(last.charAt(last.length() - 1));\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CHOOSE_NUM {\n",
      "    public static int choose_num(int x, int y) {\n",
      "        if((x % 2) == 0) return x;\n",
      "        if((y % 2) == 0) return y;\n",
      "        if(x == y) return -1;\n",
      "        return y - 1;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CHOOSE_NUM {\n",
      "    public static int choose_num(int x, int y) {\n",
      "        if((y % 2) == 0) return y;\n",
      "        if(x == y) return -1;\n",
      "        return y - 1;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CIRCULAR_SHIFT {\n",
      "    public static String circular_shift(int x, int shift) {\n",
      "        String s = x + \"\";\n",
      "\n",
      "        if (shift > s.length()) {\n",
      "            StringBuilder sb = new StringBuilder();\n",
      "            for (int i = s.length() - 1; i >= 0; i -= 1)\n",
      "                sb.append(s.charAt(i));\n",
      "            return sb.toString();\n",
      "        } else {\n",
      "            StringBuilder sb = new StringBuilder();\n",
      "            for (int i = s.length() - 1; i >= shift; i -= 1)\n",
      "                sb.append(s.charAt(i));\n",
      "            for (int i = shift - 1; i >= 0; i -= 1)\n",
      "                sb.append(s.charAt(i));\n",
      "            return sb.toString();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CIRCULAR_SHIFT {\n",
      "    public static String circular_shift(int x, int shift) {\n",
      "        String s = x + \"\";\n",
      "\n",
      "        if (shift > s.length()) {\n",
      "            StringBuilder sb = new StringBuilder();\n",
      "            for (int i = s.length() - 1; i >= 0; i -= 1)\n",
      "                sb.append(s.charAt(i));\n",
      "            return sb.toString();\n",
      "        } else {\n",
      "            StringBuilder sb = new StringBuilder();\n",
      "            for (int i = s.length() - 1; i >= 0; i -= 1)\n",
      "                sb.append(s.charAt(i));\n",
      "            return sb.toString().substring(shift) + sb.toString().substring(0, shift);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CLOSEST_INTEGER {\n",
      "    public static int closest_integer(String value) {\n",
      "        if (value.indexOf('.') >= 0) {\n",
      "            while (value.charAt(value.length() - 1) == '0')\n",
      "                value = value.substring(0, value.length() - 1);\n",
      "        }\n",
      "\n",
      "        double num = Double.parseDouble(value);\n",
      "        int result = 0;\n",
      "        if (num < 0) {\n",
      "            result = (int)Math.floor(num);\n",
      "        } else if (value.length() > 0){\n",
      "            result = (int)Math.round(num);\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CLOSEST_INTEGER {\n",
      "    public static int closest_integer(String value) {\n",
      "        if (value.indexOf('.') >= 0) {\n",
      "            while (value.charAt(value.length() - 1) == '0')\n",
      "                value = value.substring(0, value.length() - 1);\n",
      "        }\n",
      "\n",
      "        double num = Double.parseDouble(value);\n",
      "        int result = 0;\n",
      "\n",
      "        if (num < 0) {\n",
      "            result = (int)Math.floor(num);\n",
      "        } else if (value.length() > 0){\n",
      "            result = (int)Math.round(num);\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class COMMON {\n",
      "    public static int[] common(int[] l1, int[] l2) {\n",
      "        ArrayList<Integer> common = new ArrayList<Integer>();\n",
      "        \n",
      "        for (int i = 0; i < l1.length; i += 1){\n",
      "            for (int j = 0; j < l2.length; j += 1){\n",
      "                if (l1[i] == l2[j])\n",
      "                    common.add(l1[i]);\n",
      "            }\n",
      "        }\n",
      "        int[] result = new int[common.size()];\n",
      "        int index = 0;\n",
      "        for (Integer c : common) {\n",
      "            result[index] = c;\n",
      "            index += 1;\n",
      "        }\n",
      "        Arrays.sort(result);\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class COMMON {\n",
      "    public static int[] common(int[] l1, int[] l2) {\n",
      "        Set<Integer> common = new HashSet<Integer>();\n",
      "        \n",
      "        for (int i = 0; i < l1.length; i += 1){\n",
      "            for (int j = 0; j < l2.length; j += 1){\n",
      "                if (l1[i] == l2[j])\n",
      "                    common.add(l1[i]);\n",
      "            }\n",
      "        }\n",
      "        int[] result = new int[common.size()];\n",
      "        int index = 0;\n",
      "        for (Integer c : common) {\n",
      "            result[index] = c;\n",
      "            index += 1;\n",
      "        }\n",
      "        Arrays.sort(result);\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class COMPARE {\n",
      "    public static int[] compare(int[] game, int[] guess) {\n",
      "        int[] result = new int[game.length];\n",
      "\n",
      "        for (int i = 0; i < game.length; i += 1) {\n",
      "            if (game[i] == guess[i]) {\n",
      "                result[i] = 1;\n",
      "            } else {\n",
      "                result[i] = 0;\n",
      "            }\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class COMPARE {\n",
      "    public static int[] compare(int[] game, int[] guess) {\n",
      "        int[] result = new int[game.length];\n",
      "\n",
      "        for (int i = 0; i < game.length; i += 1) {\n",
      "            if (game[i] == guess[i]) {\n",
      "                result[i] = 1;\n",
      "            } else {\n",
      "                result[i] = 0;\n",
      "            }\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class COMPARE_ONE {\n",
      "    public static Object compare_one(Object a, Object b) {\n",
      "        double temp_a = 0, temp_b = 0;\n",
      "        if(a instanceof String) {\n",
      "            String temp_a_string = a.toString();\n",
      "            temp_a_string = temp_a_string.replace(',', '.');\n",
      "            temp_a = Double.parseDouble(temp_a_string);\n",
      "        }\n",
      "        if(b instanceof String) {\n",
      "            String temp_b_string = b.toString();\n",
      "            temp_b_string = temp_b_string.replace(',', '.');\n",
      "            temp_b = Double.parseDouble(temp_b_string);\n",
      "        }\n",
      "        if(temp_a == temp_b) return null;\n",
      "        if(temp_a > temp_b) return a;\n",
      "        else return b;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class COMPARE_ONE {\n",
      "    public static Object compare_one(Object a, Object b) {\n",
      "        double temp_a = 0, temp_b = 0;\n",
      "        if(a instanceof String) {\n",
      "            String temp_a_string = a.toString();\n",
      "            temp_a_string = temp_a_string.replace(',', '.');\n",
      "            temp_a = Double.parseDouble(temp_a_string);\n",
      "        }\n",
      "        if(b instanceof String) {\n",
      "            String temp_b_string = b.toString();\n",
      "            temp_b_string = temp_b_string.replace(',', '.');\n",
      "            temp_b = Double.parseDouble(temp_b_string);\n",
      "        }\n",
      "        if(a instanceof Integer) temp_a = (int) a;\n",
      "        if(b instanceof Integer) temp_b = (int) b;\n",
      "        if(temp_a == temp_b) return null;\n",
      "        if(temp_a > temp_b) return a;\n",
      "        else return b;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CONCATENATE {\n",
      "    public static String concatenate(String[] strings) {\n",
      "        String result = \"\";\n",
      "        for (String string : strings)\n",
      "            result += string;\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CONCATENATE {\n",
      "    public static String concatenate(String[] strings) {\n",
      "        String result = \"\";\n",
      "        for (String string : strings)\n",
      "            result += string;\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CORRECT_BRACKETING {\n",
      "    public static boolean correct_bracketing(String brackets) {\n",
      "        int depth = 0;\n",
      "        for (char b : brackets.toCharArray()) {\n",
      "            if (b == '<')\n",
      "                depth += 1;\n",
      "            else\n",
      "                depth -= 1;\n",
      "            if (depth < 0)\n",
      "                return false;\n",
      "        }\n",
      "        return depth == 0;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CORRECT_BRACKETING {\n",
      "    public static boolean correct_bracketing(String brackets) {\n",
      "        int depth = 0;\n",
      "        for (char b : brackets.toCharArray()) {\n",
      "            if (b == '<')\n",
      "                depth += 1;\n",
      "            else\n",
      "                depth -= 1;\n",
      "            if (depth < 0)\n",
      "                return false;\n",
      "        }\n",
      "        return depth == 0;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class COUNT_DISTINCT_CHARACTERS {\n",
      "    public static int count_distinct_characters(String string) {\n",
      "        HashSet<Character> distinct = new HashSet<Character>();\n",
      "\n",
      "        for (char c : string.toCharArray()) {\n",
      "            if (! distinct.contains(Character.toLowerCase(c)))\n",
      "                distinct.add(Character.toLowerCase(c));\n",
      "        }\n",
      "        return distinct.size();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class COUNT_DISTINCT_CHARACTERS {\n",
      "    public static int count_distinct_characters(String string) {\n",
      "        HashSet<Character> distinct = new HashSet<Character>();\n",
      "\n",
      "        for (char c : string.toCharArray()) {\n",
      "            if (! distinct.contains(Character.toLowerCase(c)))\n",
      "                distinct.add(Character.toLowerCase(c));\n",
      "        }\n",
      "        return distinct.size();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class COUNT_NUMS {\n",
      "    public static int count_nums(ArrayList<Integer> arr) {\n",
      "        int count = 0;\n",
      "        for(int num : arr) {\n",
      "            boolean neg = num < 0;\n",
      "            String num_string = Integer.toString(num);\n",
      "            int sum = 0;\n",
      "            for(int i = 0; i < num_string.length(); i++) {\n",
      "                if((i == 0) && neg) sum -= num_string.charAt(i) - '0';\n",
      "                else sum += num_string.charAt(i) - '0';\n",
      "            }\n",
      "            if(sum > 0) count++;\n",
      "        }\n",
      "        return count;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class COUNT_NUMS {\n",
      "    public static int count_nums(ArrayList<Integer> arr) {\n",
      "        int count = 0;\n",
      "        for(int num : arr) {\n",
      "            boolean neg = num < 0;\n",
      "            String num_string = Integer.toString(num);\n",
      "            int sum = 0;\n",
      "            for(int i = 0; i < num_string.length(); i++) {\n",
      "                if((i == 0) && neg) sum -= num_string.charAt(i) - '0';\n",
      "                else sum += num_string.charAt(i) - '0';\n",
      "            }\n",
      "            if(sum > 0) count++;\n",
      "        }\n",
      "        return count;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class COUNT_UP_TO {\n",
      "    public static List<Integer> count_up_to(int n) {\n",
      "        List<Integer> primes = new ArrayList<Integer>();\n",
      "\n",
      "        for (int i = 2; i < n; i += 1){\n",
      "            boolean is_prime = true;\n",
      "            for (int j = 2; j < i; j += 1){\n",
      "                if (i % j == 0){\n",
      "                    is_prime = false;\n",
      "                    break;\n",
      "                }\n",
      "            }\n",
      "            if (is_prime) primes.add(i);\n",
      "        }\n",
      "        return primes;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class COUNT_UP_TO {\n",
      "    public static List<Integer> count_up_to(int n) {\n",
      "        List<Integer> primes = new ArrayList<Integer>();\n",
      "\n",
      "        for (int i = 2; i < n; i += 1){\n",
      "            boolean is_prime = true;\n",
      "            for (int j = 2; j < i; j += 1){\n",
      "                if (i % j == 0){\n",
      "                    is_prime = false;\n",
      "                    break;\n",
      "                }\n",
      "            }\n",
      "            if (is_prime) primes.add(i);\n",
      "        }\n",
      "        return primes;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class COUNT_UPPER {\n",
      "    public static int count_upper(String s) {\n",
      "        int count = 0;\n",
      "        for (int i = 0; i < s.length(); i += 2) {\n",
      "            char c = s.charAt(i);\n",
      "            if (Character.isUpperCase(c))\n",
      "                count += 1;\n",
      "        }\n",
      "        return count;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class COUNT_UPPER {\n",
      "    public static int count_upper(String s) {\n",
      "        int count = 0;\n",
      "        for (int i = 0; i < s.length(); i += 2) {\n",
      "            char c = s.charAt(i);\n",
      "            if (Character.isUpperCase(c))\n",
      "                count += 1;\n",
      "        }\n",
      "        return count;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class CYCPATTERN_CHECK {\n",
      "    public static boolean cycpattern_check(String a, String b) {\n",
      "        int l = b.length();\n",
      "        String pat = a + a;\n",
      "        for (int i = 0; i < a.length(); i++) {\n",
      "            for (int j = 0; j < a.length(); j++) {\n",
      "                if (a.substring(i, i + l).equals(pat.substring(j, j + l))) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class CYCPATTERN_CHECK {\n",
      "    public static boolean cycpattern_check(String a, String b) {\n",
      "        int l = b.length();\n",
      "        for (int i = 0; i < a.length(); i++) {\n",
      "            for (int j = 0; j < l; j++) {\n",
      "                if (a.substring(i, i + l).equals(pat.substring(j, j + l))) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class DECIMAL_TO_BINARY {\n",
      "    public static String decimal_to_binary(int decimal) {\n",
      "        String result = \"\";\n",
      "        while (decimal > 0) {\n",
      "            result = decimal % 2 + result;\n",
      "            decimal /= 2;\n",
      "        }\n",
      "        return \"db\" + result + \"db\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class DECIMAL_TO_BINARY {\n",
      "    public static String decimal_to_binary(int decimal) {\n",
      "        String result = \"\";\n",
      "        while (decimal > 0) {\n",
      "            result = decimal % 2 + result;\n",
      "            decimal /= 2;\n",
      "        }\n",
      "        return \"db\" + result + \"db\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class DECODE_CYCLIC {\n",
      "    public static String decode_cyclic(String str) {\n",
      "        class Cyclic {\n",
      "            String encode_cyclic(String s) {\n",
      "                ArrayList<String> groups = new ArrayList<String>();\n",
      "                for (int i = 0; i < s.length(); i += 3){\n",
      "                    groups.add(s.substring(i, Math.min(s.length(), i + 3)));\n",
      "                }\n",
      "                int remain = s.length() % 3;\n",
      "                \n",
      "                String result = \"\";\n",
      "                for (String group : groups){\n",
      "                    result += group.substring(1) + group.substring(0, 1);\n",
      "                }\n",
      "\n",
      "                if (remain > 0){\n",
      "                    result += s.substring(s.length() - remain);\n",
      "                }\n",
      "\n",
      "                return result;\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        final Cyclic cyclic = new Cyclic();\n",
      "        return cyclic.encode_cyclic(cyclic.encode_cyclic(str));\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class DECODE_CYCLIC {\n",
      "    public static String decode_cyclic(String str) {\n",
      "        class Cyclic {\n",
      "            String encode_cyclic(String s) {\n",
      "                ArrayList<String> groups = new ArrayList<String>();\n",
      "                for (int i = 0; i < s.length(); i += 3) {\n",
      "                    groups.add(s.substring(i, Math.min(s.length(), i + 3)));\n",
      "                }\n",
      "                int remain = s.length() % 3;\n",
      "                \n",
      "                String result = \"\";\n",
      "                for (String group : groups){\n",
      "                    result += group.substring(1) + group.substring(0, 1);\n",
      "                }\n",
      "\n",
      "                if (remain > 0){\n",
      "                    result += s.substring(s.length() - remain);\n",
      "                }\n",
      "\n",
      "                return result;\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        final Cyclic cyclic = new Cyclic();\n",
      "        return cyclic.encode_cyclic(cyclic.encode_cyclic(str));\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class DECODE_SHIFT {\n",
      "    public static String encode_shift(String string){\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : string.toCharArray()) {\n",
      "            c = (char)((((int) c + 5 - (int)('a')) % 26) + (int)('a'));\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "\n",
      "    public static String decode_shift(String string) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : string.toCharArray()) {\n",
      "            c = (char)((((int) c - 5 - (int)('a')) % 26) + (int)('a'));\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class DECODE_SHIFT {\n",
      "    public static String encode_shift(String string){\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : string.toCharArray()) {\n",
      "            c = (char)((((int) c + 5 - (int)('a')) % 26) + (int)('a'));\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "\n",
      "    public static String decode_shift(String string) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : string.toCharArray()) {\n",
      "            c = (char)((((int) c - 5 - (int)('a')) % 26) + (int)('a'));\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class DERIVATIVE {\n",
      "    public static List<Integer> derivative(List<Integer> xs) {\n",
      "        List<Integer> result = new ArrayList<Integer>();\n",
      "        for (int i = 0; i < xs.size() - 1; i++) {\n",
      "            result.add(xs.get(i + 1) * (i + 1));\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class DERIVATIVE {\n",
      "    public static List<Integer> derivative(List<Integer> xs) {\n",
      "        List<Integer> result = new ArrayList<Integer>();\n",
      "        for (int i = 1; i < xs.size(); i++) {\n",
      "            result.add(xs.get(i) * i);\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class DIGIT_SUM {\n",
      "    public static int digit_sum(String s) {\n",
      "        int result = 0;\n",
      "        for (char c : s.toCharArray()) {\n",
      "            if ('0' <= c && c <= '9')\n",
      "                result += (int) c;\n",
      "            if ('A' <= c && c <= 'Z')\n",
      "                result += (int) c;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class DIGIT_SUM {\n",
      "    public static int digit_sum(String s) {\n",
      "        int result = 0;\n",
      "        for (char c : s.toCharArray()) {\n",
      "            if ('A' <= c && c <= 'Z')\n",
      "                result += (int) c;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class DIGITS {\n",
      "    public static int digits(int n) {\n",
      "        int product = 1;\n",
      "        int odd_count = 0;\n",
      "        while(n > 0) {\n",
      "            int digit = n % 10;\n",
      "            if(digit % 2 == 1) {\n",
      "                product *= digit;\n",
      "                odd_count++;\n",
      "            }\n",
      "            n /= 10;\n",
      "        }\n",
      "        if(odd_count == 0) return 0;\n",
      "        return product;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class DIGITS {\n",
      "    public static int digits(int n) {\n",
      "        int product = 1;\n",
      "        int odd_count = 0;\n",
      "        while(n > 0) {\n",
      "            int digit = n % 10;\n",
      "            if(digit % 2 == 1) {\n",
      "                product *= digit;\n",
      "                odd_count++;\n",
      "            }\n",
      "            n /= 10;\n",
      "        }\n",
      "        if(odd_count == 0) return 0;\n",
      "        return product;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class DO_ALGEBRA {\n",
      "    public static double do_algebra(String[] operator, int[] operand) throws NumberFormatException, ScriptException {\n",
      "        ScriptEngineManager mgr = new ScriptEngineManager();\n",
      "        ScriptEngine engine = mgr.getEngineByName(\"JavaScript\");\n",
      "        String expression = \"\";\n",
      "        for (int i = 0; i < operator.length; i++) {\n",
      "            expression += operand[i];\n",
      "            expression += operator[i];\n",
      "        }\n",
      "        return Double.parseDouble(engine.eval(expression).toString());\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class DO_ALGEBRA {\n",
      "    public static double do_algebra(String[] operator, int[] operand) throws NumberFormatException, ScriptException {\n",
      "        ScriptEngineManager mgr = new ScriptEngineManager();\n",
      "        ScriptEngine engine = mgr.getEngineByName(\"JavaScript\");\n",
      "        String expression = \"\";\n",
      "        for (int i = 0; i < operator.length; i++) {\n",
      "            expression += operand[i] + operator[i];\n",
      "        }\n",
      "        return Double.parseDouble(engine.eval(expression).toString());\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class DOUBLE_THE_DIFFERENCE {\n",
      "    public static int double_the_difference(double[] lst) {\n",
      "        int result = 0;\n",
      "        for (Double d : lst) {\n",
      "            if ((int) d.doubleValue() == d) {\n",
      "                int d_int = (int) d.doubleValue();\n",
      "                if (d_int % 2 == 1 && d_int >= 0) {\n",
      "                    result += d_int;\n",
      "                } else if (d_int % 2 == 0 && d_int >= 0) {\n",
      "                    result -= d_int;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class DOUBLE_THE_DIFFERENCE {\n",
      "    public static int double_the_difference(double[] lst) {\n",
      "        int result = 0;\n",
      "        for (Double d : lst) {\n",
      "            if ((int) d.doubleValue() == d) {\n",
      "                int d_int = (int) d.doubleValue();\n",
      "                if (d_int % 2 == 1 && d_int >= 0) {\n",
      "                    result += d_int;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class EAT {\n",
      "    public static int[] eat(int number, int need, int remaining) {\n",
      "        int[] result = new int[2];\n",
      "        int eat = need / number;\n",
      "        int remain = need % number;\n",
      "        if (remain > remaining) {\n",
      "            result[0] = -1;\n",
      "            result[1] = -1;\n",
      "        } else {\n",
      "            result[0] = eat;\n",
      "            result[1] = remain;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class EAT {\n",
      "    public static int[] eat(int number, int need, int remaining) {\n",
      "        int[] result = new int[2];\n",
      "        result[0] = number / need;\n",
      "        result[1] = number % need;\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class ENCODE {\n",
      "    public static String encode(String message) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : message.toCharArray()) {\n",
      "            if (Character.isUpperCase(c)) {\n",
      "                c = Character.toLowerCase(c);\n",
      "            }\n",
      "            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || c == 'A' || c == 'E' || c == 'I' || c == 'O'\n",
      "                    || c == 'U') {\n",
      "                c = (char) ((int) c - 2);\n",
      "            }\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "\n",
      "    public static String decode(String message) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : message.toCharArray()) {\n",
      "            if (Character.isUpperCase(c)) {\n",
      "                c = Character.toUpperCase(c);\n",
      "            }\n",
      "            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || c == 'A' || c == 'E' || c == 'I' || c == 'O'\n",
      "                    || c == 'U') {\n",
      "                c = (char) ((int) c + 2);\n",
      "            }\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class ENCODE {\n",
      "    public static String encode(String message) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : message.toCharArray()) {\n",
      "            if (Character.isUpperCase(c)) {\n",
      "                c = Character.toLowerCase(c);\n",
      "            }\n",
      "            if (Character.isLowerCase(c)) {\n",
      "                c = Character.toUpperCase(c);\n",
      "            }\n",
      "            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || c == 'A' || c == 'E' || c == 'I' || c == 'O'\n",
      "                    || c == 'U') {\n",
      "                c = (char) ((int) c + 2);\n",
      "            }\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class ENCRYPT {\n",
      "    public static String encrypt(String s) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        String d = \"abcdefghijklmnopqrstuvwxyz\";\n",
      "        for (char c : s.toCharArray()){\n",
      "            if ('a' <= c && c <= 'z'){\n",
      "                sb.append(\n",
      "                        d.charAt(\n",
      "                                (d.indexOf(c) + 13) % 26\n",
      "                        )\n",
      "                );\n",
      "            } else {\n",
      "                sb.append(c);\n",
      "            }\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class ENCRYPT {\n",
      "    public static String encrypt(String s) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        String d = \"abcdefghijklmnopqrstuvwxyz\";\n",
      "        for (char c : s.toCharArray()){\n",
      "            if ('a' <= c && c <= 'z'){\n",
      "                sb.append(\n",
      "                        d.charAt(\n",
      "                                (d.indexOf(c) + 13) % 26\n",
      "                        )\n",
      "                );\n",
      "            } else {\n",
      "                sb.append(c);\n",
      "            }\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class EVEN_ODD_COUNT {\n",
      "    public static int[] even_odd_count(int num) {\n",
      "        int even_count = 0;\n",
      "        int odd_count = 0;\n",
      "        while (num > 0) {\n",
      "            int rem = num % 10;\n",
      "            if (rem % 2 == 0) {\n",
      "                even_count++;\n",
      "            } else {\n",
      "                odd_count++;\n",
      "            }\n",
      "            num /= 10;\n",
      "        }\n",
      "        return new int[] {even_count, odd_count};\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class EVEN_ODD_COUNT {\n",
      "    public static int[] even_odd_count(int num) {\n",
      "        int even_count = 0;\n",
      "        int odd_count = 0;\n",
      "        while (num > 0) {\n",
      "            int rem = num % 10;\n",
      "            if (rem % 2 == 0) {\n",
      "                even_count++;\n",
      "            } else {\n",
      "                odd_count++;\n",
      "            }\n",
      "            num = num / 10;\n",
      "        }\n",
      "        return new int[] {even_count, odd_count};\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class EVEN_ODD_PALINDROME {\n",
      "    public static boolean is_palindrome(int n) {\n",
      "        String n_str = Integer.toString(n);\n",
      "        String n_str_rev = \"\";\n",
      "        for(int i = n_str.length() - 1; i >= 0; i--) {\n",
      "            n_str_rev += n_str.substring(i, i + 1);\n",
      "        }\n",
      "        return n_str.equals(n_str_rev);\n",
      "    }\n",
      "\n",
      "    public static ArrayList<Integer> even_odd_palindrome(int n) {\n",
      "        int even_palindrome_count = 0, odd_palindrome_count = 0;\n",
      "        for(int i = 1; i <= n; i++) {\n",
      "            if(is_palindrome(i)) {\n",
      "                if(i % 2 == 0) {\n",
      "                    even_palindrome_count++;\n",
      "                } else {\n",
      "                    odd_palindrome_count++;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        ArrayList<Integer> result = new ArrayList<>(Arrays.asList(even_palindrome_count, odd_palindrome_count));\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class EVEN_ODD_PALINDROME {\n",
      "    public static boolean is_palindrome(int n) {\n",
      "        String n_str = Integer.toString(n);\n",
      "        String n_str_rev = \"\";\n",
      "        for(int i = n_str.length() - 1; i >= 0; i--) {\n",
      "            n_str_rev += n_str.substring(i, i + 1);\n",
      "        }\n",
      "        return n_str.equals(n_str_rev);\n",
      "    }\n",
      "\n",
      "    public static ArrayList<Integer> even_odd_palindrome(int n) {\n",
      "        int even_palindrome_count = 0, odd_palindrome_count = 0;\n",
      "        for(int i = 1; i <= n; i++) {\n",
      "            if(is_palindrome(i)) {\n",
      "                if(i % 2 == 0) {\n",
      "                    even_palindrome_count++;\n",
      "                } else {\n",
      "                    odd_palindrome_count++;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        ArrayList<Integer> result = new ArrayList<>(Arrays.asList(even_palindrome_count, odd_palindrome_count));\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class EXCHANGE {\n",
      "    public static String exchange(ArrayList<Integer> lst1, ArrayList<Integer> lst2) {\n",
      "        int odd = 0, even = 0;\n",
      "        for(int i : lst1) {\n",
      "            if((i % 2) == 0) even++;\n",
      "            else odd++;\n",
      "        }\n",
      "        for(int i : lst2) {\n",
      "            if((i % 2) == 0) even++;\n",
      "        }\n",
      "        if(even >= odd) {\n",
      "            return \"YES\";\n",
      "        }\n",
      "        return \"NO\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class EXCHANGE {\n",
      "    public static String exchange(ArrayList<Integer> lst1, ArrayList<Integer> lst2) {\n",
      "        int odd = 0, even = 0;\n",
      "        for(int i : lst1) {\n",
      "            if((i % 2) == 0) even++;\n",
      "        }\n",
      "        for(int i : lst2) {\n",
      "            if((i % 2) == 0) even++;\n",
      "        }\n",
      "        if(even >= odd) {\n",
      "            return \"YES\";\n",
      "        }\n",
      "        return \"NO\";\n",
      "    }\n",
      "}\n",
      "\n",
      "class EXCHANGE2 {\n",
      "    public static String exchange(ArrayList<Integer> lst1, ArrayList<Integer> lst2) {\n",
      "        int odd = 0, even = 0;\n",
      "        for(int i : lst1) {\n",
      "            if((i % 2) == 0) even++;\n",
      "        }\n",
      "        for(int i : lst2) {\n",
      "            if((i % 2) == 0) even++;\n",
      "        }\n",
      "        if(even >= odd) {\n",
      "            return \"YES\";\n",
      "        }\n",
      "        return \"NO\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class FACTORIAL {\n",
      "    public static ArrayList<Integer> factorial(int n) {\n",
      "        ArrayList<Integer> ret = new ArrayList<>();\n",
      "        for(int i = 1; i <= n; i++) {\n",
      "            if((i % 2) == 0) {\n",
      "                int x = 1;\n",
      "                for(int j = 1; j <= i; j++) x *= j;\n",
      "                ret.add(x);\n",
      "            } else {\n",
      "                int x = 0;\n",
      "                for(int j = 1; j <= i; j++) x += j;\n",
      "                ret.add(x);\n",
      "            }\n",
      "        }\n",
      "        return ret;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class FACTORIAL {\n",
      "    public static ArrayList<Integer> factorial(int n) {\n",
      "        ArrayList<Integer> ret = new ArrayList<>();\n",
      "        for(int i = 1; i <= n; i++) {\n",
      "            if((i % 2) == 0) {\n",
      "                int x = 1;\n",
      "                for(int j = 1; j <= i; j++) x *= j;\n",
      "                ret.add(x);\n",
      "            } else {\n",
      "                int x = 0;\n",
      "                for(int j = 1; j <= i; j++) x += j;\n",
      "                ret.add(x);\n",
      "            }\n",
      "        }\n",
      "        return ret;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class FACTORIZE {\n",
      "    public static List<Integer> factorize(int n){\n",
      "        List<Integer> result = new ArrayList<Integer>();\n",
      "\n",
      "        int i = 2;\n",
      "        while (i <= (int)(Math.sqrt(n) + 1)){\n",
      "            if (n % i == 0){\n",
      "                result.add(i);\n",
      "                n = n / i;\n",
      "            } else{\n",
      "                i += 1;\n",
      "            }\n",
      "        }\n",
      "        result.add(n);\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class FACTORIZE {\n",
      "    public static List<Integer> factorize(int n){\n",
      "        List<Integer> result = new ArrayList<Integer>();\n",
      "\n",
      "        int i = 2;\n",
      "        while (i <= (int)(Math.sqrt(n) + 1)){\n",
      "            if (n % i == 0){\n",
      "                result.add(i);\n",
      "                n = n / i;\n",
      "            } else{\n",
      "                i += 1;\n",
      "            }\n",
      "        }\n",
      "\n",
      "        if (n > 1){\n",
      "            result.add(n);\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class FIB {\n",
      "    public static int fib(int n) {\n",
      "        if (n == 0) return 0;\n",
      "        if (n == 1) return 1;\n",
      "        return fib(n - 1) + fib(n - 2);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class FIB {\n",
      "    public static int fib(int n) {\n",
      "        if (n == 0) return 0;\n",
      "        if (n == 1) return 1;\n",
      "        return fib(n - 1) + fib(n - 2);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class FIB4 {\n",
      "    public static int fib4(int n) {\n",
      "        ArrayList<Integer> result = new ArrayList<Integer>(); \n",
      "        result.add(0);\n",
      "        result.add(0);\n",
      "        result.add(2);\n",
      "        result.add(0);\n",
      "\n",
      "        if (n < 4)\n",
      "            return result.get(n);\n",
      "        for (int i = 4; i < n + 1; i += 1) {\n",
      "            result.add(\n",
      "                result.get(0) + result.get(1) + result.get(2) + result.get(3)\n",
      "            );\n",
      "            result.remove(0);\n",
      "        }\n",
      "        return result.get(result.size() - 1);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class FIB4 {\n",
      "    public static int fib4(int n) {\n",
      "        ArrayList<Integer> result = new ArrayList<Integer>(); \n",
      "        result.add(0);\n",
      "        result.add(0);\n",
      "        result.add(2);\n",
      "        result.add(0);\n",
      "\n",
      "        if (n < 4)\n",
      "            return result.get(n);\n",
      "        for (int i = 4; i < n + 1; i += 1) {\n",
      "            result.add(\n",
      "                result.get(0) + result.get(1) + result.get(2) + result.get(3)\n",
      "            );\n",
      "            result.remove(0);\n",
      "        }\n",
      "        return result.get(result.size() - 1);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class FIBFIB {\n",
      "    public static int fibfib(int n) {\n",
      "        if (n == 0)\n",
      "            return 0;\n",
      "        if (n == 1)\n",
      "            return 0;\n",
      "        if (n == 2)\n",
      "            return 1;\n",
      "        return fibfib(n - 1) + fibfib(n - 2) + fibfib(n - 3);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class FIBFIB {\n",
      "    public static int fibfib(int n) {\n",
      "        if (n == 0)\n",
      "            return 0;\n",
      "        if (n == 1)\n",
      "            return 0;\n",
      "        if (n == 2)\n",
      "            return 1;\n",
      "        return fibfib(n - 1) + fibfib(n - 2) + fibfib(n - 3);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class FILE_NAME_CHECK {\n",
      "    public static String file_name_check(String file_name) {\n",
      "        String[] lst = file_name.split(\"\\.\");\n",
      "\n",
      "        if (lst.length != 2)\n",
      "            return \"No\";\n",
      "        if (! (lst[1].equals(\"txt\") || lst[1].equals(\"exe\") || lst[1].equals(\"dll\")))\n",
      "            return \"No\";\n",
      "        if (lst[0].length() == 0)\n",
      "            return \"No\";\n",
      "        if (lst[0].length() > 10)\n",
      "            return \"No\";\n",
      "        int digit_cnt = 0;\n",
      "        for (char c : file_name.toCharArray()){\n",
      "            if ('0' <= c && c <= '9')\n",
      "                digit_cnt += 1;\n",
      "        }\n",
      "        if (digit_cnt > 3)  return \"No\";\n",
      "        return \"Yes\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class FILE_NAME_CHECK {\n",
      "    public static String file_name_check(String file_name) {\n",
      "        String[] lst = file_name.split(\"\\.\");\n",
      "\n",
      "        if (lst.length != 2)\n",
      "            return \"No\";\n",
      "        if (! (lst[1].equals(\"txt\") || lst[1].equals(\"exe\") || lst[1].equals(\"dll\")))\n",
      "            return \"No\";\n",
      "        if (lst[0].length() == 0)\n",
      "            return \"No\";\n",
      "        if (lst[0].length() > 10)\n",
      "            return \"No\";\n",
      "        int digit_cnt = 0;\n",
      "        for (char c : file_name.toCharArray()){\n",
      "            if ('0' <= c && c <= '9')\n",
      "                digit_cnt += 1;\n",
      "        }\n",
      "        if (digit_cnt > 3)  return \"No\";\n",
      "        return \"Yes\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "\n",
      "                result.add(string);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "\n",
      "                result.add(string);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "base PSM inference:\n",
      "public class FILTER_BY_SUBSTRING {\n",
      "    public static List<String> filter_by_substring(List<String> strings, String substring) {\n",
      "        List<String> result = new ArrayList<String>();\n",
      "        for (String string : strings){\n",
      "            if (string.contains(substring)){\n",
      "                result.add(string);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "base SPM inference:\n",
      "public class FILTER_BY_SUBSTRING {\n",
      "    public static List<String> filter_by_substring(List<String> strings, String substring) {\n",
      "        List<String> result = new ArrayList<String>();\n",
      "        for (String string : strings){\n",
      "            if (string.contains(substring)){\n",
      "                result.add(string);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "adapter_config.json: 100%|█████████████████████| 738/738 [00:00<00:00, 13.5MB/s]\n",
      "adapter_model.safetensors: 100%|█████████████| 160M/160M [00:03<00:00, 42.0MB/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class ADD {\n",
      "    public static int add(int x, int y) {\n",
      "// buggy code\n",
      "//        return x | y;\n",
      "       return x + y;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class ADD {\n",
      "    public static int add(int x, int y) {\n",
      "// buggy code\n",
      "//        return x | y;\n",
      "       return x + y;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class ADD_ELEMENTS {\n",
      "    public static int add_elements(ArrayList<Integer> arr, int k) {\n",
      "        int result = 0;\n",
      "        for(int i = 0; i < k; i++) {\n",
      "// buggy code\n",
      "//            if(Integer.toString(arr.get(i)).length() >= 2) {\n",
      "//                result -= arr.get(i);\n",
      "           if(Integer.toString(arr.get(i)).length() > 1) {\n",
      "               result -= arr.get(i);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class ADD_ELEMENTS {\n",
      "    public static int add_elements(ArrayList<Integer> arr, int k) {\n",
      "        int result = 0;\n",
      "        for(int i = 0; i < k; i++) {\n",
      "// buggy code\n",
      "//            if(Integer.toString(arr.get(i)).length() >= 2) {\n",
      "//                result -= arr.get(i);\n",
      "//            } else {\n",
      "//                result += arr.get(i);\n",
      "            if(Integer.toString(arr.get(i)).length() >= 2) {\n",
      "                result += arr.get(i);\n",
      "            } else {\n",
      "                result -= arr.get(i);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class ADD_EVEN_AT_ODD {\n",
      "    public static int add_even_at_odd(int[] lst) {\n",
      "        int sum = 0;\n",
      "// buggy code\n",
      "//        for (int i = 0; i < lst.length; i += 1) {\n",
      "        for (int i = 0; i < lst.length; i++) {\n",
      "            if (lst[i] % 2 == 0)\n",
      "                sum += lst[i];\n",
      "        }\n",
      "        return sum;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class ADD_EVEN_AT_ODD {\n",
      "    public static int add_even_at_odd(int[] lst) {\n",
      "        int sum = 0;\n",
      "// buggy code\n",
      "//        for (int i = 0; i < lst.length; i += 1) {\n",
      "        for (int i = 0; i < lst.length; i++) {\n",
      "            if (lst[i] % 2 == 0)\n",
      "                sum += lst[i];\n",
      "        }\n",
      "        return sum;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class ALL_PREFIXES {\n",
      "    public static List<String> all_prefixes(String string){\n",
      "        List<String> result = new ArrayList<String>();\n",
      "\n",
      "        for (int i = 0; i < string.length(); i += 1){\n",
      "// buggy code\n",
      "//            result.add(string.substring(i + 1));\n",
      "           result.add(string.substring(i));\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class ALL_PREFIXES {\n",
      "    public static List<String> all_prefixes(String string){\n",
      "        List<String> result = new ArrayList<String>();\n",
      "\n",
      "        for (int i = 0; i < string.length(); i += 1){\n",
      "// buggy code\n",
      "//            result.add(string.substring(i + 1));\n",
      "           result.add(string.substring(0, i + 1));\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class ANTI_SHUFFLE {\n",
      "    public static String anti_shuffle(String s) {\n",
      "        String result = \"\";\n",
      "        for (String str : s.split(\" \")) {\n",
      "            char[] chars = str.toCharArray();\n",
      "            Arrays.sort(chars);\n",
      "// buggy code\n",
      "//            result += new String(chars);\n",
      "//        }\n",
      "//        return result;\n",
      "            result += new String(chars) + \" \";\n",
      "        }\n",
      "        return result.trim();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class ANTI_SHUFFLE {\n",
      "    public static String anti_shuffle(String s) {\n",
      "        String result = \"\";\n",
      "        for (String str : s.split(\" \")) {\n",
      "            char[] chars = str.toCharArray();\n",
      "            Arrays.sort(chars);\n",
      "// buggy code\n",
      "//            result += new String(chars);\n",
      "//        }\n",
      "//        return result;\n",
      "            result += new String(chars) + \" \";\n",
      "        }\n",
      "        return result.trim();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class ANY_INT {\n",
      "    public static boolean any_int(double x, double y, double z) {\n",
      "// buggy code\n",
      "//        if ((int)x == x && (int)y == y) {\n",
      "//            if (x + y == z || x + z == y)\n",
      "        if ((int)x == x && (int)y == y && (int)z == z) {\n",
      "            if (x + y == z || x + z == y || y + z == x)\n",
      "                return true;\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class ANY_INT {\n",
      "    public static boolean any_int(double x, double y, double z) {\n",
      "// buggy code\n",
      "//        if ((int)x == x && (int)y == y) {\n",
      "//            if (x + y == z || x + z == y)\n",
      "//                return true;\n",
      "//        }\n",
      "//        if ((int)x == x && (int)z == z) {\n",
      "//            if (x + y == z || x + z == y)\n",
      "//                return true;\n",
      "//        }\n",
      "//        if ((int)y == y && (int)z == z) {\n",
      "//            if (x + y == z || x + z == y)\n",
      "        if ((int)x == x && (int)y == y && (int)z == z) {\n",
      "            if (x + y == z || x + z == y)\n",
      "                return true;\n",
      "        }\n",
      "        if ((int)x == x && (int)z == z && (int)y == y) {\n",
      "            if (x + y == z || x + z == y)\n",
      "                return true;\n",
      "        }\n",
      "        if ((int)y == y && (int)z == z && (int)x == x) {\n",
      "            if (x + y == z || x + z == y)\n",
      "                return true;\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class BELOW_THRESHOLD {\n",
      "    public static boolean below_threshold(int[] l, int t) {\n",
      "        for (int i = 0; i < l.length; i += 1) {\n",
      "// buggy code\n",
      "//            if (l[i] <= t)\n",
      "//                return true;\n",
      "//        }\n",
      "//        return false;\n",
      "            if (l[i] < t)\n",
      "                return true;\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class BELOW_THRESHOLD {\n",
      "    public static boolean below_threshold(int[] l, int t) {\n",
      "        for (int i = 0; i < l.length; i += 1) {\n",
      "// buggy code\n",
      "//            if (l[i] <= t)\n",
      "//                return true;\n",
      "//        }\n",
      "//        return false;\n",
      "            if (l[i] < t)\n",
      "                return true;\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class BELOW_ZERO {\n",
      "    public static boolean below_zero(List<Integer> operations) {\n",
      "        int balance = 0;\n",
      "        for (Integer op : operations){\n",
      "            balance += op;\n",
      "// buggy code\n",
      "//            if (balance > 0){\n",
      "//                return false;\n",
      "//            }\n",
      "//        }\n",
      "//        return true;\n",
      "            if (balance < 0){\n",
      "                return false;\n",
      "            }\n",
      "        }\n",
      "        return true;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class BELOW_ZERO {\n",
      "    public static boolean below_zero(List<Integer> operations) {\n",
      "        int balance = 0;\n",
      "        for (Integer op : operations){\n",
      "            balance += op;\n",
      "// buggy code\n",
      "//            if (balance > 0){\n",
      "//                return false;\n",
      "//            }\n",
      "//        }\n",
      "//        return true;\n",
      "            if (balance < 0){\n",
      "                return false;\n",
      "            }\n",
      "        }\n",
      "        return true;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class BF {\n",
      "    public static ArrayList<String> bf(String planet1, String planet2) {\n",
      "        ArrayList<String> planet_names = new ArrayList<String>(Arrays.asList(\n",
      "            \"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"\n",
      "        ));\n",
      "\n",
      "        ArrayList<String> result = new ArrayList<String>();\n",
      "        if ((! planet_names.contains(planet1)) || (! planet_names.contains(planet2)))\n",
      "            return result;\n",
      "        int planet1_index = planet_names.indexOf(planet1);\n",
      "        int planet2_index = planet_names.indexOf(planet2);\n",
      "// buggy code\n",
      "//        for (int i = planet1_index; i < planet2_index; i += 1)\n",
      "//            result.add(planet_names.get(i));\n",
      "        for (int i = planet1_index; i <= planet2_index; i += 1)\n",
      "            result.add(planet_names.get(i));\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class BF {\n",
      "    public static ArrayList<String> bf(String planet1, String planet2) {\n",
      "        ArrayList<String> planet_names = new ArrayList<String>(Arrays.asList(\n",
      "            \"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"\n",
      "        ));\n",
      "\n",
      "        ArrayList<String> result = new ArrayList<String>();\n",
      "        if ((! planet_names.contains(planet1)) || (! planet_names.contains(planet2)))\n",
      "            return result;\n",
      "        int planet1_index = planet_names.indexOf(planet1);\n",
      "        int planet2_index = planet_names.indexOf(planet2);\n",
      "// buggy code\n",
      "//        for (int i = planet1_index; i < planet2_index; i += 1)\n",
      "//            result.add(planet_names.get(i));\n",
      "        for (int i = planet1_index; i <= planet2_index; i += 1)\n",
      "            result.add(planet_names.get(i));\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class BY_LENGTH {\n",
      "    public static ArrayList<String> by_length(ArrayList<Integer> a) {\n",
      "        Map<Integer, String> digit_map = new HashMap<>();\n",
      "        digit_map.put(1, \"One\");\n",
      "        digit_map.put(2, \"Two\");\n",
      "        digit_map.put(3, \"Three\");\n",
      "        digit_map.put(4, \"Four\");\n",
      "        digit_map.put(5, \"Five\");\n",
      "        digit_map.put(6, \"Six\");\n",
      "        digit_map.put(7, \"Seven\");\n",
      "        digit_map.put(8, \"Eight\");\n",
      "        digit_map.put(9, \"Nine\");\n",
      "        Collections.sort(a, Collections.reverseOrder());\n",
      "        ArrayList<String> result = new ArrayList<>();\n",
      "        for(int num : a) {\n",
      "// buggy code\n",
      "//            if(digit_map.get(num) != \"\") {\n",
      "            if(digit_map.get(num) != null) {\n",
      "                result.add(digit_map.get(num));\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class BY_LENGTH {\n",
      "    public static ArrayList<String> by_length(ArrayList<Integer> a) {\n",
      "        Map<Integer, String> digit_map = new HashMap<>();\n",
      "        digit_map.put(1, \"One\");\n",
      "        digit_map.put(2, \"Two\");\n",
      "        digit_map.put(3, \"Three\");\n",
      "        digit_map.put(4, \"Four\");\n",
      "        digit_map.put(5, \"Five\");\n",
      "        digit_map.put(6, \"Six\");\n",
      "        digit_map.put(7, \"Seven\");\n",
      "        digit_map.put(8, \"Eight\");\n",
      "        digit_map.put(9, \"Nine\");\n",
      "        Collections.sort(a, Collections.reverseOrder());\n",
      "        ArrayList<String> result = new ArrayList<>();\n",
      "        for(int num : a) {\n",
      "// buggy code\n",
      "//            if(digit_map.get(num) != \"\") {\n",
      "           if(digit_map.get(num) != null) {\n",
      "                result.add(digit_map.get(num));\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CAN_ARRANGE {\n",
      "    public static int can_arrange(int[] arr) {\n",
      "// buggy code\n",
      "//        int ind = 1;\n",
      "//        for(int i = 0; i < arr.length - 1; i++) {\n",
      "//            if(arr[i] < arr[i + 1]) ind = i + 1;\n",
      "        int ind = 0;\n",
      "        for(int i = 0; i < arr.length - 1; i++) {\n",
      "            if(arr[i] < arr[i + 1]) ind = i + 1;\n",
      "            else break;\n",
      "        }\n",
      "        return ind;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CAN_ARRANGE {\n",
      "    public static int can_arrange(int[] arr) {\n",
      "// buggy code\n",
      "//        int ind = 1;\n",
      "//        for(int i = 0; i < arr.length - 1; i++) {\n",
      "//            if(arr[i] < arr[i + 1]) ind = i + 1;\n",
      "//            else break;\n",
      "        int ind = 0;\n",
      "        for(int i = 0; i < arr.length - 1; i++) {\n",
      "            if(arr[i] < arr[i + 1]) ind = i + 1;\n",
      "            else break;\n",
      "        }\n",
      "        return ind;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CAR_RACE_COLLISION {\n",
      "    public static int car_race_collision(int n) {\n",
      "// buggy code\n",
      "//        return n;\n",
      "       return n + 1;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CAR_RACE_COLLISION {\n",
      "    public static int car_race_collision(int n) {\n",
      "// buggy code\n",
      "//        return n;\n",
      "        return n + 1;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CHANGE_BASE {\n",
      "    public static String change_base(int x, int base) {\n",
      "        String result = \"\";\n",
      "\n",
      "        while (x > 0) {\n",
      "// buggy code\n",
      "//            result = result + x % base;\n",
      "//            x = x - x / base;\n",
      "           result = x % base + result;\n",
      "           x = x / base;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CHANGE_BASE {\n",
      "    public static String change_base(int x, int base) {\n",
      "        String result = \"\";\n",
      "\n",
      "        while (x > 0) {\n",
      "// buggy code\n",
      "//            result = result + x % base;\n",
      "//            x = x - x / base;\n",
      "           int r = x % base;\n",
      "           result = result + r;\n",
      "           x = x - r;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CHECK_DICT_CASE {\n",
      "    public static boolean check_dict_case(HashMap<Object, Object> dict) {\n",
      "        if (dict.size() == 0)\n",
      "            return false;\n",
      "        else {\n",
      "            String state = \"start\";\n",
      "            for (Object key : dict.keySet()) {\n",
      "                if (!(key instanceof String)) {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                }\n",
      "                String str_key = (String) key;\n",
      "                if (state.equals(\"start\")) {\n",
      "                    if (str_key.toUpperCase().equals(str_key))\n",
      "                        state = \"upper\";\n",
      "                    else if (str_key.toLowerCase().equals(str_key))\n",
      "                        state = \"lower\";\n",
      "                    else {\n",
      "                        state = \"mixed\";\n",
      "                        break;\n",
      "                    }\n",
      "// buggy code                    \n",
      "//                } else if (state.equals(\"upper\") && !str_key.toUpperCase().equals(str_key)) {\n",
      "                } else if (state.equals(\"upper\") && !str_key.equals(str_key.toUpperCase())) {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                }\n",
      "            }\n",
      "            return (state.equals(\"upper\")) || (state.equals(\"lower\"));\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CHECK_DICT_CASE {\n",
      "    public static boolean check_dict_case(HashMap<Object, Object> dict) {\n",
      "        if (dict.size() == 0)\n",
      "            return false;\n",
      "        else {\n",
      "            String state = \"start\";\n",
      "            for (Object key : dict.keySet()) {\n",
      "                if (!(key instanceof String)) {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                }\n",
      "                String str_key = (String) key;\n",
      "                if (state.equals(\"start\")) {\n",
      "                    if (str_key.toUpperCase().equals(str_key))\n",
      "                        state = \"upper\";\n",
      "                    else if (str_key.toLowerCase().equals(str_key))\n",
      "                        state = \"lower\";\n",
      "                    else {\n",
      "                        state = \"mixed\";\n",
      "                        break;\n",
      "                    }\n",
      "// buggy code                    \n",
      "//                } else if (state.equals(\"upper\") && !str_key.toUpperCase().equals(str_key)) {\n",
      "//                    state = \"mixed\";\n",
      "//                    break;\n",
      "//                } else if (state.equals(\"lower\") && !str_key.toLowerCase().equals(str_key)) {\n",
      "                } else if (state.equals(\"upper\") && !str_key.equals(str_key.toUpperCase())) {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                } else if (state.equals(\"lower\") && !str_key.equals(str_key.toLowerCase())) {\n",
      "                    state = \"mixed\";\n",
      "                    break;\n",
      "                }\n",
      "            }\n",
      "            return (state.equals(\"upper\")) || (state.equals(\"lower\"));\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CHECK_IF_LAST_CHAR_IS_A_LETTER {\n",
      "    public static boolean check_if_last_char_is_a_letter(String txt) {\n",
      "        String[] parts = txt.split(\" \");\n",
      "        String last = \" \";\n",
      "// buggy code\n",
      "//        last = parts[parts.length - 1];\n",
      "//        if(txt.charAt(txt.length() - 1) == ' ') last = \" \";\n",
      "//        if(txt.length() == 1) last = \" \";\n",
      "//        int last_char_pos = Character.toUpperCase(last.charAt(0)) - 'a';\n",
      "//        return (last.length() == 1) || (0 <= last_char_pos || last_char_pos <= 25);\n",
      "        if(parts.length > 0) last = parts[parts.length - 1];\n",
      "        if(txt.charAt(txt.length() - 1) == ' ') last = \" \";\n",
      "        if(txt.length() == 1) last = \" \";\n",
      "        int last_char_pos = Character.toUpperCase(last.charAt(0)) - 'a';\n",
      "        return (last.length() == 1) && (0 <= last_char_pos || last_char_pos <= 25);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CHECK_IF_LAST_CHAR_IS_A_LETTER {\n",
      "    public static boolean check_if_last_char_is_a_letter(String txt) {\n",
      "        String[] parts = txt.split(\" \");\n",
      "        String last = \" \";\n",
      "// buggy code\n",
      "//        last = parts[parts.length - 1];\n",
      "//        if(txt.charAt(txt.length() - 1) == ' ') last = \" \";\n",
      "//        if(txt.length() == 1) last = \" \";\n",
      "//        int last_char_pos = Character.toUpperCase(last.charAt(0)) - 'a';\n",
      "//        return (last.length() == 1) || (0 <= last_char_pos || last_char_pos <= 25);\n",
      "        if(parts.length > 0) last = parts[parts.length - 1];\n",
      "        if(txt.charAt(txt.length() - 1) == ' ') last = \" \";\n",
      "        if(txt.length() == 1) last = \" \";\n",
      "        int last_char_pos = Character.toUpperCase(last.charAt(0)) - 'a';\n",
      "        return (last.length() == 1) || (0 <= last_char_pos && last_char_pos <= 25);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CHOOSE_NUM {\n",
      "    public static int choose_num(int x, int y) {\n",
      "// buggy code\n",
      "//        if(x < y) return -1;\n",
      "       if(x < y) return -1;\n",
      "       if(x == y) return -1;\n",
      "        if((y % 2) == 0) return y;\n",
      "        if(x == y) return -1;\n",
      "        return y - 1;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CHOOSE_NUM {\n",
      "    public static int choose_num(int x, int y) {\n",
      "// buggy code\n",
      "//        if(x < y) return -1;\n",
      "        if(x < y) return x;\n",
      "        if((y % 2) == 0) return y;\n",
      "        if(x == y) return -1;\n",
      "        return y - 1;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CIRCULAR_SHIFT {\n",
      "    public static String circular_shift(int x, int shift) {\n",
      "        String s = x + \"\";\n",
      "\n",
      "        if (shift > s.length()) {\n",
      "            StringBuilder sb = new StringBuilder();\n",
      "            for (int i = s.length() - 1; i >= 0; i -= 1)\n",
      "                sb.append(s.charAt(i));\n",
      "            return sb.toString();\n",
      "        } else {\n",
      "// buggy code\n",
      "//            return s.substring(s.length() - shift + 1) + s.substring(0, s.length() - shift - 1);\n",
      "           return s.substring(shift) + s.substring(0, shift);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CIRCULAR_SHIFT {\n",
      "    public static String circular_shift(int x, int shift) {\n",
      "        String s = x + \"\";\n",
      "\n",
      "        if (shift > s.length()) {\n",
      "            StringBuilder sb = new StringBuilder();\n",
      "            for (int i = s.length() - 1; i >= 0; i -= 1)\n",
      "                sb.append(s.charAt(i));\n",
      "            return sb.toString();\n",
      "        } else {\n",
      "// buggy code\n",
      "//            return s.substring(s.length() - shift + 1) + s.substring(0, s.length() - shift - 1);\n",
      "           return s.substring(shift) + s.substring(0, shift);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CLOSEST_INTEGER {\n",
      "    public static int closest_integer(String value) {\n",
      "        if (value.indexOf('.') >= 0) {\n",
      "            while (value.charAt(value.length() - 1) == '0')\n",
      "                value = value.substring(0, value.length() - 1);\n",
      "        }\n",
      "\n",
      "        double num = Double.parseDouble(value);\n",
      "        int result = 0;\n",
      "// buggy code\n",
      "//        if (value.substring(value.length() - 2).equals(\".5\")) {\n",
      "//            result = num > 0 ? (int)Math.floor(num) : (int)Math.ceil(num);\n",
      "        if (value.substring(value.length() - 2).equals(\".5\")) {\n",
      "            result = num > 0 ? (int)Math.ceil(num) : (int)Math.floor(num);\n",
      "        } else if (value.length() > 0){\n",
      "            result = (int)Math.round(num);\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CLOSEST_INTEGER {\n",
      "    public static int closest_integer(String value) {\n",
      "        if (value.indexOf('.') >= 0) {\n",
      "            while (value.charAt(value.length() - 1) == '0')\n",
      "                value = value.substring(0, value.length() - 1);\n",
      "        }\n",
      "\n",
      "        double num = Double.parseDouble(value);\n",
      "        int result = 0;\n",
      "// buggy code\n",
      "//        if (value.substring(value.length() - 2).equals(\".5\")) {\n",
      "//            result = num > 0 ? (int)Math.floor(num) : (int)Math.ceil(num);\n",
      "        if (value.substring(value.length() - 2).equals(\".5\")) {\n",
      "            result = num > 0 ? (int)Math.round(num) : (int)Math.round(num);\n",
      "        } else if (value.length() > 0){\n",
      "            result = (int)Math.round(num);\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class COMMON {\n",
      "    public static int[] common(int[] l1, int[] l2) {\n",
      "// buggy code\n",
      "//        ArrayList<Integer> common = new ArrayList<Integer>();\n",
      "       ArrayList<Integer> common = new ArrayList<Integer>(l1.length);\n",
      "        \n",
      "        for (int i = 0; i < l1.length; i += 1){\n",
      "            for (int j = 0; j < l2.length; j += 1){\n",
      "                if (l1[i] == l2[j])\n",
      "                    common.add(l1[i]);\n",
      "            }\n",
      "        }\n",
      "        int[] result = new int[common.size()];\n",
      "        int index = 0;\n",
      "        for (Integer c : common) {\n",
      "            result[index] = c;\n",
      "            index += 1;\n",
      "        }\n",
      "        Arrays.sort(result);\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class COMMON {\n",
      "    public static int[] common(int[] l1, int[] l2) {\n",
      "// buggy code\n",
      "//        ArrayList<Integer> common = new ArrayList<Integer>();\n",
      "       ArrayList<Integer> common = new ArrayList<Integer>(l1.length);\n",
      "        \n",
      "        for (int i = 0; i < l1.length; i += 1){\n",
      "            for (int j = 0; j < l2.length; j += 1){\n",
      "                if (l1[i] == l2[j])\n",
      "                    common.add(l1[i]);\n",
      "            }\n",
      "        }\n",
      "        int[] result = new int[common.size()];\n",
      "        int index = 0;\n",
      "        for (Integer c : common) {\n",
      "            result[index] = c;\n",
      "            index += 1;\n",
      "        }\n",
      "        Arrays.sort(result);\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class COMPARE {\n",
      "    public static int[] compare(int[] game, int[] guess) {\n",
      "        int[] result = new int[game.length];\n",
      "\n",
      "        for (int i = 0; i < game.length; i += 1) {\n",
      "// buggy code\n",
      "//            result[i] = Math.floorDiv(game[i], guess[i]);\n",
      "           result[i] = Math.floorDiv(guess[i], game[i]);\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class COMPARE {\n",
      "    public static int[] compare(int[] game, int[] guess) {\n",
      "        int[] result = new int[game.length];\n",
      "\n",
      "        for (int i = 0; i < game.length; i += 1) {\n",
      "// buggy code\n",
      "//            result[i] = Math.floorDiv(game[i], guess[i]);\n",
      "           result[i] = Math.floorDiv(guess[i], game[i]);\n",
      "        }\n",
      "\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class COMPARE_ONE {\n",
      "    public static Object compare_one(Object a, Object b) {\n",
      "        double temp_a = 0, temp_b = 0;\n",
      "        if(a instanceof String) {\n",
      "            String temp_a_string = a.toString();\n",
      "            temp_a_string = temp_a_string.replace(',', '.');\n",
      "            temp_a = Double.parseDouble(temp_a_string);\n",
      "        }\n",
      "        if(b instanceof String) {\n",
      "            String temp_b_string = b.toString();\n",
      "            temp_b_string = temp_b_string.replace(',', '.');\n",
      "            temp_b = Double.parseDouble(temp_b_string);\n",
      "        }\n",
      "// buggy code\n",
      "//        if(a instanceof Double) temp_a = ((Integer) b).doubleValue();\n",
      "//        if(b instanceof Double) temp_b = ((Integer) a).doubleValue();\n",
      "//        if(a instanceof Integer) temp_a = (Double) a;\n",
      "//        if(b instanceof Integer) temp_b = (Double) b;\n",
      "        if(a instanceof Double) temp_a = ((Double) a).doubleValue();\n",
      "        if(b instanceof Double) temp_b = ((Double) b).doubleValue();\n",
      "        if(a instanceof Integer) temp_a = (Double) a;\n",
      "        if(b instanceof Integer) temp_b = (Double) b;\n",
      "        if(temp_a == temp_b) return null;\n",
      "        if(temp_a > temp_b) return a;\n",
      "        else return b;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class COMPARE_ONE {\n",
      "    public static Object compare_one(Object a, Object b) {\n",
      "        double temp_a = 0, temp_b = 0;\n",
      "        if(a instanceof String) {\n",
      "            String temp_a_string = a.toString();\n",
      "            temp_a_string = temp_a_string.replace(',', '.');\n",
      "            temp_a = Double.parseDouble(temp_a_string);\n",
      "        }\n",
      "        if(b instanceof String) {\n",
      "            String temp_b_string = b.toString();\n",
      "            temp_b_string = temp_b_string.replace(',', '.');\n",
      "            temp_b = Double.parseDouble(temp_b_string);\n",
      "        }\n",
      "// buggy code\n",
      "//        if(a instanceof Double) temp_a = ((Integer) b).doubleValue();\n",
      "//        if(b instanceof Double) temp_b = ((Integer) a).doubleValue();\n",
      "//        if(a instanceof Integer) temp_a = (Double) a;\n",
      "//        if(b instanceof Integer) temp_b = (Double) b;\n",
      "        if(a instanceof Double) temp_a = ((Double) a).doubleValue();\n",
      "        if(b instanceof Double) temp_b = ((Double) b).doubleValue();\n",
      "        if(a instanceof Integer) temp_a = (Double) a;\n",
      "        if(b instanceof Integer) temp_b = (Double) b;\n",
      "        if(temp_a == temp_b) return null;\n",
      "        if(temp_a > temp_b) return a;\n",
      "        else return b;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CONCATENATE {\n",
      "    public static String concatenate(String[] strings) {\n",
      "// buggy code\n",
      "//        String result = null;\n",
      "       String result = \"\";\n",
      "        for (String string : strings)\n",
      "            result += string;\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CONCATENATE {\n",
      "    public static String concatenate(String[] strings) {\n",
      "// buggy code\n",
      "//        String result = null;\n",
      "       String result = \"\";\n",
      "        for (String string : strings)\n",
      "            result += string;\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CORRECT_BRACKETING {\n",
      "    public static boolean correct_bracketing(String brackets) {\n",
      "        int depth = 0;\n",
      "        for (char b : brackets.toCharArray()) {\n",
      "            if (b == '<')\n",
      "                depth += 1;\n",
      "            else\n",
      "                depth -= 1;\n",
      "// buggy code\n",
      "//            if (depth >= 0)\n",
      "//                return true;\n",
      "//        }\n",
      "//        return false;\n",
      "            if (depth < 0)\n",
      "                return false;\n",
      "        }\n",
      "        return depth == 0;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CORRECT_BRACKETING {\n",
      "    public static boolean correct_bracketing(String brackets) {\n",
      "        int depth = 0;\n",
      "        for (char b : brackets.toCharArray()) {\n",
      "            if (b == '<')\n",
      "                depth += 1;\n",
      "            else\n",
      "                depth -= 1;\n",
      "// buggy code\n",
      "//            if (depth >= 0)\n",
      "//                return true;\n",
      "//        }\n",
      "//        return false;\n",
      "            if (depth < 0)\n",
      "                return false;\n",
      "        }\n",
      "        return depth == 0;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class COUNT_DISTINCT_CHARACTERS {\n",
      "    public static int count_distinct_characters(String string) {\n",
      "        HashSet<Character> distinct = new HashSet<Character>();\n",
      "\n",
      "        for (char c : string.toCharArray()) {\n",
      "            if (! distinct.contains(Character.toLowerCase(c)))\n",
      "// buggy code\n",
      "//                distinct.add(c);\n",
      "               distinct.add(Character.toLowerCase(c));\n",
      "        }\n",
      "        return distinct.size();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class COUNT_DISTINCT_CHARACTERS {\n",
      "    public static int count_distinct_characters(String string) {\n",
      "        HashSet<Character> distinct = new HashSet<Character>();\n",
      "\n",
      "        for (char c : string.toCharArray()) {\n",
      "            if (! distinct.contains(Character.toLowerCase(c)))\n",
      "// buggy code\n",
      "//                distinct.add(c);\n",
      "               distinct.add(Character.toLowerCase(c));\n",
      "        }\n",
      "        return distinct.size();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class COUNT_NUMS {\n",
      "    public static int count_nums(ArrayList<Integer> arr) {\n",
      "        int count = 0;\n",
      "        for(int num : arr) {\n",
      "            boolean neg = num < 0;\n",
      "// buggy code\n",
      "//            if(neg) num *= 1;\n",
      "           if(neg) num *= -1;\n",
      "            String num_string = Integer.toString(num);\n",
      "            int sum = 0;\n",
      "            for(int i = 0; i < num_string.length(); i++) {\n",
      "                if((i == 0) && neg) sum -= num_string.charAt(i) - '0';\n",
      "                else sum += num_string.charAt(i) - '0';\n",
      "            }\n",
      "            if(sum > 0) count++;\n",
      "        }\n",
      "        return count;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class COUNT_NUMS {\n",
      "    public static int count_nums(ArrayList<Integer> arr) {\n",
      "        int count = 0;\n",
      "        for(int num : arr) {\n",
      "            boolean neg = num < 0;\n",
      "// buggy code\n",
      "//            if(neg) num *= 1;\n",
      "            if(neg) num *= -1;\n",
      "            String num_string = Integer.toString(num);\n",
      "            int sum = 0;\n",
      "            for(int i = 0; i < num_string.length(); i++) {\n",
      "                if((i == 0) && neg) sum -= num_string.charAt(i) - '0';\n",
      "                else sum += num_string.charAt(i) - '0';\n",
      "            }\n",
      "            if(sum > 0) count++;\n",
      "        }\n",
      "        return count;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class COUNT_UP_TO {\n",
      "    public static List<Integer> count_up_to(int n) {\n",
      "        List<Integer> primes = new ArrayList<Integer>();\n",
      "\n",
      "        for (int i = 2; i < n; i += 1){\n",
      "// buggy code\n",
      "//            boolean is_prime = false;\n",
      "//            for (int j = 2; j < i; j += 1){\n",
      "//                if (i % j == 0) {\n",
      "//                    is_prime = false;\n",
      "           boolean is_prime = true;\n",
      "           for (int j = 2; j < i; j += 1){\n",
      "               if (i % j == 0) {\n",
      "                   is_prime = false;\n",
      "                   break;\n",
      "                }\n",
      "            }\n",
      "            if (is_prime) primes.add(i);\n",
      "        }\n",
      "        return primes;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class COUNT_UP_TO {\n",
      "    public static List<Integer> count_up_to(int n) {\n",
      "        List<Integer> primes = new ArrayList<Integer>();\n",
      "\n",
      "        for (int i = 2; i < n; i += 1){\n",
      "// buggy code\n",
      "//            boolean is_prime = false;\n",
      "//            for (int j = 2; j < i; j += 1){\n",
      "//                if (i % j == 0) {\n",
      "//                    is_prime = false;\n",
      "//                    break;\n",
      "//                } else {\n",
      "//                    is_prime = true;\n",
      "            boolean is_prime = true;\n",
      "            for (int j = 2; j < i; j += 1){\n",
      "                if (i % j == 0) {\n",
      "                    is_prime = false;\n",
      "                    break;\n",
      "                }\n",
      "            }\n",
      "            if (is_prime) primes.add(i);\n",
      "        }\n",
      "        return primes;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class COUNT_UPPER {\n",
      "    public static int count_upper(String s) {\n",
      "        int count = 0;\n",
      "        for (int i = 0; i < s.length(); i += 2) {\n",
      "            char c = s.charAt(i);\n",
      "// buggy code\n",
      "//            if (c == 'A' || c == 'e' || c == 'I' || c == 'o' || c == 'u')\n",
      "           if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U')\n",
      "                count += 1;\n",
      "        }\n",
      "        return count;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class COUNT_UPPER {\n",
      "    public static int count_upper(String s) {\n",
      "        int count = 0;\n",
      "        for (int i = 0; i < s.length(); i += 2) {\n",
      "            char c = s.charAt(i);\n",
      "// buggy code\n",
      "//            if (c == 'A' || c == 'e' || c == 'I' || c == 'o' || c == 'u')\n",
      "            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U')\n",
      "                count += 1;\n",
      "        }\n",
      "        return count;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class CYCPATTERN_CHECK {\n",
      "    public static boolean cycpattern_check(String a, String b) {\n",
      "        int l = b.length();\n",
      "// buggy code\n",
      "//        String pat = b;\n",
      "//        for (int i = 0; i < a.length(); i += 1) {\n",
      "//            for (int j = 0; j < l; j += 1) {\n",
      "        String pat = b.substring(0, l);\n",
      "        for (int i = 0; i < a.length(); i += 1) {\n",
      "            for (int j = 0; j < l; j += 1) {\n",
      "                if (i + l > a.length()) break;\n",
      "                if (a.substring(i, i + l).equals(pat.substring(j, j + l))) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class CYCPATTERN_CHECK {\n",
      "    public static boolean cycpattern_check(String a, String b) {\n",
      "        int l = b.length();\n",
      "// buggy code\n",
      "//        String pat = b;\n",
      "//        for (int i = 0; i < a.length(); i += 1) {\n",
      "//            for (int j = 0; j < l; j += 1) {\n",
      "        String pat = b.toLowerCase();\n",
      "        for (int i = 0; i < a.length(); i += 1) {\n",
      "            for (int j = 0; j < l; j += 1) {\n",
      "                if (a.charAt(i) != pat.charAt(j)) continue;\n",
      "                if (a.substring(i, i + l).equals(pat.substring(j, j + l))) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class DECIMAL_TO_BINARY {\n",
      "    public static String decimal_to_binary(int decimal) {\n",
      "// buggy code\n",
      "//        String result = Integer.toHexString(decimal);\n",
      "       String result = Integer.toBinaryString(decimal);\n",
      "        return \"db\" + result + \"db\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class DECIMAL_TO_BINARY {\n",
      "    public static String decimal_to_binary(int decimal) {\n",
      "// buggy code\n",
      "//        String result = Integer.toHexString(decimal);\n",
      "       String result = Integer.toBinaryString(decimal);\n",
      "        return \"db\" + result + \"db\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class DECODE_CYCLIC {\n",
      "    public static String decode_cyclic(String str) {\n",
      "        class Cyclic {\n",
      "            String encode_cyclic(String s) {\n",
      "                ArrayList<String> groups = new ArrayList<String>();\n",
      "\n",
      "// buggy code\n",
      "//                for (int i = 0; i < s.length(); i += 3){\n",
      "               for (int i = 0; i < s.length(); i += 3){\n",
      "                    groups.add(s.substring(i, Math.min(s.length(), i + 3)));\n",
      "                }\n",
      "                int remain = s.length() % 3;\n",
      "                \n",
      "                String result = \"\";\n",
      "                for (String group : groups){\n",
      "                    result += group.substring(1) + group.substring(0, 1);\n",
      "                }\n",
      "\n",
      "                if (remain > 0){\n",
      "                    result += s.substring(s.length() - remain);\n",
      "                }\n",
      "\n",
      "                return result;\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        final Cyclic cyclic = new Cyclic();\n",
      "        return cyclic.encode_cyclic(cyclic.encode_cyclic(str));\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class DECODE_CYCLIC {\n",
      "    public static String decode_cyclic(String str) {\n",
      "        class Cyclic {\n",
      "            String encode_cyclic(String s) {\n",
      "                ArrayList<String> groups = new ArrayList<String>();\n",
      "\n",
      "// buggy code\n",
      "//                for (int i = 0; i < s.length(); i += 3){\n",
      "                for (int i = 0; i < s.length(); i += 3){\n",
      "                    groups.add(s.substring(i, Math.min(s.length(), i + 3)));\n",
      "                }\n",
      "                int remain = s.length() % 3;\n",
      "                \n",
      "                String result = \"\";\n",
      "                for (String group : groups){\n",
      "                    result += group.substring(1) + group.substring(0, 1);\n",
      "                }\n",
      "\n",
      "                if (remain > 0){\n",
      "                    result += s.substring(s.length() - remain);\n",
      "                }\n",
      "\n",
      "                return result;\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        final Cyclic cyclic = new Cyclic();\n",
      "        return cyclic.encode_cyclic(cyclic.encode_cyclic(str));\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class DECODE_SHIFT {\n",
      "    public static String encode_shift(String string){\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : string.toCharArray()) {\n",
      "            c = (char)((((int) c + 5 - (int)('a')) % 26) + (int)('a'));\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "\n",
      "    public static String decode_shift(String string) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : string.toCharArray()) {\n",
      "// buggy code\n",
      "//            c = (char)((((int) c - 5 - (int)('a')) % 26) + (int)('a'));\n",
      "           c = (char)((((int) c + 21 - (int)('a')) % 26) + (int)('a'));\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class DECODE_SHIFT {\n",
      "    public static String encode_shift(String string){\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : string.toCharArray()) {\n",
      "            c = (char)((((int) c + 5 - (int)('a')) % 26) + (int)('a'));\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "\n",
      "    public static String decode_shift(String string) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : string.toCharArray()) {\n",
      "// buggy code\n",
      "//            c = (char)((((int) c - 5 - (int)('a')) % 26) + (int)('a'));\n",
      "            c = (char)((((int) c + 21 - (int)('a')) % 26) + (int)('a'));\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class DERIVATIVE {\n",
      "    public static List<Integer> derivative(List<Integer> xs) {\n",
      "        List<Integer> result = new ArrayList<Integer>();\n",
      "        \n",
      "// buggy code\n",
      "//        for (int i = 0; i < xs.size(); i += 1)\n",
      "//            result.add(xs.get(i));\n",
      "        for (int i = 1; i < xs.size(); i += 1)\n",
      "            result.add(xs.get(i) * i);\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class DERIVATIVE {\n",
      "    public static List<Integer> derivative(List<Integer> xs) {\n",
      "        List<Integer> result = new ArrayList<Integer>();\n",
      "        \n",
      "// buggy code\n",
      "//        for (int i = 0; i < xs.size(); i += 1)\n",
      "//            result.add(xs.get(i));\n",
      "        for (int i = 0; i < xs.size() - 1; i += 1)\n",
      "            result.add(xs.get(i + 1) - xs.get(i));\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class DIGIT_SUM {\n",
      "    public static int digit_sum(String s) {\n",
      "        int result = 0;\n",
      "        for (char c : s.toCharArray()) {\n",
      "// buggy code\n",
      "//            if (c == ' ')\n",
      "//                break;\n",
      "           if (c == ' ')\n",
      "               break;\n",
      "            if ('A' <= c && c <= 'Z')\n",
      "                result += (int) c;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class DIGIT_SUM {\n",
      "    public static int digit_sum(String s) {\n",
      "        int result = 0;\n",
      "        for (char c : s.toCharArray()) {\n",
      "// buggy code\n",
      "//            if (c == ' ')\n",
      "//                break;\n",
      "            if (c == ' ')\n",
      "                continue;\n",
      "            if ('A' <= c && c <= 'Z')\n",
      "                result += (int) c;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class DIGITS {\n",
      "    public static int digits(int n) {\n",
      "        int product = 1;\n",
      "        int odd_count = 0;\n",
      "// buggy code\n",
      "//        while(n >= 0) {\n",
      "//            int digit = n * 10;\n",
      "//            if(digit % 2 == 0) {\n",
      "//                product /= digit;\n",
      "        while(n > 0) {\n",
      "            int digit = n % 10;\n",
      "            if(digit % 2 == 0) {\n",
      "                product /= digit;\n",
      "            } else {\n",
      "                odd_count++;\n",
      "            }\n",
      "            n /= 10;\n",
      "        }\n",
      "        if(odd_count == 0) return 0;\n",
      "        return product;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class DIGITS {\n",
      "    public static int digits(int n) {\n",
      "        int product = 1;\n",
      "        int odd_count = 0;\n",
      "// buggy code\n",
      "//        while(n >= 0) {\n",
      "//            int digit = n * 10;\n",
      "//            if(digit % 2 == 0) {\n",
      "//                product /= digit;\n",
      "        while(n > 0) {\n",
      "            int digit = n % 10;\n",
      "            if(digit % 2 == 0) {\n",
      "                product *= digit;\n",
      "                odd_count++;\n",
      "            }\n",
      "            n /= 10;\n",
      "        }\n",
      "        if(odd_count == 0) return 0;\n",
      "        return product;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class DO_ALGEBRA {\n",
      "    public static double do_algebra(String[] operator, int[] operand) throws NumberFormatException, ScriptException {\n",
      "        ScriptEngineManager mgr = new ScriptEngineManager();\n",
      "        ScriptEngine engine = mgr.getEngineByName(\"JavaScript\");\n",
      "// buggy code\n",
      "//        String expression = \"\";\n",
      "//        for (int i = 0; i < operand.length; i += 1) {\n",
      "//            expression += operator[i] + operand[i];\n",
      "        String expression = \"(\";\n",
      "        for (int i = 0; i < operand.length; i += 1) {\n",
      "            expression += operator[i] + operand[i];\n",
      "        }\n",
      "        return Double.parseDouble(engine.eval(expression).toString());\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class DO_ALGEBRA {\n",
      "    public static double do_algebra(String[] operator, int[] operand) throws NumberFormatException, ScriptException {\n",
      "        ScriptEngineManager mgr = new ScriptEngineManager();\n",
      "        ScriptEngine engine = mgr.getEngineByName(\"JavaScript\");\n",
      "// buggy code\n",
      "//        String expression = \"\";\n",
      "//        for (int i = 0; i < operand.length; i += 1) {\n",
      "//            expression += operator[i] + operand[i];\n",
      "        String expression = \"\";\n",
      "        for (int i = 0; i < operand.length; i += 1) {\n",
      "            expression += operand[i] + operator[i];\n",
      "        }\n",
      "        return Double.parseDouble(engine.eval(expression).toString());\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class DOUBLE_THE_DIFFERENCE {\n",
      "    public static int double_the_difference(double[] lst) {\n",
      "        int result = 0;\n",
      "        for (Double d : lst) {\n",
      "            if ((int) d.doubleValue() == d) {\n",
      "                int d_int = (int) d.doubleValue();\n",
      "                if (d_int % 2 == 1 && d_int >= 0) {\n",
      "// buggy code\n",
      "//                    result += d_int;\n",
      "                   result += d_int * 2;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class DOUBLE_THE_DIFFERENCE {\n",
      "    public static int double_the_difference(double[] lst) {\n",
      "        int result = 0;\n",
      "        for (Double d : lst) {\n",
      "            if ((int) d.doubleValue() == d) {\n",
      "                int d_int = (int) d.doubleValue();\n",
      "                if (d_int % 2 == 1 && d_int >= 0) {\n",
      "// buggy code\n",
      "//                    result += d_int;\n",
      "//                 } else if (d_int % 2 == 0 && d_int >= 0) {\n",
      "//                     result -= d_int;\n",
      "                    result += d_int * 2;\n",
      "                } else if (d_int % 2 == 0 && d_int >= 0) {\n",
      "                    result -= d_int * 2;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class EAT {\n",
      "    public static int[] eat(int number, int need, int remaining) {\n",
      "// buggy code\n",
      "//        return new int[] {number + need,  remaining - need};\n",
      "       return new int[] {number + need,  remaining - need, need};\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class EAT {\n",
      "    public static int[] eat(int number, int need, int remaining) {\n",
      "// buggy code\n",
      "//        return new int[] {number + need,  remaining - need};\n",
      "        return new int[] {number + need,  remaining - need, need};\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class ENCODE {\n",
      "    public static String encode(String message) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : message.toCharArray()) {\n",
      "            if (Character.isUpperCase(c)) {\n",
      "                c = Character.toLowerCase(c);\n",
      "            }\n",
      "// buggy code\n",
      "//            if (Character.isLowerCase(c)) {\n",
      "            if (Character.isLetter(c)) {\n",
      "                c = Character.toUpperCase(c);\n",
      "            }\n",
      "            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || c == 'A' || c == 'E' || c == 'I' || c == 'O'\n",
      "                    || c == 'U') {\n",
      "                c = (char) ((int) c + 2);\n",
      "            }\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class ENCODE {\n",
      "    public static String encode(String message) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        for (char c : message.toCharArray()) {\n",
      "            if (Character.isUpperCase(c)) {\n",
      "                c = Character.toLowerCase(c);\n",
      "            }\n",
      "// buggy code\n",
      "//            if (Character.isLowerCase(c)) {\n",
      "            if (Character.isLowerCase(c) && c != 'a' && c != 'e' && c != 'i' && c != 'o' && c != 'u') {\n",
      "                c = Character.toUpperCase(c);\n",
      "            }\n",
      "            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || c == 'A' || c == 'E' || c == 'I' || c == 'O'\n",
      "                    || c == 'U') {\n",
      "                c = (char) ((int) c + 2);\n",
      "            }\n",
      "            sb.append(c);\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class ENCRYPT {\n",
      "    public static String encrypt(String s) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        String d = \"abcdefghijklmnopqrstuvwxyz\";\n",
      "        for (char c : s.toCharArray()){\n",
      "            if ('a' <= c && c <= 'z'){\n",
      "                sb.append(\n",
      "// buggy code\n",
      "//                    d.charAt(s.indexOf((int) c) + 2 * 2)\n",
      "                   d.charAt(d.indexOf(c) + 2 * 2)\n",
      "                );\n",
      "            } else {\n",
      "                sb.append(c);\n",
      "            }\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class ENCRYPT {\n",
      "    public static String encrypt(String s) {\n",
      "        StringBuilder sb = new StringBuilder();\n",
      "        String d = \"abcdefghijklmnopqrstuvwxyz\";\n",
      "        for (char c : s.toCharArray()){\n",
      "            if ('a' <= c && c <= 'z'){\n",
      "                sb.append(\n",
      "// buggy code\n",
      "//                    d.charAt(s.indexOf((int) c) + 2 * 2)\n",
      "                    d.charAt(d.indexOf(c) + 2 * 2)\n",
      "                );\n",
      "            } else {\n",
      "                sb.append(c);\n",
      "            }\n",
      "        }\n",
      "        return sb.toString();\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class EVEN_ODD_COUNT {\n",
      "    public static int[] even_odd_count(int num) {\n",
      "        int even_count = 0;\n",
      "        int odd_count = 0;\n",
      "\n",
      "// buggy code\n",
      "//        for (char c : (num + \"\").toCharArray()) {\n",
      "//            if ((int)c % 2 == 0) even_count += 1;\n",
      "//            if ((int)c % 2 == 1) odd_count += 1;\n",
      "       String num_str = num + \"\";\n",
      "       for (char c : num_str.toCharArray()) {\n",
      "           if ((int)c % 2 == 0) even_count += 1;\n",
      "           if ((int)c % 2 == 1) odd_count += 1;\n",
      "        }\n",
      "        return new int[] {even_count, odd_count};\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class EVEN_ODD_COUNT {\n",
      "    public static int[] even_odd_count(int num) {\n",
      "        int even_count = 0;\n",
      "        int odd_count = 0;\n",
      "\n",
      "// buggy code\n",
      "//        for (char c : (num + \"\").toCharArray()) {\n",
      "//            if ((int)c % 2 == 0) even_count += 1;\n",
      "//            if ((int)c % 2 == 1) odd_count += 1;\n",
      "        for (char c : (num + \"\").toCharArray()) {\n",
      "            if ((c - '0') % 2 == 0) even_count += 1;\n",
      "            if ((c - '0') % 2 == 1) odd_count += 1;\n",
      "        }\n",
      "        return new int[] {even_count, odd_count};\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class EVEN_ODD_PALINDROME {\n",
      "    public static boolean is_palindrome(int n) {\n",
      "        String n_str = Integer.toString(n);\n",
      "        String n_str_rev = \"\";\n",
      "        for(int i = n_str.length() - 1; i >= 0; i--) {\n",
      "            n_str_rev += n_str.substring(i, i + 1);\n",
      "        }\n",
      "        return n_str.equals(n_str_rev);\n",
      "    }\n",
      "\n",
      "    public static ArrayList<Integer> even_odd_palindrome(int n) {\n",
      "        int even_palindrome_count = 0, odd_palindrome_count = 0;\n",
      "        for(int i = 1; i <= n; i++) {\n",
      "// buggy code\n",
      "//            if((i % 2) == 0 && is_palindrome(i)) odd_palindrome_count--;\n",
      "//            else if((i % 2) == 1 && is_palindrome(i)) even_palindrome_count--;\n",
      "           if((i % 2) == 0 && is_palindrome(i)) even_palindrome_count++;\n",
      "           else if((i % 2) == 1 && is_palindrome(i)) odd_palindrome_count++;\n",
      "        }\n",
      "        ArrayList<Integer> result = new ArrayList<>(Arrays.asList(even_palindrome_count, odd_palindrome_count));\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class EVEN_ODD_PALINDROME {\n",
      "    public static boolean is_palindrome(int n) {\n",
      "        String n_str = Integer.toString(n);\n",
      "        String n_str_rev = \"\";\n",
      "        for(int i = n_str.length() - 1; i >= 0; i--) {\n",
      "            n_str_rev += n_str.substring(i, i + 1);\n",
      "        }\n",
      "        return n_str.equals(n_str_rev);\n",
      "    }\n",
      "\n",
      "    public static ArrayList<Integer> even_odd_palindrome(int n) {\n",
      "        int even_palindrome_count = 0, odd_palindrome_count = 0;\n",
      "        for(int i = 1; i <= n; i++) {\n",
      "// buggy code\n",
      "//            if((i % 2) == 0 && is_palindrome(i)) odd_palindrome_count--;\n",
      "//            else if((i % 2) == 1 && is_palindrome(i)) even_palindrome_count--;\n",
      "           if((i % 2) == 0 && is_palindrome(i)) even_palindrome_count++;\n",
      "           else if((i % 2) == 1 && is_palindrome(i)) odd_palindrome_count++;\n",
      "        }\n",
      "        ArrayList<Integer> result = new ArrayList<>(Arrays.asList(even_palindrome_count, odd_palindrome_count));\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class EXCHANGE {\n",
      "    public static String exchange(ArrayList<Integer> lst1, ArrayList<Integer> lst2) {\n",
      "        int odd = 0, even = 0;\n",
      "// buggy code\n",
      "//        for(int i : lst2) {\n",
      "//            if((i % 2) == 1) even++;\n",
      "        for(int i : lst1) {\n",
      "            if((i % 2) == 1) odd++;\n",
      "        }\n",
      "        for(int i : lst2) {\n",
      "            if((i % 2) == 0) even++;\n",
      "        }\n",
      "        if(even >= odd) {\n",
      "            return \"YES\";\n",
      "        }\n",
      "        return \"NO\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class EXCHANGE {\n",
      "    public static String exchange(ArrayList<Integer> lst1, ArrayList<Integer> lst2) {\n",
      "        int odd = 0, even = 0;\n",
      "// buggy code\n",
      "//        for(int i : lst2) {\n",
      "//            if((i % 2) == 1) even++;\n",
      "        for(int i : lst1) {\n",
      "            if((i % 2) == 1) odd++;\n",
      "        }\n",
      "        for(int i : lst2) {\n",
      "            if((i % 2) == 0) even++;\n",
      "        }\n",
      "        if(even >= odd) {\n",
      "            return \"YES\";\n",
      "        }\n",
      "        return \"NO\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class FACTORIAL {\n",
      "    public static ArrayList<Integer> factorial(int n) {\n",
      "        ArrayList<Integer> ret = new ArrayList<>();\n",
      "        for(int i = 1; i <= n; i++) {\n",
      "            if((i % 2) == 0) {\n",
      "                int x = 1;\n",
      "                for(int j = 1; j <= i; j++) x *= j;\n",
      "                ret.add(x);\n",
      "            } else {\n",
      "                int x = 0;\n",
      "// buggy code\n",
      "//                for(int j = 1; j <= i; i++) x -= j;\n",
      "               for(int j = 1; j <= i; j++) x -= j;\n",
      "                ret.add(x);\n",
      "            }\n",
      "        }\n",
      "        return ret;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class FACTORIAL {\n",
      "    public static ArrayList<Integer> factorial(int n) {\n",
      "        ArrayList<Integer> ret = new ArrayList<>();\n",
      "        for(int i = 1; i <= n; i++) {\n",
      "            if((i % 2) == 0) {\n",
      "                int x = 1;\n",
      "                for(int j = 1; j <= i; j++) x *= j;\n",
      "                ret.add(x);\n",
      "            } else {\n",
      "                int x = 0;\n",
      "// buggy code\n",
      "//                for(int j = 1; j <= i; i++) x -= j;\n",
      "               for(int j = 1; j <= i; j++) x -= j;\n",
      "                ret.add(x);\n",
      "            }\n",
      "        }\n",
      "        return ret;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class FACTORIZE {\n",
      "    public static List<Integer> factorize(int n){\n",
      "        List<Integer> result = new ArrayList<Integer>();\n",
      "\n",
      "        int i = 2;\n",
      "        while (i <= (int)(Math.sqrt(n) + 1)){\n",
      "            if (n % i == 0){\n",
      "                result.add(i);\n",
      "                n = n / i;\n",
      "            } else{\n",
      "                i += 1;\n",
      "            }\n",
      "        }\n",
      "\n",
      "// buggy code\n",
      "        if (n > 1){\n",
      "            result.add(n);\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class FACTORIZE {\n",
      "    public static List<Integer> factorize(int n){\n",
      "        List<Integer> result = new ArrayList<Integer>();\n",
      "\n",
      "        int i = 2;\n",
      "        while (i <= (int)(Math.sqrt(n) + 1)){\n",
      "            if (n % i == 0){\n",
      "                result.add(i);\n",
      "                n = n / i;\n",
      "            } else{\n",
      "                i += 1;\n",
      "            }\n",
      "        }\n",
      "\n",
      "// buggy code\n",
      "//         result.add(n);\n",
      "        if (n != 1) result.add(n);\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class FIB {\n",
      "    public static int fib(int n) {\n",
      "// buggy code\n",
      "        if (n < 2) return n;\n",
      "        return fib(n - 1) + fib(n - 2);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class FIB {\n",
      "    public static int fib(int n) {\n",
      "// buggy code\n",
      "//         if (n < 2) return 1;\n",
      "        if (n < 2) return n;\n",
      "        return fib(n - 1) + fib(n - 2);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class FIB4 {\n",
      "    public static int fib4(int n) {\n",
      "        ArrayList<Integer> result = new ArrayList<Integer>(); \n",
      "        result.add(0);\n",
      "        result.add(0);\n",
      "        result.add(2);\n",
      "        result.add(0);\n",
      "\n",
      "        if (n < 4)\n",
      "            return result.get(n);\n",
      "        for (int i = 4; i < n + 1; i += 1) {\n",
      "            result.add(\n",
      "                result.get(0) + result.get(1) + result.get(2) + result.get(3)\n",
      "            );\n",
      "// buggy code\n",
      "            result.remove(0);\n",
      "        }\n",
      "        return result.get(result.size() - 1);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class FIB4 {\n",
      "    public static int fib4(int n) {\n",
      "        ArrayList<Integer> result = new ArrayList<Integer>(); \n",
      "        result.add(0);\n",
      "        result.add(0);\n",
      "        result.add(2);\n",
      "        result.add(0);\n",
      "\n",
      "        if (n < 4)\n",
      "            return result.get(n);\n",
      "        for (int i = 4; i < n + 1; i += 1) {\n",
      "            result.add(\n",
      "                result.get(0) + result.get(1) + result.get(2) + result.get(3)\n",
      "            );\n",
      "// buggy code\n",
      "//             result.remove(0);\n",
      "            result.remove(0);\n",
      "            result.remove(0);\n",
      "        }\n",
      "        return result.get(result.size() - 1);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class FIBFIB {\n",
      "    public static int fibfib(int n) {\n",
      "        if (n == 0)\n",
      "            return 0;\n",
      "        if (n == 1)\n",
      "            return 0;\n",
      "// buggy code\n",
      "        if (n == 2)\n",
      "            return 1;\n",
      "        return fibfib(n - 1) + fibfib(n - 2) + fibfib(n - 3);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class FIBFIB {\n",
      "    public static int fibfib(int n) {\n",
      "        if (n == 0)\n",
      "            return 0;\n",
      "        if (n == 1)\n",
      "            return 0;\n",
      "// buggy code\n",
      "//         if (n == 2)\n",
      "//             return 1;\n",
      "        if (n == 2)\n",
      "            return 1;\n",
      "        if (n == 3)\n",
      "            return 1;\n",
      "        return fibfib(n - 1) + fibfib(n - 2) + fibfib(n - 3);\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class FILE_NAME_CHECK {\n",
      "    public static String file_name_check(String file_name) {\n",
      "        String[] lst = file_name.split(\"\\.\");\n",
      "\n",
      "        if (lst.length != 2)\n",
      "            return \"No\";\n",
      "        if (! (lst[1].equals(\"txt\") || lst[1].equals(\"exe\") || lst[1].equals(\"dll\")))\n",
      "            return \"No\";\n",
      "        if (lst[0].length() == 0)\n",
      "            return \"No\";\n",
      "// buggy code\n",
      "//        if ('a' > lst[0].charAt(0) && lst[0].charAt(0) > 'z' || 'A' > lst[0].charAt(0) && lst[0].charAt(0) > 'Z')\n",
      "        if ('a' > lst[0].charAt(0) && lst[0].charAt(0) > 'z' || 'A' > lst[0].charAt(0) && lst[0].charAt(0) > 'Z' || lst[0].charAt(0) == '_')\n",
      "            return \"No\";\n",
      "        int digit_cnt = 0;\n",
      "        for (char c : file_name.toCharArray()){\n",
      "            if ('0' <= c && c <= '9')\n",
      "                digit_cnt += 1;\n",
      "        }\n",
      "        if (digit_cnt > 3)  return \"No\";\n",
      "        return \"Yes\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class FILE_NAME_CHECK {\n",
      "    public static String file_name_check(String file_name) {\n",
      "        String[] lst = file_name.split(\"\\.\");\n",
      "\n",
      "        if (lst.length != 2)\n",
      "            return \"No\";\n",
      "        if (! (lst[1].equals(\"txt\") || lst[1].equals(\"exe\") || lst[1].equals(\"dll\")))\n",
      "            return \"No\";\n",
      "        if (lst[0].length() == 0)\n",
      "            return \"No\";\n",
      "// buggy code\n",
      "//        if ('a' > lst[0].charAt(0) && lst[0].charAt(0) > 'z' || 'A' > lst[0].charAt(0) && lst[0].charAt(0) > 'Z')\n",
      "        if ('a' > lst[0].charAt(0) || lst[0].charAt(0) > 'z' || 'A' > lst[0].charAt(0) || lst[0].charAt(0) > 'Z')\n",
      "            return \"No\";\n",
      "        int digit_cnt = 0;\n",
      "        for (char c : file_name.toCharArray()){\n",
      "            if ('0' <= c && c <= '9')\n",
      "                digit_cnt += 1;\n",
      "        }\n",
      "        if (digit_cnt > 3)  return \"No\";\n",
      "        return \"Yes\";\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class FILTER_BY_PREFIX {\n",
      "    public static List<String> filter_by_prefix(List<String> strings, String prefix) {\n",
      "        List<String> result = new ArrayList<String>();\n",
      "\n",
      "        for (String string : strings){\n",
      "// buggy code\n",
      "//            if (string.startsWith(prefix, 1)){\n",
      "           if (string.startsWith(prefix)){\n",
      "                result.add(string);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class FILTER_BY_PREFIX {\n",
      "    public static List<String> filter_by_prefix(List<String> strings, String prefix) {\n",
      "        List<String> result = new ArrayList<String>();\n",
      "\n",
      "        for (String string : strings){\n",
      "// buggy code\n",
      "//            if (string.startsWith(prefix, 1)){\n",
      "           if (string.startsWith(prefix)){\n",
      "                result.add(string);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "PSM inference:\n",
      "public class FILTER_BY_SUBSTRING {\n",
      "    public static List<String> filter_by_substring(List<String> strings, String substring) {\n",
      "        List<String> result = new ArrayList<String>();\n",
      "        for (String string : strings){\n",
      "// buggy code\n",
      "//            result.add(string);\n",
      "           if (string.contains(substring))\n",
      "               result.add(string);\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n",
      "SPM inference:\n",
      "public class FILTER_BY_SUBSTRING {\n",
      "    public static List<String> filter_by_substring(List<String> strings, String substring) {\n",
      "        List<String> result = new ArrayList<String>();\n",
      "        for (String string : strings){\n",
      "// buggy code\n",
      "//            result.add(string);\n",
      "           if (string.contains(substring))\n",
      "              result.add(string);\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "}\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/peft\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "\n",
    "!git clone https://github.com/ardalaan/DeepSeekCoder6.7B_APR_FIM_finetuning\n",
    "\n",
    "%cd DeepSeekCoder6.7B_APR_FIM_finetuning/\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6edcb-fb65-46cb-a1be-7aa3af1f9cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
